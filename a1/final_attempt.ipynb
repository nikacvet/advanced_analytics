{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2731\n",
      "[LightGBM] [Info] Number of data points in the train set: 26983, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 199000.000000\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001527 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2731\n",
      "[LightGBM] [Info] Number of data points in the train set: 26983, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 544008.437500\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Categorical features with more bins than the configured maximum bin number found.\n",
      "[LightGBM] [Warning] For categorical features, max_bin and max_bin_by_feature may be ignored with a large number of categories.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001089 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2731\n",
      "[LightGBM] [Info] Number of data points in the train set: 26983, number of used features: 12\n",
      "[LightGBM] [Info] Start training from score 327000.000000\n",
      "âœ… Submission file saved: finall2_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "train_set = pd.read_csv(\"./data/cleaned_train.csv\")\n",
    "val_set = pd.read_csv(\"./data/cleaned_val.csv\")\n",
    "test_df = pd.read_csv(\"./data/cleaned_test.csv\")\n",
    "y_train = pd.read_csv(\"./data/y_train.csv\")\n",
    "y_val = pd.read_csv(\"./data/y_val.csv\")\n",
    "\n",
    "# Concatenate train + val\n",
    "X_full = pd.concat([train_set.drop(columns=[\"id\"]), val_set.drop(columns=[\"id\"])])\n",
    "y_full = pd.concat([y_train, y_val])\n",
    "\n",
    "# Categorical features\n",
    "categorical_cols = [\"advertiser\", \"subtype\", \"energy_label\", \"province\"]\n",
    "for col in categorical_cols:\n",
    "    X_full[col] = X_full[col].astype(\"category\")\n",
    "    test_df[col] = test_df[col].astype(\"category\")\n",
    "\n",
    "X_test = test_df.drop(columns=[\"id\"])\n",
    "\n",
    "# Use best parameters found earlier\n",
    "# Below is the code for hyperparameter tuning using RandomizedSearchCV from a previous attempt.\n",
    "\n",
    "# Define parameter distributions\n",
    "# param_dist = {\n",
    "#     'learning_rate': st.uniform(0.01, 0.1),\n",
    "#     'num_leaves': st.randint(20, 50),\n",
    "#     'max_depth': st.randint(5, 15),\n",
    "#     'n_estimators': st.randint(500, 2000),\n",
    "#     'subsample': st.uniform(0.6, 0.9),\n",
    "#     'colsample_bytree': st.uniform(0.6, 0.9),\n",
    "#     'min_child_samples': st.randint(10, 50),\n",
    "#     'reg_alpha': st.uniform(0, 10),\n",
    "#     'reg_lambda': st.uniform(0, 10)\n",
    "# }\n",
    "# lgb_model = LGBMRegressor(objective=\"regression\", boosting_type=\"gbdt\", random_state=42)\n",
    "# # Randomized Search\n",
    "# random_search = RandomizedSearchCV(lgb_model, param_dist, n_iter=50, scoring=\"neg_mean_absolute_error\", cv=3, verbose=2, n_jobs=-1, random_state=42)\n",
    "# random_search.fit(X_train, y_train)\n",
    "# # Best parameters\n",
    "# print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "best_params = {\n",
    "    'learning_rate': 0.1094550510797341,\n",
    "    'num_leaves': 47,\n",
    "    'max_depth': 9,\n",
    "    'min_child_samples': 15,\n",
    "    'subsample': 0.9853657334855829,\n",
    "    'colsample_bytree': 0.6392433945789904,\n",
    "    'reg_alpha': 0.15456616528867428,\n",
    "    'reg_lambda': 9.283185625877254,\n",
    "    'n_estimators': 598,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train three quantile regressors (Lower, Median, Upper)\n",
    "lgb_lower = LGBMRegressor(objective='quantile', alpha=0.1, **best_params)\n",
    "lgb_upper = LGBMRegressor(objective='quantile', alpha=0.9, **best_params)\n",
    "lgb_median = LGBMRegressor(objective='quantile', alpha=0.5, **best_params)\n",
    "\n",
    "lgb_lower.fit(X_full, y_full)\n",
    "lgb_upper.fit(X_full, y_full)\n",
    "lgb_median.fit(X_full, y_full)\n",
    "\n",
    "# Predict on test\n",
    "lower_bound = lgb_lower.predict(X_test)\n",
    "upper_bound = lgb_upper.predict(X_test)\n",
    "y_pred_test = lgb_median.predict(X_test)\n",
    "\n",
    "# Fix bounds\n",
    "lower_bound = np.minimum(lower_bound, upper_bound)\n",
    "upper_bound = np.maximum(lower_bound, upper_bound)\n",
    "\n",
    "# Ensure PRED is within bounds\n",
    "y_pred_test = np.clip(y_pred_test, lower_bound, upper_bound)\n",
    "\n",
    "# Final Submission File\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"id\"],\n",
    "    \"LOWER\": lower_bound,\n",
    "    \"UPPER\": upper_bound,\n",
    "    \"PRED\": y_pred_test\n",
    "})\n",
    "\n",
    "# Check bounds again\n",
    "mask_invalid = (submission[\"LOWER\"] > submission[\"UPPER\"]) | \\\n",
    "               (submission[\"PRED\"] < submission[\"LOWER\"]) | \\\n",
    "               (submission[\"PRED\"] > submission[\"UPPER\"])\n",
    "if mask_invalid.any():\n",
    "    print(\"Invalid predictions in the submission file.\")\n",
    "\n",
    "submission.to_csv(\"finall_submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advanced_analytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
