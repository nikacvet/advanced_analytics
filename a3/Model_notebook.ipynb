{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2329258-6fa0-4815-b41c-99f37a1a28af",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a290ca-ddb9-424c-95ad-9d5a1e5e32b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 aid                        categories  \\\n",
      "0  http://arxiv.org/abs/2504.15253v1                       cs.CL,cs.LG   \n",
      "1  http://arxiv.org/abs/2504.01456v1                       astro-ph.GA   \n",
      "2  http://arxiv.org/abs/2504.02811v1                             cs.IR   \n",
      "3  http://arxiv.org/abs/2504.02637v1                             cs.NI   \n",
      "4  http://arxiv.org/abs/2504.15250v1  cond-mat.stat-mech,cond-mat.soft   \n",
      "5  http://arxiv.org/abs/2504.02646v1         cs.LG,cs.AI,cs.IR,stat.ML   \n",
      "6  http://arxiv.org/abs/2504.02538v1                       astro-ph.GA   \n",
      "7  http://arxiv.org/abs/2504.01395v1                       cs.CR,cs.AI   \n",
      "8  http://arxiv.org/abs/2504.02539v1                           math.HO   \n",
      "9  http://arxiv.org/abs/2504.16528v1                             cs.GT   \n",
      "\n",
      "        main_category             published  \\\n",
      "0               cs.CL  2025-04-21T17:33:23Z   \n",
      "1         astro-ph.GA  2025-04-02T08:10:05Z   \n",
      "2               cs.IR  2025-04-03T17:55:12Z   \n",
      "3               cs.NI  2025-04-03T14:34:38Z   \n",
      "4  cond-mat.stat-mech  2025-04-21T17:27:41Z   \n",
      "5               cs.LG  2025-04-03T14:40:40Z   \n",
      "6         astro-ph.GA  2025-04-03T12:39:47Z   \n",
      "7               cs.CR  2025-04-02T06:30:55Z   \n",
      "8             math.HO  2025-04-03T12:40:16Z   \n",
      "9               cs.GT  2025-04-23T08:53:10Z   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                summary  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Scaling test-time computation, or affording a generator large language model\\n(LLM) extra compute during inference, typically employs the help of external\\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\\nmodels trained to generate evaluations and critiques (explanations) in natural\\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\\njudge empirical successes, their effectiveness as evaluators in test-time\\nscaling settings is largely unknown. In this paper, we introduce the Judge\\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\\nperformance in three domains (math reasoning, code generation, and instruction\\nfollowing) under three task settings: response reranking, step-level beam\\nsearch, and critique-based response refinement. We evaluate 10 different judge\\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\\nparameters). Our benchmark shows that while judges are competitive with outcome\\nreward models in reranking, they are consistently worse than process reward\\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\\ntheir natural language critiques are currently ineffective in guiding the\\ngenerator towards better responses.   \n",
      "1                                                                                                        In this work an analysis of the intracluster light (ICL) in the galaxy\\ncluster SMACS J0723.3-7327 (hereafter, SMACS J0723) using JWST/NIRCam deep\\nimaging in six filters (F090W to F444W) is presented. The images were processed\\nfor low surface brightness (LSB) science, with additional correction for\\ninstrumental scattering in the short-wavelength channels, and analysed using\\nwavelet-based decomposition. The ICL, brightest cluster galaxy (BCG), and\\nsatellite galaxies were extracted and modelled, with 2D maps for each\\ncomponent. ICL and ICL+BCG fractions, computed across all filters within a 400\\nkpc radius, exhibit a flat trend with wavelength, averaging 28% and 34%,\\nrespectively. Flux ratios between the BCG and the next brightest members\\n(M$_{12}$, M$_{13}$ and M$_{14}$) also display minimal wavelength dependence.\\nThese results indicate that SMACS J0723 is a dynamically evolved cluster with a\\ndominant BCG and well-developed ICL. Five prominent ICL substructures are\\nanalysed, contributing to 10-12% of the total ICL+BCG flux budget, slightly\\nexceeding simulation predictions. Their short dynamical timescales suggest an\\ninstantaneous ICL injection rate of several $10^3 L_{\\odot}\\,{\\rm yr}^{-1}$,\\nconsistent with active dynamical assembly. These findings support a scenario\\nwhere SMACS J0723's ICL growth is currently driven by galaxy mergers involving\\nthe BCG and other bright satellites, rather than by the accretion of\\npre-processed ICL from a recent cluster merger. However, extrapolating the\\ncurrent injection rate to the cluster's lifetime indicates that additional\\nmechanisms are required to match the growth observed in other clusters over\\ncosmic times.   \n",
      "2                                                                                                                                                                                                                                                                                                                    Intermittent renewable energies are increasingly dominating electricity grids\\nand are forecasted to be the main force driving out fossil fuels from the grid\\nin most major economies until 2040. However, grids based on intermittent\\nrenewables are challenged by diurnal and seasonal mismatch between supply of\\nsun and wind and demand for electricity, including for heat pumps and electric\\ntwo and four wheelers. Load management and demand response measures promise to\\nadjust for this mismatch, utilizing information- and price-based approaches to\\nsteer demand towards times with high supply of intermittent renewables. Here,\\nwe systematically review the literature estimating CO2 savings from residential\\nload management in developing and developed nations. We find that load\\nmanagement holds high potential, locally differentiated with energy mix\\n(including the respective share of renewables and fossils), climate zone, and\\nthe regulatory environment and price mechanism. Most identified studies suggest\\na mitigation potential between 1 and 20%. Load management becomes more relevant\\nwith higher shares of intermittent renewables, and when electricity prices are\\nhigh. Importantly, load management aligns consumers' financial incentives with\\nclimate change mitigation, thus rendering accompanying strategies politically\\nfeasible. We summarize key regulatory steps to facilitate load management in\\neconomies and to realize relevant consumer surplus and mitigation potential.   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Medium access in 5G systems was tailored to accommodate diverse traffic\\nclasses through network resource slicing. 6G wireless systems are expected to\\nbe significantly reliant on Artificial Intelligence (AI), leading to\\ndata-driven and goal-oriented communication. This leads to augmentation of the\\ndesign space for Medium Access Control (MAC) protocols, which is the focus of\\nthis article. We introduce a taxonomy based on push-based and pull-based\\ncommunication, which is useful to categorize both the legacy and the AI-driven\\naccess schemes. We provide MAC protocol design guidelines for pull- and\\npush-based communication in terms of goal-oriented criteria, such as timing and\\ndata relevance. We articulate a framework for co-existence between pull and\\npush-based communications in 6G systems, combining their advantages. We\\nhighlight the design principles and main tradeoffs, as well as the\\narchitectural considerations for integrating these designs in Open-Radio Access\\nNetwork (O-RAN) and 6G systems.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We investigate the dynamics of a massive tracer particle coupled to an\\ninteracting active bath, modeled as a harmonic chain of overdamped active\\nparticles analytically, with an aim to understand the impact of bath\\ninteractions and activity on the nonequilibrium fluctuations of the tracer.\\nFrom the microscopic equations, we derive the tracer particle's effective\\nLangevin equation, obtaining the dissipative and stochastic forces from the\\nbath. We analyze the friction kernel, revealing power-law tails in the weak\\ncoupling limit and exponential decay in the strong coupling regime. Due to the\\ninterplay between bath interactions, probe-bath coupling, and activity, the\\nmean squared displacement, velocity, and stationary velocity correlations\\nexhibit different dynamical regimes, which we characterize analytically. Under\\nharmonic confinement, we find that energy equipartition holds at low activity\\nbut breaks down at higher activity, with the kinetic energy exhibiting a\\nnon-monotonic dependence on the activity of the bath.   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We study how to use naturally available user feedback, such as clicks, to\\noptimize large language model (LLM) pipelines for generating personalized\\nsentences using prompts. Naive approaches, which estimate the policy gradient\\nin the prompt space, suffer either from variance caused by the large action\\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\\nthese challenges, we propose a novel kernel-based off-policy gradient method,\\nwhich estimates the policy gradient by leveraging similarity among generated\\nsentences, substantially reducing variance while suppressing the bias.\\nEmpirical results on our newly established suite of benchmarks demonstrate the\\neffectiveness of the proposed approach in generating personalized descriptions\\nfor movie recommendations, particularly when the number of candidate prompts is\\nlarge.   \n",
      "6                                                                                                                                                                                                                                                                                                                                 Deuteration of hydrocarbon material, including polycyclic aromatic\\nhydrocarbons (PAHs), has been proposed to account for the low gas-phase\\nabundances of D in the interstellar medium. JWST spectra of four star-forming\\nregions in M51 show an emission feature, with central wavelength\\n$\\sim$4.647$\\mu$m and FWHM 0.0265$\\mu$m, corresponding to the C-D stretching\\nmode in aliphatic hydrocarbons. The emitting aliphatic material is estimated to\\nhave (D/H)$_{\\rm aliph}\\approx 0.17\\pm0.02$ -- a factor $\\sim$$10^4$ enrichment\\nrelative to the overall interstellar medium (ISM). On $\\sim$$50\\,$pc scales,\\ndeuteration levels toward four H$\\,$II regions in M51 are 2-3 times higher than\\nin the Orion Bar photodissociation region (PDR), with implications for the\\nprocesses responsible for the formation and evolution of hydrocarbon\\nnanoparticles, including PAHs. The deuteration of the aliphatic material is\\nfound to anticorrelate with helium ionization in the associated H$\\,$II,\\nsuggesting that harsh FUV radiation may act to lower the deuteration of\\naliphatics in PDRs near massive stars. No evidence is found for deuteration of\\naromatic material, with (D/H)$_{\\rm arom} \\lesssim 0.016$: deuteration of the\\naliphatic material exceeds that of the aromatic material by at least a factor\\n10. The observed levels of deuteration may account for the depletion of D\\nobserved in the Galactic interstellar medium. If so, the $4.65\\mu$m feature may\\nbe detectable in absorption.   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Differentially private (DP) image synthesis aims to generate synthetic images\\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\\norganizations sharing and utilizing synthetic images. Although previous methods\\nhave significantly progressed, especially in training diffusion models on\\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\\nstage in the beginning, where diffusion models learn simple features of the\\nsensitive images. To facilitate this easy stage, we propose to use `central\\nimages', simply aggregations of random samples of the sensitive dataset.\\nIntuitively, although those central images do not show details, they\\ndemonstrate useful characteristics of all images and only incur minimal privacy\\ncosts, thus helping early-phase model training. We conduct experiments to\\npresent that on the average of four investigated image datasets, the fidelity\\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\\nstate-of-the-art method.   \n",
      "8  In the present work, we aim to restore Book X of the $\\it{Elements}$ to its\\noriginal Theaetetean, pre-Eudoxean form in two separate ways. First, we restore\\nthe considerable mathematical content of Book X, by correlating Book X with\\nPlato's account of Theaetetus' mathematical discoveries and Plato's imitations\\nof these discoveries for his philosophy. Thus, Theaetetus proved (i) the\\neventual periodicity of the anthyphairesis of lines a to b, satisfying $Ma^2 =\\nNb^2$, for MN not square number; (ii) the eventual periodic anthyphairesis of\\nlines a to b, satisfying more general quadratic expressions, including the\\nApplication of Areas in defect, and employing this to show that the 12 classes\\nof alogoi lines, including the minor, despite being alogoi, are determined by\\nan eventually periodic Application of Areas in defect; (iii) the anthyphairetic\\npalindromic periodicity of the anthyphairesis of the surds $\\sqrt{N}$ for any\\nnon-square number N, of relevance to the general Pell's Diophantine problem.\\nSecondly, we restore the proofs of all propositions of Book X, in such way that\\nthese are proofs based on Theaetetus', and not on Eudoxus' theory of proportion\\nof magnitudes, in particular not making any use of Eudoxus' condition (namely\\nof definition 4 of Book V). The restoration is based on our reconstruction of\\nTheaetetus' theory of proportion for magnitudes, for the limited class of\\nratios a/b such that either a, b are commensurable or the anthyphairesis of a\\nto b is eventually periodic, without employing Eudoxus' condition, and its\\nsuccess provides a confirmation of our reconstruction.\\n  The final version of this paper will appear as a chapter in the journal\\nGanita Bh\\=arat\\=i, Bulletin of the Indian Society for History of Mathematics,\\n(2) 45 (2023).   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This paper presents (permissive) \\emph{Quantitative Strategy Templates}\\n(QaSTels) to succinctly represent infinitely many winning strategies in\\ntwo-player energy and mean-payoff games. This transfers the recently introduced\\nconcept of \\emph{Permissive (qualitative) Strategy Templates} (PeSTels) for\\n$\\omega$-regular games to games with quantitative objectives. We provide the\\ntheoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their\\n(incremental) combination with PeSTels for games with mixed quantitative and\\nqualitative objectives. Using a prototype implementation of our synthesis\\nalgorithms, we demonstrate empirically that QaSTels extend the advantageous\\nproperties of strategy templates over single winning strategies -- known from\\nPeSTels -- to games with (additional) quantitative objectives. This includes\\n(i) the enhanced robustness of strategies due to their runtime-adaptability,\\nand (ii) the compositionality of templates w.r.t. incrementally arriving\\nobjectives. We use control-inspired examples to illustrate these superior\\nproperties of QaSTels for CPS design.   \n",
      "\n",
      "                                                                                                                           title  \n",
      "0                       Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\\n  Test-Time Scaling Evaluators  \n",
      "1                               Multiscale exploration of SMACS J0723.3--7327's intracluster light and\\n  past dynamical history  \n",
      "2  An Assessment of the CO2 Emission Reduction Potential of Residential\\n  Load Management in Developing and Developed Countries  \n",
      "3                                                           Medium Access for Push-Pull Data Transmission in 6G Wireless Systems  \n",
      "4                                            Tracer dynamics in an interacting active bath: fluctuations and energy\\n  partition  \n",
      "5                                                                                    Prompt Optimization with Logged Bandit Data  \n",
      "6                                              Detection of Deuterated Hydrocarbon Nanoparticles in the Whirlpool\\n  Galaxy, M51  \n",
      "7                                           From Easy to Hard: Building a Shortcut for Differentially Private Image\\n  Synthesis  \n",
      "8                                                  The restoration of Book X of the Elements to its original Theaetetean\\n  form  \n",
      "9                                                                                                Quantitative Strategy Templates  \n",
      "Index(['aid', 'categories', 'main_category', 'published', 'summary', 'title'], dtype='object')\n",
      "aid              object\n",
      "categories       object\n",
      "main_category    object\n",
      "published        object\n",
      "summary          object\n",
      "title            object\n",
      "dtype: object\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12355 entries, 0 to 12354\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   aid            12355 non-null  object\n",
      " 1   categories     12355 non-null  object\n",
      " 2   main_category  12355 non-null  object\n",
      " 3   published      12355 non-null  object\n",
      " 4   summary        12355 non-null  object\n",
      " 5   title          12355 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 579.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_excel(\"./data/output_test.xlsx\")\n",
    "print(df.head(10))\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c7e56dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aid              0\n",
      "categories       0\n",
      "main_category    0\n",
      "published        0\n",
      "summary          0\n",
      "title            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# No missings\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1c5a8cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories\n",
      "cs.CV                                                                           861\n",
      "quant-ph                                                                        318\n",
      "cs.CL                                                                           283\n",
      "cs.LG                                                                           263\n",
      "math.AP                                                                         213\n",
      "                                                                               ... \n",
      "astro-ph.HE,astro-ph.CO,astro-ph.SR,hep-ph,nucl-th                                1\n",
      "astro-ph.EP,physics.soc-ph                                                        1\n",
      "astro-ph.GA,astro-ph.CO,astro-ph.IM,astro-ph.SR,physics.ed-ph,physics.soc-ph      1\n",
      "astro-ph.GA,astro-ph.CO,astro-ph.SR                                               1\n",
      "stat.CO,62-08                                                                     1\n",
      "Name: count, Length: 2580, dtype: int64\n",
      "main_category\n",
      "cs.CV               1349\n",
      "cs.LG                826\n",
      "cs.CL                598\n",
      "quant-ph             570\n",
      "cs.RO                360\n",
      "                    ... \n",
      "math.GM                1\n",
      "physics.atm-clus       1\n",
      "q-bio.MN               1\n",
      "q-bio.TO               1\n",
      "stat.OT                1\n",
      "Name: count, Length: 145, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Lot of Class imbalance, most come from computer science (computer vision and pattern recognition the most)\n",
    "\n",
    "print(df.value_counts(\"categories\", sort=True))\n",
    "print(df.value_counts(\"main_category\", sort=True)) #145 differnet categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31915dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                           title  \\\n",
      "0                       Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\\n  Test-Time Scaling Evaluators   \n",
      "1                               Multiscale exploration of SMACS J0723.3--7327's intracluster light and\\n  past dynamical history   \n",
      "2  An Assessment of the CO2 Emission Reduction Potential of Residential\\n  Load Management in Developing and Developed Countries   \n",
      "3                                                           Medium Access for Push-Pull Data Transmission in 6G Wireless Systems   \n",
      "4                                            Tracer dynamics in an interacting active bath: fluctuations and energy\\n  partition   \n",
      "5                                                                                    Prompt Optimization with Logged Bandit Data   \n",
      "6                                              Detection of Deuterated Hydrocarbon Nanoparticles in the Whirlpool\\n  Galaxy, M51   \n",
      "7                                           From Easy to Hard: Building a Shortcut for Differentially Private Image\\n  Synthesis   \n",
      "8                                                  The restoration of Book X of the Elements to its original Theaetetean\\n  form   \n",
      "9                                                                                                Quantitative Strategy Templates   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                summary  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Scaling test-time computation, or affording a generator large language model\\n(LLM) extra compute during inference, typically employs the help of external\\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\\nmodels trained to generate evaluations and critiques (explanations) in natural\\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\\njudge empirical successes, their effectiveness as evaluators in test-time\\nscaling settings is largely unknown. In this paper, we introduce the Judge\\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\\nperformance in three domains (math reasoning, code generation, and instruction\\nfollowing) under three task settings: response reranking, step-level beam\\nsearch, and critique-based response refinement. We evaluate 10 different judge\\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\\nparameters). Our benchmark shows that while judges are competitive with outcome\\nreward models in reranking, they are consistently worse than process reward\\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\\ntheir natural language critiques are currently ineffective in guiding the\\ngenerator towards better responses.   \n",
      "1                                                                                                        In this work an analysis of the intracluster light (ICL) in the galaxy\\ncluster SMACS J0723.3-7327 (hereafter, SMACS J0723) using JWST/NIRCam deep\\nimaging in six filters (F090W to F444W) is presented. The images were processed\\nfor low surface brightness (LSB) science, with additional correction for\\ninstrumental scattering in the short-wavelength channels, and analysed using\\nwavelet-based decomposition. The ICL, brightest cluster galaxy (BCG), and\\nsatellite galaxies were extracted and modelled, with 2D maps for each\\ncomponent. ICL and ICL+BCG fractions, computed across all filters within a 400\\nkpc radius, exhibit a flat trend with wavelength, averaging 28% and 34%,\\nrespectively. Flux ratios between the BCG and the next brightest members\\n(M$_{12}$, M$_{13}$ and M$_{14}$) also display minimal wavelength dependence.\\nThese results indicate that SMACS J0723 is a dynamically evolved cluster with a\\ndominant BCG and well-developed ICL. Five prominent ICL substructures are\\nanalysed, contributing to 10-12% of the total ICL+BCG flux budget, slightly\\nexceeding simulation predictions. Their short dynamical timescales suggest an\\ninstantaneous ICL injection rate of several $10^3 L_{\\odot}\\,{\\rm yr}^{-1}$,\\nconsistent with active dynamical assembly. These findings support a scenario\\nwhere SMACS J0723's ICL growth is currently driven by galaxy mergers involving\\nthe BCG and other bright satellites, rather than by the accretion of\\npre-processed ICL from a recent cluster merger. However, extrapolating the\\ncurrent injection rate to the cluster's lifetime indicates that additional\\nmechanisms are required to match the growth observed in other clusters over\\ncosmic times.   \n",
      "2                                                                                                                                                                                                                                                                                                                    Intermittent renewable energies are increasingly dominating electricity grids\\nand are forecasted to be the main force driving out fossil fuels from the grid\\nin most major economies until 2040. However, grids based on intermittent\\nrenewables are challenged by diurnal and seasonal mismatch between supply of\\nsun and wind and demand for electricity, including for heat pumps and electric\\ntwo and four wheelers. Load management and demand response measures promise to\\nadjust for this mismatch, utilizing information- and price-based approaches to\\nsteer demand towards times with high supply of intermittent renewables. Here,\\nwe systematically review the literature estimating CO2 savings from residential\\nload management in developing and developed nations. We find that load\\nmanagement holds high potential, locally differentiated with energy mix\\n(including the respective share of renewables and fossils), climate zone, and\\nthe regulatory environment and price mechanism. Most identified studies suggest\\na mitigation potential between 1 and 20%. Load management becomes more relevant\\nwith higher shares of intermittent renewables, and when electricity prices are\\nhigh. Importantly, load management aligns consumers' financial incentives with\\nclimate change mitigation, thus rendering accompanying strategies politically\\nfeasible. We summarize key regulatory steps to facilitate load management in\\neconomies and to realize relevant consumer surplus and mitigation potential.   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Medium access in 5G systems was tailored to accommodate diverse traffic\\nclasses through network resource slicing. 6G wireless systems are expected to\\nbe significantly reliant on Artificial Intelligence (AI), leading to\\ndata-driven and goal-oriented communication. This leads to augmentation of the\\ndesign space for Medium Access Control (MAC) protocols, which is the focus of\\nthis article. We introduce a taxonomy based on push-based and pull-based\\ncommunication, which is useful to categorize both the legacy and the AI-driven\\naccess schemes. We provide MAC protocol design guidelines for pull- and\\npush-based communication in terms of goal-oriented criteria, such as timing and\\ndata relevance. We articulate a framework for co-existence between pull and\\npush-based communications in 6G systems, combining their advantages. We\\nhighlight the design principles and main tradeoffs, as well as the\\narchitectural considerations for integrating these designs in Open-Radio Access\\nNetwork (O-RAN) and 6G systems.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     We investigate the dynamics of a massive tracer particle coupled to an\\ninteracting active bath, modeled as a harmonic chain of overdamped active\\nparticles analytically, with an aim to understand the impact of bath\\ninteractions and activity on the nonequilibrium fluctuations of the tracer.\\nFrom the microscopic equations, we derive the tracer particle's effective\\nLangevin equation, obtaining the dissipative and stochastic forces from the\\nbath. We analyze the friction kernel, revealing power-law tails in the weak\\ncoupling limit and exponential decay in the strong coupling regime. Due to the\\ninterplay between bath interactions, probe-bath coupling, and activity, the\\nmean squared displacement, velocity, and stationary velocity correlations\\nexhibit different dynamical regimes, which we characterize analytically. Under\\nharmonic confinement, we find that energy equipartition holds at low activity\\nbut breaks down at higher activity, with the kinetic energy exhibiting a\\nnon-monotonic dependence on the activity of the bath.   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       We study how to use naturally available user feedback, such as clicks, to\\noptimize large language model (LLM) pipelines for generating personalized\\nsentences using prompts. Naive approaches, which estimate the policy gradient\\nin the prompt space, suffer either from variance caused by the large action\\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\\nthese challenges, we propose a novel kernel-based off-policy gradient method,\\nwhich estimates the policy gradient by leveraging similarity among generated\\nsentences, substantially reducing variance while suppressing the bias.\\nEmpirical results on our newly established suite of benchmarks demonstrate the\\neffectiveness of the proposed approach in generating personalized descriptions\\nfor movie recommendations, particularly when the number of candidate prompts is\\nlarge.   \n",
      "6                                                                                                                                                                                                                                                                                                                                 Deuteration of hydrocarbon material, including polycyclic aromatic\\nhydrocarbons (PAHs), has been proposed to account for the low gas-phase\\nabundances of D in the interstellar medium. JWST spectra of four star-forming\\nregions in M51 show an emission feature, with central wavelength\\n$\\sim$4.647$\\mu$m and FWHM 0.0265$\\mu$m, corresponding to the C-D stretching\\nmode in aliphatic hydrocarbons. The emitting aliphatic material is estimated to\\nhave (D/H)$_{\\rm aliph}\\approx 0.17\\pm0.02$ -- a factor $\\sim$$10^4$ enrichment\\nrelative to the overall interstellar medium (ISM). On $\\sim$$50\\,$pc scales,\\ndeuteration levels toward four H$\\,$II regions in M51 are 2-3 times higher than\\nin the Orion Bar photodissociation region (PDR), with implications for the\\nprocesses responsible for the formation and evolution of hydrocarbon\\nnanoparticles, including PAHs. The deuteration of the aliphatic material is\\nfound to anticorrelate with helium ionization in the associated H$\\,$II,\\nsuggesting that harsh FUV radiation may act to lower the deuteration of\\naliphatics in PDRs near massive stars. No evidence is found for deuteration of\\naromatic material, with (D/H)$_{\\rm arom} \\lesssim 0.016$: deuteration of the\\naliphatic material exceeds that of the aromatic material by at least a factor\\n10. The observed levels of deuteration may account for the depletion of D\\nobserved in the Galactic interstellar medium. If so, the $4.65\\mu$m feature may\\nbe detectable in absorption.   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Differentially private (DP) image synthesis aims to generate synthetic images\\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\\norganizations sharing and utilizing synthetic images. Although previous methods\\nhave significantly progressed, especially in training diffusion models on\\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\\nstage in the beginning, where diffusion models learn simple features of the\\nsensitive images. To facilitate this easy stage, we propose to use `central\\nimages', simply aggregations of random samples of the sensitive dataset.\\nIntuitively, although those central images do not show details, they\\ndemonstrate useful characteristics of all images and only incur minimal privacy\\ncosts, thus helping early-phase model training. We conduct experiments to\\npresent that on the average of four investigated image datasets, the fidelity\\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\\nstate-of-the-art method.   \n",
      "8  In the present work, we aim to restore Book X of the $\\it{Elements}$ to its\\noriginal Theaetetean, pre-Eudoxean form in two separate ways. First, we restore\\nthe considerable mathematical content of Book X, by correlating Book X with\\nPlato's account of Theaetetus' mathematical discoveries and Plato's imitations\\nof these discoveries for his philosophy. Thus, Theaetetus proved (i) the\\neventual periodicity of the anthyphairesis of lines a to b, satisfying $Ma^2 =\\nNb^2$, for MN not square number; (ii) the eventual periodic anthyphairesis of\\nlines a to b, satisfying more general quadratic expressions, including the\\nApplication of Areas in defect, and employing this to show that the 12 classes\\nof alogoi lines, including the minor, despite being alogoi, are determined by\\nan eventually periodic Application of Areas in defect; (iii) the anthyphairetic\\npalindromic periodicity of the anthyphairesis of the surds $\\sqrt{N}$ for any\\nnon-square number N, of relevance to the general Pell's Diophantine problem.\\nSecondly, we restore the proofs of all propositions of Book X, in such way that\\nthese are proofs based on Theaetetus', and not on Eudoxus' theory of proportion\\nof magnitudes, in particular not making any use of Eudoxus' condition (namely\\nof definition 4 of Book V). The restoration is based on our reconstruction of\\nTheaetetus' theory of proportion for magnitudes, for the limited class of\\nratios a/b such that either a, b are commensurable or the anthyphairesis of a\\nto b is eventually periodic, without employing Eudoxus' condition, and its\\nsuccess provides a confirmation of our reconstruction.\\n  The final version of this paper will appear as a chapter in the journal\\nGanita Bh\\=arat\\=i, Bulletin of the Indian Society for History of Mathematics,\\n(2) 45 (2023).   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            This paper presents (permissive) \\emph{Quantitative Strategy Templates}\\n(QaSTels) to succinctly represent infinitely many winning strategies in\\ntwo-player energy and mean-payoff games. This transfers the recently introduced\\nconcept of \\emph{Permissive (qualitative) Strategy Templates} (PeSTels) for\\n$\\omega$-regular games to games with quantitative objectives. We provide the\\ntheoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their\\n(incremental) combination with PeSTels for games with mixed quantitative and\\nqualitative objectives. Using a prototype implementation of our synthesis\\nalgorithms, we demonstrate empirically that QaSTels extend the advantageous\\nproperties of strategy templates over single winning strategies -- known from\\nPeSTels -- to games with (additional) quantitative objectives. This includes\\n(i) the enhanced robustness of strategies due to their runtime-adaptability,\\nand (ii) the compositionality of templates w.r.t. incrementally arriving\\nobjectives. We use control-inspired examples to illustrate these superior\\nproperties of QaSTels for CPS design.   \n",
      "\n",
      "                         categories  \n",
      "0                       cs.CL,cs.LG  \n",
      "1                       astro-ph.GA  \n",
      "2                             cs.IR  \n",
      "3                             cs.NI  \n",
      "4  cond-mat.stat-mech,cond-mat.soft  \n",
      "5         cs.LG,cs.AI,cs.IR,stat.ML  \n",
      "6                       astro-ph.GA  \n",
      "7                       cs.CR,cs.AI  \n",
      "8                           math.HO  \n",
      "9                             cs.GT  \n"
     ]
    }
   ],
   "source": [
    "df_summary_title = df.loc[:, [\"title\",\"summary\", \"categories\"]].copy()\n",
    "print(df_summary_title.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89ff7ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       combined\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\\n  Test-Time Scaling Evaluators [SUMMARY] Scaling test-time computation, or affording a generator large language model\\n(LLM) extra compute during inference, typically employs the help of external\\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\\nmodels trained to generate evaluations and critiques (explanations) in natural\\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\\njudge empirical successes, their effectiveness as evaluators in test-time\\nscaling settings is largely unknown. In this paper, we introduce the Judge\\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\\nperformance in three domains (math reasoning, code generation, and instruction\\nfollowing) under three task settings: response reranking, step-level beam\\nsearch, and critique-based response refinement. We evaluate 10 different judge\\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\\nparameters). Our benchmark shows that while judges are competitive with outcome\\nreward models in reranking, they are consistently worse than process reward\\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\\ntheir natural language critiques are currently ineffective in guiding the\\ngenerator towards better responses.\n",
      "1                                                                                     Multiscale exploration of SMACS J0723.3--7327's intracluster light and\\n  past dynamical history [SUMMARY] In this work an analysis of the intracluster light (ICL) in the galaxy\\ncluster SMACS J0723.3-7327 (hereafter, SMACS J0723) using JWST/NIRCam deep\\nimaging in six filters (F090W to F444W) is presented. The images were processed\\nfor low surface brightness (LSB) science, with additional correction for\\ninstrumental scattering in the short-wavelength channels, and analysed using\\nwavelet-based decomposition. The ICL, brightest cluster galaxy (BCG), and\\nsatellite galaxies were extracted and modelled, with 2D maps for each\\ncomponent. ICL and ICL+BCG fractions, computed across all filters within a 400\\nkpc radius, exhibit a flat trend with wavelength, averaging 28% and 34%,\\nrespectively. Flux ratios between the BCG and the next brightest members\\n(M$_{12}$, M$_{13}$ and M$_{14}$) also display minimal wavelength dependence.\\nThese results indicate that SMACS J0723 is a dynamically evolved cluster with a\\ndominant BCG and well-developed ICL. Five prominent ICL substructures are\\nanalysed, contributing to 10-12% of the total ICL+BCG flux budget, slightly\\nexceeding simulation predictions. Their short dynamical timescales suggest an\\ninstantaneous ICL injection rate of several $10^3 L_{\\odot}\\,{\\rm yr}^{-1}$,\\nconsistent with active dynamical assembly. These findings support a scenario\\nwhere SMACS J0723's ICL growth is currently driven by galaxy mergers involving\\nthe BCG and other bright satellites, rather than by the accretion of\\npre-processed ICL from a recent cluster merger. However, extrapolating the\\ncurrent injection rate to the cluster's lifetime indicates that additional\\nmechanisms are required to match the growth observed in other clusters over\\ncosmic times.\n",
      "2                                                                                                                                                                                                                                                                    An Assessment of the CO2 Emission Reduction Potential of Residential\\n  Load Management in Developing and Developed Countries [SUMMARY] Intermittent renewable energies are increasingly dominating electricity grids\\nand are forecasted to be the main force driving out fossil fuels from the grid\\nin most major economies until 2040. However, grids based on intermittent\\nrenewables are challenged by diurnal and seasonal mismatch between supply of\\nsun and wind and demand for electricity, including for heat pumps and electric\\ntwo and four wheelers. Load management and demand response measures promise to\\nadjust for this mismatch, utilizing information- and price-based approaches to\\nsteer demand towards times with high supply of intermittent renewables. Here,\\nwe systematically review the literature estimating CO2 savings from residential\\nload management in developing and developed nations. We find that load\\nmanagement holds high potential, locally differentiated with energy mix\\n(including the respective share of renewables and fossils), climate zone, and\\nthe regulatory environment and price mechanism. Most identified studies suggest\\na mitigation potential between 1 and 20%. Load management becomes more relevant\\nwith higher shares of intermittent renewables, and when electricity prices are\\nhigh. Importantly, load management aligns consumers' financial incentives with\\nclimate change mitigation, thus rendering accompanying strategies politically\\nfeasible. We summarize key regulatory steps to facilitate load management in\\neconomies and to realize relevant consumer surplus and mitigation potential.\n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Medium Access for Push-Pull Data Transmission in 6G Wireless Systems [SUMMARY] Medium access in 5G systems was tailored to accommodate diverse traffic\\nclasses through network resource slicing. 6G wireless systems are expected to\\nbe significantly reliant on Artificial Intelligence (AI), leading to\\ndata-driven and goal-oriented communication. This leads to augmentation of the\\ndesign space for Medium Access Control (MAC) protocols, which is the focus of\\nthis article. We introduce a taxonomy based on push-based and pull-based\\ncommunication, which is useful to categorize both the legacy and the AI-driven\\naccess schemes. We provide MAC protocol design guidelines for pull- and\\npush-based communication in terms of goal-oriented criteria, such as timing and\\ndata relevance. We articulate a framework for co-existence between pull and\\npush-based communications in 6G systems, combining their advantages. We\\nhighlight the design principles and main tradeoffs, as well as the\\narchitectural considerations for integrating these designs in Open-Radio Access\\nNetwork (O-RAN) and 6G systems.\n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tracer dynamics in an interacting active bath: fluctuations and energy\\n  partition [SUMMARY] We investigate the dynamics of a massive tracer particle coupled to an\\ninteracting active bath, modeled as a harmonic chain of overdamped active\\nparticles analytically, with an aim to understand the impact of bath\\ninteractions and activity on the nonequilibrium fluctuations of the tracer.\\nFrom the microscopic equations, we derive the tracer particle's effective\\nLangevin equation, obtaining the dissipative and stochastic forces from the\\nbath. We analyze the friction kernel, revealing power-law tails in the weak\\ncoupling limit and exponential decay in the strong coupling regime. Due to the\\ninterplay between bath interactions, probe-bath coupling, and activity, the\\nmean squared displacement, velocity, and stationary velocity correlations\\nexhibit different dynamical regimes, which we characterize analytically. Under\\nharmonic confinement, we find that energy equipartition holds at low activity\\nbut breaks down at higher activity, with the kinetic energy exhibiting a\\nnon-monotonic dependence on the activity of the bath.\n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Prompt Optimization with Logged Bandit Data [SUMMARY] We study how to use naturally available user feedback, such as clicks, to\\noptimize large language model (LLM) pipelines for generating personalized\\nsentences using prompts. Naive approaches, which estimate the policy gradient\\nin the prompt space, suffer either from variance caused by the large action\\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\\nthese challenges, we propose a novel kernel-based off-policy gradient method,\\nwhich estimates the policy gradient by leveraging similarity among generated\\nsentences, substantially reducing variance while suppressing the bias.\\nEmpirical results on our newly established suite of benchmarks demonstrate the\\neffectiveness of the proposed approach in generating personalized descriptions\\nfor movie recommendations, particularly when the number of candidate prompts is\\nlarge.\n",
      "6                                                                                                                                                                                                                                                                                                                             Detection of Deuterated Hydrocarbon Nanoparticles in the Whirlpool\\n  Galaxy, M51 [SUMMARY] Deuteration of hydrocarbon material, including polycyclic aromatic\\nhydrocarbons (PAHs), has been proposed to account for the low gas-phase\\nabundances of D in the interstellar medium. JWST spectra of four star-forming\\nregions in M51 show an emission feature, with central wavelength\\n$\\sim$4.647$\\mu$m and FWHM 0.0265$\\mu$m, corresponding to the C-D stretching\\nmode in aliphatic hydrocarbons. The emitting aliphatic material is estimated to\\nhave (D/H)$_{\\rm aliph}\\approx 0.17\\pm0.02$ -- a factor $\\sim$$10^4$ enrichment\\nrelative to the overall interstellar medium (ISM). On $\\sim$$50\\,$pc scales,\\ndeuteration levels toward four H$\\,$II regions in M51 are 2-3 times higher than\\nin the Orion Bar photodissociation region (PDR), with implications for the\\nprocesses responsible for the formation and evolution of hydrocarbon\\nnanoparticles, including PAHs. The deuteration of the aliphatic material is\\nfound to anticorrelate with helium ionization in the associated H$\\,$II,\\nsuggesting that harsh FUV radiation may act to lower the deuteration of\\naliphatics in PDRs near massive stars. No evidence is found for deuteration of\\naromatic material, with (D/H)$_{\\rm arom} \\lesssim 0.016$: deuteration of the\\naliphatic material exceeds that of the aromatic material by at least a factor\\n10. The observed levels of deuteration may account for the depletion of D\\nobserved in the Galactic interstellar medium. If so, the $4.65\\mu$m feature may\\nbe detectable in absorption.\n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                      From Easy to Hard: Building a Shortcut for Differentially Private Image\\n  Synthesis [SUMMARY] Differentially private (DP) image synthesis aims to generate synthetic images\\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\\norganizations sharing and utilizing synthetic images. Although previous methods\\nhave significantly progressed, especially in training diffusion models on\\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\\nstage in the beginning, where diffusion models learn simple features of the\\nsensitive images. To facilitate this easy stage, we propose to use `central\\nimages', simply aggregations of random samples of the sensitive dataset.\\nIntuitively, although those central images do not show details, they\\ndemonstrate useful characteristics of all images and only incur minimal privacy\\ncosts, thus helping early-phase model training. We conduct experiments to\\npresent that on the average of four investigated image datasets, the fidelity\\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\\nstate-of-the-art method.\n",
      "8  The restoration of Book X of the Elements to its original Theaetetean\\n  form [SUMMARY] In the present work, we aim to restore Book X of the $\\it{Elements}$ to its\\noriginal Theaetetean, pre-Eudoxean form in two separate ways. First, we restore\\nthe considerable mathematical content of Book X, by correlating Book X with\\nPlato's account of Theaetetus' mathematical discoveries and Plato's imitations\\nof these discoveries for his philosophy. Thus, Theaetetus proved (i) the\\neventual periodicity of the anthyphairesis of lines a to b, satisfying $Ma^2 =\\nNb^2$, for MN not square number; (ii) the eventual periodic anthyphairesis of\\nlines a to b, satisfying more general quadratic expressions, including the\\nApplication of Areas in defect, and employing this to show that the 12 classes\\nof alogoi lines, including the minor, despite being alogoi, are determined by\\nan eventually periodic Application of Areas in defect; (iii) the anthyphairetic\\npalindromic periodicity of the anthyphairesis of the surds $\\sqrt{N}$ for any\\nnon-square number N, of relevance to the general Pell's Diophantine problem.\\nSecondly, we restore the proofs of all propositions of Book X, in such way that\\nthese are proofs based on Theaetetus', and not on Eudoxus' theory of proportion\\nof magnitudes, in particular not making any use of Eudoxus' condition (namely\\nof definition 4 of Book V). The restoration is based on our reconstruction of\\nTheaetetus' theory of proportion for magnitudes, for the limited class of\\nratios a/b such that either a, b are commensurable or the anthyphairesis of a\\nto b is eventually periodic, without employing Eudoxus' condition, and its\\nsuccess provides a confirmation of our reconstruction.\\n  The final version of this paper will appear as a chapter in the journal\\nGanita Bh\\=arat\\=i, Bulletin of the Indian Society for History of Mathematics,\\n(2) 45 (2023).\n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Quantitative Strategy Templates [SUMMARY] This paper presents (permissive) \\emph{Quantitative Strategy Templates}\\n(QaSTels) to succinctly represent infinitely many winning strategies in\\ntwo-player energy and mean-payoff games. This transfers the recently introduced\\nconcept of \\emph{Permissive (qualitative) Strategy Templates} (PeSTels) for\\n$\\omega$-regular games to games with quantitative objectives. We provide the\\ntheoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their\\n(incremental) combination with PeSTels for games with mixed quantitative and\\nqualitative objectives. Using a prototype implementation of our synthesis\\nalgorithms, we demonstrate empirically that QaSTels extend the advantageous\\nproperties of strategy templates over single winning strategies -- known from\\nPeSTels -- to games with (additional) quantitative objectives. This includes\\n(i) the enhanced robustness of strategies due to their runtime-adaptability,\\nand (ii) the compositionality of templates w.r.t. incrementally arriving\\nobjectives. We use control-inspired examples to illustrate these superior\\nproperties of QaSTels for CPS design.\n"
     ]
    }
   ],
   "source": [
    "df['combined'] = df['title'] + \" [SUMMARY] \" + df['summary']\n",
    "print(df[['combined']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86f3ec33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       combined  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\\n  Test-Time Scaling Evaluators [SUMMARY] Scaling test-time computation, or affording a generator large language model\\n(LLM) extra compute during inference, typically employs the help of external\\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\\nmodels trained to generate evaluations and critiques (explanations) in natural\\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\\njudge empirical successes, their effectiveness as evaluators in test-time\\nscaling settings is largely unknown. In this paper, we introduce the Judge\\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\\nperformance in three domains (math reasoning, code generation, and instruction\\nfollowing) under three task settings: response reranking, step-level beam\\nsearch, and critique-based response refinement. We evaluate 10 different judge\\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\\nparameters). Our benchmark shows that while judges are competitive with outcome\\nreward models in reranking, they are consistently worse than process reward\\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\\ntheir natural language critiques are currently ineffective in guiding the\\ngenerator towards better responses.   \n",
      "1                                                                                     Multiscale exploration of SMACS J0723.3--7327's intracluster light and\\n  past dynamical history [SUMMARY] In this work an analysis of the intracluster light (ICL) in the galaxy\\ncluster SMACS J0723.3-7327 (hereafter, SMACS J0723) using JWST/NIRCam deep\\nimaging in six filters (F090W to F444W) is presented. The images were processed\\nfor low surface brightness (LSB) science, with additional correction for\\ninstrumental scattering in the short-wavelength channels, and analysed using\\nwavelet-based decomposition. The ICL, brightest cluster galaxy (BCG), and\\nsatellite galaxies were extracted and modelled, with 2D maps for each\\ncomponent. ICL and ICL+BCG fractions, computed across all filters within a 400\\nkpc radius, exhibit a flat trend with wavelength, averaging 28% and 34%,\\nrespectively. Flux ratios between the BCG and the next brightest members\\n(M$_{12}$, M$_{13}$ and M$_{14}$) also display minimal wavelength dependence.\\nThese results indicate that SMACS J0723 is a dynamically evolved cluster with a\\ndominant BCG and well-developed ICL. Five prominent ICL substructures are\\nanalysed, contributing to 10-12% of the total ICL+BCG flux budget, slightly\\nexceeding simulation predictions. Their short dynamical timescales suggest an\\ninstantaneous ICL injection rate of several $10^3 L_{\\odot}\\,{\\rm yr}^{-1}$,\\nconsistent with active dynamical assembly. These findings support a scenario\\nwhere SMACS J0723's ICL growth is currently driven by galaxy mergers involving\\nthe BCG and other bright satellites, rather than by the accretion of\\npre-processed ICL from a recent cluster merger. However, extrapolating the\\ncurrent injection rate to the cluster's lifetime indicates that additional\\nmechanisms are required to match the growth observed in other clusters over\\ncosmic times.   \n",
      "2                                                                                                                                                                                                                                                                    An Assessment of the CO2 Emission Reduction Potential of Residential\\n  Load Management in Developing and Developed Countries [SUMMARY] Intermittent renewable energies are increasingly dominating electricity grids\\nand are forecasted to be the main force driving out fossil fuels from the grid\\nin most major economies until 2040. However, grids based on intermittent\\nrenewables are challenged by diurnal and seasonal mismatch between supply of\\nsun and wind and demand for electricity, including for heat pumps and electric\\ntwo and four wheelers. Load management and demand response measures promise to\\nadjust for this mismatch, utilizing information- and price-based approaches to\\nsteer demand towards times with high supply of intermittent renewables. Here,\\nwe systematically review the literature estimating CO2 savings from residential\\nload management in developing and developed nations. We find that load\\nmanagement holds high potential, locally differentiated with energy mix\\n(including the respective share of renewables and fossils), climate zone, and\\nthe regulatory environment and price mechanism. Most identified studies suggest\\na mitigation potential between 1 and 20%. Load management becomes more relevant\\nwith higher shares of intermittent renewables, and when electricity prices are\\nhigh. Importantly, load management aligns consumers' financial incentives with\\nclimate change mitigation, thus rendering accompanying strategies politically\\nfeasible. We summarize key regulatory steps to facilitate load management in\\neconomies and to realize relevant consumer surplus and mitigation potential.   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Medium Access for Push-Pull Data Transmission in 6G Wireless Systems [SUMMARY] Medium access in 5G systems was tailored to accommodate diverse traffic\\nclasses through network resource slicing. 6G wireless systems are expected to\\nbe significantly reliant on Artificial Intelligence (AI), leading to\\ndata-driven and goal-oriented communication. This leads to augmentation of the\\ndesign space for Medium Access Control (MAC) protocols, which is the focus of\\nthis article. We introduce a taxonomy based on push-based and pull-based\\ncommunication, which is useful to categorize both the legacy and the AI-driven\\naccess schemes. We provide MAC protocol design guidelines for pull- and\\npush-based communication in terms of goal-oriented criteria, such as timing and\\ndata relevance. We articulate a framework for co-existence between pull and\\npush-based communications in 6G systems, combining their advantages. We\\nhighlight the design principles and main tradeoffs, as well as the\\narchitectural considerations for integrating these designs in Open-Radio Access\\nNetwork (O-RAN) and 6G systems.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tracer dynamics in an interacting active bath: fluctuations and energy\\n  partition [SUMMARY] We investigate the dynamics of a massive tracer particle coupled to an\\ninteracting active bath, modeled as a harmonic chain of overdamped active\\nparticles analytically, with an aim to understand the impact of bath\\ninteractions and activity on the nonequilibrium fluctuations of the tracer.\\nFrom the microscopic equations, we derive the tracer particle's effective\\nLangevin equation, obtaining the dissipative and stochastic forces from the\\nbath. We analyze the friction kernel, revealing power-law tails in the weak\\ncoupling limit and exponential decay in the strong coupling regime. Due to the\\ninterplay between bath interactions, probe-bath coupling, and activity, the\\nmean squared displacement, velocity, and stationary velocity correlations\\nexhibit different dynamical regimes, which we characterize analytically. Under\\nharmonic confinement, we find that energy equipartition holds at low activity\\nbut breaks down at higher activity, with the kinetic energy exhibiting a\\nnon-monotonic dependence on the activity of the bath.   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Prompt Optimization with Logged Bandit Data [SUMMARY] We study how to use naturally available user feedback, such as clicks, to\\noptimize large language model (LLM) pipelines for generating personalized\\nsentences using prompts. Naive approaches, which estimate the policy gradient\\nin the prompt space, suffer either from variance caused by the large action\\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\\nthese challenges, we propose a novel kernel-based off-policy gradient method,\\nwhich estimates the policy gradient by leveraging similarity among generated\\nsentences, substantially reducing variance while suppressing the bias.\\nEmpirical results on our newly established suite of benchmarks demonstrate the\\neffectiveness of the proposed approach in generating personalized descriptions\\nfor movie recommendations, particularly when the number of candidate prompts is\\nlarge.   \n",
      "6                                                                                                                                                                                                                                                                                                                             Detection of Deuterated Hydrocarbon Nanoparticles in the Whirlpool\\n  Galaxy, M51 [SUMMARY] Deuteration of hydrocarbon material, including polycyclic aromatic\\nhydrocarbons (PAHs), has been proposed to account for the low gas-phase\\nabundances of D in the interstellar medium. JWST spectra of four star-forming\\nregions in M51 show an emission feature, with central wavelength\\n$\\sim$4.647$\\mu$m and FWHM 0.0265$\\mu$m, corresponding to the C-D stretching\\nmode in aliphatic hydrocarbons. The emitting aliphatic material is estimated to\\nhave (D/H)$_{\\rm aliph}\\approx 0.17\\pm0.02$ -- a factor $\\sim$$10^4$ enrichment\\nrelative to the overall interstellar medium (ISM). On $\\sim$$50\\,$pc scales,\\ndeuteration levels toward four H$\\,$II regions in M51 are 2-3 times higher than\\nin the Orion Bar photodissociation region (PDR), with implications for the\\nprocesses responsible for the formation and evolution of hydrocarbon\\nnanoparticles, including PAHs. The deuteration of the aliphatic material is\\nfound to anticorrelate with helium ionization in the associated H$\\,$II,\\nsuggesting that harsh FUV radiation may act to lower the deuteration of\\naliphatics in PDRs near massive stars. No evidence is found for deuteration of\\naromatic material, with (D/H)$_{\\rm arom} \\lesssim 0.016$: deuteration of the\\naliphatic material exceeds that of the aromatic material by at least a factor\\n10. The observed levels of deuteration may account for the depletion of D\\nobserved in the Galactic interstellar medium. If so, the $4.65\\mu$m feature may\\nbe detectable in absorption.   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                      From Easy to Hard: Building a Shortcut for Differentially Private Image\\n  Synthesis [SUMMARY] Differentially private (DP) image synthesis aims to generate synthetic images\\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\\norganizations sharing and utilizing synthetic images. Although previous methods\\nhave significantly progressed, especially in training diffusion models on\\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\\nstage in the beginning, where diffusion models learn simple features of the\\nsensitive images. To facilitate this easy stage, we propose to use `central\\nimages', simply aggregations of random samples of the sensitive dataset.\\nIntuitively, although those central images do not show details, they\\ndemonstrate useful characteristics of all images and only incur minimal privacy\\ncosts, thus helping early-phase model training. We conduct experiments to\\npresent that on the average of four investigated image datasets, the fidelity\\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\\nstate-of-the-art method.   \n",
      "8  The restoration of Book X of the Elements to its original Theaetetean\\n  form [SUMMARY] In the present work, we aim to restore Book X of the $\\it{Elements}$ to its\\noriginal Theaetetean, pre-Eudoxean form in two separate ways. First, we restore\\nthe considerable mathematical content of Book X, by correlating Book X with\\nPlato's account of Theaetetus' mathematical discoveries and Plato's imitations\\nof these discoveries for his philosophy. Thus, Theaetetus proved (i) the\\neventual periodicity of the anthyphairesis of lines a to b, satisfying $Ma^2 =\\nNb^2$, for MN not square number; (ii) the eventual periodic anthyphairesis of\\nlines a to b, satisfying more general quadratic expressions, including the\\nApplication of Areas in defect, and employing this to show that the 12 classes\\nof alogoi lines, including the minor, despite being alogoi, are determined by\\nan eventually periodic Application of Areas in defect; (iii) the anthyphairetic\\npalindromic periodicity of the anthyphairesis of the surds $\\sqrt{N}$ for any\\nnon-square number N, of relevance to the general Pell's Diophantine problem.\\nSecondly, we restore the proofs of all propositions of Book X, in such way that\\nthese are proofs based on Theaetetus', and not on Eudoxus' theory of proportion\\nof magnitudes, in particular not making any use of Eudoxus' condition (namely\\nof definition 4 of Book V). The restoration is based on our reconstruction of\\nTheaetetus' theory of proportion for magnitudes, for the limited class of\\nratios a/b such that either a, b are commensurable or the anthyphairesis of a\\nto b is eventually periodic, without employing Eudoxus' condition, and its\\nsuccess provides a confirmation of our reconstruction.\\n  The final version of this paper will appear as a chapter in the journal\\nGanita Bh\\=arat\\=i, Bulletin of the Indian Society for History of Mathematics,\\n(2) 45 (2023).   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Quantitative Strategy Templates [SUMMARY] This paper presents (permissive) \\emph{Quantitative Strategy Templates}\\n(QaSTels) to succinctly represent infinitely many winning strategies in\\ntwo-player energy and mean-payoff games. This transfers the recently introduced\\nconcept of \\emph{Permissive (qualitative) Strategy Templates} (PeSTels) for\\n$\\omega$-regular games to games with quantitative objectives. We provide the\\ntheoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their\\n(incremental) combination with PeSTels for games with mixed quantitative and\\nqualitative objectives. Using a prototype implementation of our synthesis\\nalgorithms, we demonstrate empirically that QaSTels extend the advantageous\\nproperties of strategy templates over single winning strategies -- known from\\nPeSTels -- to games with (additional) quantitative objectives. This includes\\n(i) the enhanced robustness of strategies due to their runtime-adaptability,\\nand (ii) the compositionality of templates w.r.t. incrementally arriving\\nobjectives. We use control-inspired examples to illustrate these superior\\nproperties of QaSTels for CPS design.   \n",
      "\n",
      "        main_category                        categories  \n",
      "0               cs.CL                       cs.CL,cs.LG  \n",
      "1         astro-ph.GA                       astro-ph.GA  \n",
      "2               cs.IR                             cs.IR  \n",
      "3               cs.NI                             cs.NI  \n",
      "4  cond-mat.stat-mech  cond-mat.stat-mech,cond-mat.soft  \n",
      "5               cs.LG         cs.LG,cs.AI,cs.IR,stat.ML  \n",
      "6         astro-ph.GA                       astro-ph.GA  \n",
      "7               cs.CR                       cs.CR,cs.AI  \n",
      "8             math.HO                           math.HO  \n",
      "9               cs.GT                             cs.GT  \n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       combined  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as\\n  Test-Time Scaling Evaluators [SUMMARY] Scaling test-time computation, or affording a generator large language model\\n(LLM) extra compute during inference, typically employs the help of external\\nnon-generative evaluators (i.e., reward models). Concurrently, LLM-judges,\\nmodels trained to generate evaluations and critiques (explanations) in natural\\nlanguage, are becoming increasingly popular in automatic evaluation. Despite\\njudge empirical successes, their effectiveness as evaluators in test-time\\nscaling settings is largely unknown. In this paper, we introduce the Judge\\nEvaluation for Test-Time Scaling (JETTS) benchmark, which evaluates judge\\nperformance in three domains (math reasoning, code generation, and instruction\\nfollowing) under three task settings: response reranking, step-level beam\\nsearch, and critique-based response refinement. We evaluate 10 different judge\\nmodels (7B-70B parameters) for 8 different base generator models (6.7B-72B\\nparameters). Our benchmark shows that while judges are competitive with outcome\\nreward models in reranking, they are consistently worse than process reward\\nmodels in beam search procedures. Furthermore, though unique to LLM-judges,\\ntheir natural language critiques are currently ineffective in guiding the\\ngenerator towards better responses.   \n",
      "1                                                                                     Multiscale exploration of SMACS J0723.3--7327's intracluster light and\\n  past dynamical history [SUMMARY] In this work an analysis of the intracluster light (ICL) in the galaxy\\ncluster SMACS J0723.3-7327 (hereafter, SMACS J0723) using JWST/NIRCam deep\\nimaging in six filters (F090W to F444W) is presented. The images were processed\\nfor low surface brightness (LSB) science, with additional correction for\\ninstrumental scattering in the short-wavelength channels, and analysed using\\nwavelet-based decomposition. The ICL, brightest cluster galaxy (BCG), and\\nsatellite galaxies were extracted and modelled, with 2D maps for each\\ncomponent. ICL and ICL+BCG fractions, computed across all filters within a 400\\nkpc radius, exhibit a flat trend with wavelength, averaging 28% and 34%,\\nrespectively. Flux ratios between the BCG and the next brightest members\\n(M$_{12}$, M$_{13}$ and M$_{14}$) also display minimal wavelength dependence.\\nThese results indicate that SMACS J0723 is a dynamically evolved cluster with a\\ndominant BCG and well-developed ICL. Five prominent ICL substructures are\\nanalysed, contributing to 10-12% of the total ICL+BCG flux budget, slightly\\nexceeding simulation predictions. Their short dynamical timescales suggest an\\ninstantaneous ICL injection rate of several $10^3 L_{\\odot}\\,{\\rm yr}^{-1}$,\\nconsistent with active dynamical assembly. These findings support a scenario\\nwhere SMACS J0723's ICL growth is currently driven by galaxy mergers involving\\nthe BCG and other bright satellites, rather than by the accretion of\\npre-processed ICL from a recent cluster merger. However, extrapolating the\\ncurrent injection rate to the cluster's lifetime indicates that additional\\nmechanisms are required to match the growth observed in other clusters over\\ncosmic times.   \n",
      "2                                                                                                                                                                                                                                                                    An Assessment of the CO2 Emission Reduction Potential of Residential\\n  Load Management in Developing and Developed Countries [SUMMARY] Intermittent renewable energies are increasingly dominating electricity grids\\nand are forecasted to be the main force driving out fossil fuels from the grid\\nin most major economies until 2040. However, grids based on intermittent\\nrenewables are challenged by diurnal and seasonal mismatch between supply of\\nsun and wind and demand for electricity, including for heat pumps and electric\\ntwo and four wheelers. Load management and demand response measures promise to\\nadjust for this mismatch, utilizing information- and price-based approaches to\\nsteer demand towards times with high supply of intermittent renewables. Here,\\nwe systematically review the literature estimating CO2 savings from residential\\nload management in developing and developed nations. We find that load\\nmanagement holds high potential, locally differentiated with energy mix\\n(including the respective share of renewables and fossils), climate zone, and\\nthe regulatory environment and price mechanism. Most identified studies suggest\\na mitigation potential between 1 and 20%. Load management becomes more relevant\\nwith higher shares of intermittent renewables, and when electricity prices are\\nhigh. Importantly, load management aligns consumers' financial incentives with\\nclimate change mitigation, thus rendering accompanying strategies politically\\nfeasible. We summarize key regulatory steps to facilitate load management in\\neconomies and to realize relevant consumer surplus and mitigation potential.   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Medium Access for Push-Pull Data Transmission in 6G Wireless Systems [SUMMARY] Medium access in 5G systems was tailored to accommodate diverse traffic\\nclasses through network resource slicing. 6G wireless systems are expected to\\nbe significantly reliant on Artificial Intelligence (AI), leading to\\ndata-driven and goal-oriented communication. This leads to augmentation of the\\ndesign space for Medium Access Control (MAC) protocols, which is the focus of\\nthis article. We introduce a taxonomy based on push-based and pull-based\\ncommunication, which is useful to categorize both the legacy and the AI-driven\\naccess schemes. We provide MAC protocol design guidelines for pull- and\\npush-based communication in terms of goal-oriented criteria, such as timing and\\ndata relevance. We articulate a framework for co-existence between pull and\\npush-based communications in 6G systems, combining their advantages. We\\nhighlight the design principles and main tradeoffs, as well as the\\narchitectural considerations for integrating these designs in Open-Radio Access\\nNetwork (O-RAN) and 6G systems.   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Tracer dynamics in an interacting active bath: fluctuations and energy\\n  partition [SUMMARY] We investigate the dynamics of a massive tracer particle coupled to an\\ninteracting active bath, modeled as a harmonic chain of overdamped active\\nparticles analytically, with an aim to understand the impact of bath\\ninteractions and activity on the nonequilibrium fluctuations of the tracer.\\nFrom the microscopic equations, we derive the tracer particle's effective\\nLangevin equation, obtaining the dissipative and stochastic forces from the\\nbath. We analyze the friction kernel, revealing power-law tails in the weak\\ncoupling limit and exponential decay in the strong coupling regime. Due to the\\ninterplay between bath interactions, probe-bath coupling, and activity, the\\nmean squared displacement, velocity, and stationary velocity correlations\\nexhibit different dynamical regimes, which we characterize analytically. Under\\nharmonic confinement, we find that energy equipartition holds at low activity\\nbut breaks down at higher activity, with the kinetic energy exhibiting a\\nnon-monotonic dependence on the activity of the bath.   \n",
      "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Prompt Optimization with Logged Bandit Data [SUMMARY] We study how to use naturally available user feedback, such as clicks, to\\noptimize large language model (LLM) pipelines for generating personalized\\nsentences using prompts. Naive approaches, which estimate the policy gradient\\nin the prompt space, suffer either from variance caused by the large action\\nspace of prompts or bias caused by inaccurate reward predictions. To circumvent\\nthese challenges, we propose a novel kernel-based off-policy gradient method,\\nwhich estimates the policy gradient by leveraging similarity among generated\\nsentences, substantially reducing variance while suppressing the bias.\\nEmpirical results on our newly established suite of benchmarks demonstrate the\\neffectiveness of the proposed approach in generating personalized descriptions\\nfor movie recommendations, particularly when the number of candidate prompts is\\nlarge.   \n",
      "6                                                                                                                                                                                                                                                                                                                             Detection of Deuterated Hydrocarbon Nanoparticles in the Whirlpool\\n  Galaxy, M51 [SUMMARY] Deuteration of hydrocarbon material, including polycyclic aromatic\\nhydrocarbons (PAHs), has been proposed to account for the low gas-phase\\nabundances of D in the interstellar medium. JWST spectra of four star-forming\\nregions in M51 show an emission feature, with central wavelength\\n$\\sim$4.647$\\mu$m and FWHM 0.0265$\\mu$m, corresponding to the C-D stretching\\nmode in aliphatic hydrocarbons. The emitting aliphatic material is estimated to\\nhave (D/H)$_{\\rm aliph}\\approx 0.17\\pm0.02$ -- a factor $\\sim$$10^4$ enrichment\\nrelative to the overall interstellar medium (ISM). On $\\sim$$50\\,$pc scales,\\ndeuteration levels toward four H$\\,$II regions in M51 are 2-3 times higher than\\nin the Orion Bar photodissociation region (PDR), with implications for the\\nprocesses responsible for the formation and evolution of hydrocarbon\\nnanoparticles, including PAHs. The deuteration of the aliphatic material is\\nfound to anticorrelate with helium ionization in the associated H$\\,$II,\\nsuggesting that harsh FUV radiation may act to lower the deuteration of\\naliphatics in PDRs near massive stars. No evidence is found for deuteration of\\naromatic material, with (D/H)$_{\\rm arom} \\lesssim 0.016$: deuteration of the\\naliphatic material exceeds that of the aromatic material by at least a factor\\n10. The observed levels of deuteration may account for the depletion of D\\nobserved in the Galactic interstellar medium. If so, the $4.65\\mu$m feature may\\nbe detectable in absorption.   \n",
      "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                      From Easy to Hard: Building a Shortcut for Differentially Private Image\\n  Synthesis [SUMMARY] Differentially private (DP) image synthesis aims to generate synthetic images\\nfrom a sensitive dataset, alleviating the privacy leakage concerns of\\norganizations sharing and utilizing synthetic images. Although previous methods\\nhave significantly progressed, especially in training diffusion models on\\nsensitive images with DP Stochastic Gradient Descent (DP-SGD), they still\\nsuffer from unsatisfactory performance. In this work, inspired by curriculum\\nlearning, we propose a two-stage DP image synthesis framework, where diffusion\\nmodels learn to generate DP synthetic images from easy to hard. Unlike existing\\nmethods that directly use DP-SGD to train diffusion models, we propose an easy\\nstage in the beginning, where diffusion models learn simple features of the\\nsensitive images. To facilitate this easy stage, we propose to use `central\\nimages', simply aggregations of random samples of the sensitive dataset.\\nIntuitively, although those central images do not show details, they\\ndemonstrate useful characteristics of all images and only incur minimal privacy\\ncosts, thus helping early-phase model training. We conduct experiments to\\npresent that on the average of four investigated image datasets, the fidelity\\nand utility metrics of our synthetic images are 33.1% and 2.1% better than the\\nstate-of-the-art method.   \n",
      "8  The restoration of Book X of the Elements to its original Theaetetean\\n  form [SUMMARY] In the present work, we aim to restore Book X of the $\\it{Elements}$ to its\\noriginal Theaetetean, pre-Eudoxean form in two separate ways. First, we restore\\nthe considerable mathematical content of Book X, by correlating Book X with\\nPlato's account of Theaetetus' mathematical discoveries and Plato's imitations\\nof these discoveries for his philosophy. Thus, Theaetetus proved (i) the\\neventual periodicity of the anthyphairesis of lines a to b, satisfying $Ma^2 =\\nNb^2$, for MN not square number; (ii) the eventual periodic anthyphairesis of\\nlines a to b, satisfying more general quadratic expressions, including the\\nApplication of Areas in defect, and employing this to show that the 12 classes\\nof alogoi lines, including the minor, despite being alogoi, are determined by\\nan eventually periodic Application of Areas in defect; (iii) the anthyphairetic\\npalindromic periodicity of the anthyphairesis of the surds $\\sqrt{N}$ for any\\nnon-square number N, of relevance to the general Pell's Diophantine problem.\\nSecondly, we restore the proofs of all propositions of Book X, in such way that\\nthese are proofs based on Theaetetus', and not on Eudoxus' theory of proportion\\nof magnitudes, in particular not making any use of Eudoxus' condition (namely\\nof definition 4 of Book V). The restoration is based on our reconstruction of\\nTheaetetus' theory of proportion for magnitudes, for the limited class of\\nratios a/b such that either a, b are commensurable or the anthyphairesis of a\\nto b is eventually periodic, without employing Eudoxus' condition, and its\\nsuccess provides a confirmation of our reconstruction.\\n  The final version of this paper will appear as a chapter in the journal\\nGanita Bh\\=arat\\=i, Bulletin of the Indian Society for History of Mathematics,\\n(2) 45 (2023).   \n",
      "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Quantitative Strategy Templates [SUMMARY] This paper presents (permissive) \\emph{Quantitative Strategy Templates}\\n(QaSTels) to succinctly represent infinitely many winning strategies in\\ntwo-player energy and mean-payoff games. This transfers the recently introduced\\nconcept of \\emph{Permissive (qualitative) Strategy Templates} (PeSTels) for\\n$\\omega$-regular games to games with quantitative objectives. We provide the\\ntheoretical and algorithmic foundations of (i) QaSTel synthesis, and (ii) their\\n(incremental) combination with PeSTels for games with mixed quantitative and\\nqualitative objectives. Using a prototype implementation of our synthesis\\nalgorithms, we demonstrate empirically that QaSTels extend the advantageous\\nproperties of strategy templates over single winning strategies -- known from\\nPeSTels -- to games with (additional) quantitative objectives. This includes\\n(i) the enhanced robustness of strategies due to their runtime-adaptability,\\nand (ii) the compositionality of templates w.r.t. incrementally arriving\\nobjectives. We use control-inspired examples to illustrate these superior\\nproperties of QaSTels for CPS design.   \n",
      "\n",
      "        main_category                        categories main_cat_main  \n",
      "0               cs.CL                       cs.CL,cs.LG            cs  \n",
      "1         astro-ph.GA                       astro-ph.GA       physics  \n",
      "2               cs.IR                             cs.IR            cs  \n",
      "3               cs.NI                             cs.NI            cs  \n",
      "4  cond-mat.stat-mech  cond-mat.stat-mech,cond-mat.soft       physics  \n",
      "5               cs.LG         cs.LG,cs.AI,cs.IR,stat.ML            cs  \n",
      "6         astro-ph.GA                       astro-ph.GA       physics  \n",
      "7               cs.CR                       cs.CR,cs.AI            cs  \n",
      "8             math.HO                           math.HO          math  \n",
      "9               cs.GT                             cs.GT            cs  \n",
      "main_cat_main\n",
      "cs         5256\n",
      "physics    3975\n",
      "math       2113\n",
      "eess        524\n",
      "stat        255\n",
      "q-bio       100\n",
      "econ         87\n",
      "q-fin        45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Only keep needed columns\n",
    "newdf = df[['combined','main_category', 'categories']].copy()\n",
    "print(newdf.head(10))\n",
    "\n",
    "# Preparation for multiclass main category classification\n",
    "# Create a new column 'main_cat_main' based on the value in 'main_category'\n",
    "def map_main_category(value):\n",
    "    prefix_map = {\n",
    "        \"cs.\": \"cs\",\n",
    "        \"econ.\": \"econ\",\n",
    "        \"eess.\": \"eess\",\n",
    "        \"math.\": \"math\",\n",
    "        \"q-bio.\": \"q-bio\",\n",
    "        \"q-fin.\": \"q-fin\",\n",
    "        \"stat.\": \"stat\"\n",
    "    }\n",
    "\n",
    "    for prefix, category in prefix_map.items():\n",
    "        if value.startswith(prefix):\n",
    "            return category\n",
    "\n",
    "    physics_prefixes = [\n",
    "        \"astro-\", \"cond-mat\", \"gr-\", \"hep-\", \"math-\", \"nlin.\",\n",
    "        \"nucl-\", \"physics.\", \"quant-\"\n",
    "    ]\n",
    "    if any(value.startswith(prefix) for prefix in physics_prefixes):\n",
    "        return \"physics\"\n",
    "\n",
    "    return \"other\"\n",
    "\n",
    "newdf['main_cat_main'] = newdf['main_category'].apply(map_main_category)\n",
    "print(newdf.head(10))\n",
    "print(newdf['main_cat_main'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "471e91cc-b403-48d8-82a0-4d6839842457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      combined  \\\n",
      "0                                                                                                                                                                                                                                                                                                                                          Evaluating Judges as Evaluators The JETTS Benchmark of LLMasJudges as TestTime Scaling Evaluators SUMMARY Scaling testtime computation or affording a generator large language model LLM extra compute during inference typically employs the help of external nongenerative evaluators ie reward models Concurrently LLMjudges models trained to generate evaluations and critiques explanations in natural language are becoming increasingly popular in automatic evaluation Despite judge empirical successes their effectiveness as evaluators in testtime scaling settings is largely unknown In this paper we introduce the Judge Evaluation for TestTime Scaling JETTS benchmark which evaluates judge performance in three domains math reasoning code generation and instruction following under three task settings response reranking steplevel beam search and critiquebased response refinement We evaluate different judge models BB parameters for different base generator models BB parameters Our benchmark shows that while judges are competitive with outcome reward models in reranking they are consistently worse than process reward models in beam search procedures Furthermore though unique to LLMjudges their natural language critiques are currently ineffective in guiding the generator towards better responses   \n",
      "1      Multiscale exploration of SMACS Js intracluster light and past dynamical history SUMMARY In this work an analysis of the intracluster light ICL in the galaxy cluster SMACS J hereafter SMACS J using JWSTNIRCam deep imaging in six filters FW to FW is presented The images were processed for low surface brightness LSB science with additional correction for instrumental scattering in the shortwavelength channels and analysed using waveletbased decomposition The ICL brightest cluster galaxy BCG and satellite galaxies were extracted and modelled with D maps for each component ICL and ICLBCG fractions computed across all filters within a kpc radius exhibit a flat trend with wavelength averaging and respectively Flux ratios between the BCG and the next brightest members M M and M also display minimal wavelength dependence These results indicate that SMACS J is a dynamically evolved cluster with a dominant BCG and welldeveloped ICL Five prominent ICL substructures are analysed contributing to of the total ICLBCG flux budget slightly exceeding simulation predictions Their short dynamical timescales suggest an instantaneous ICL injection rate of several Lodotrm yr consistent with active dynamical assembly These findings support a scenario where SMACS Js ICL growth is currently driven by galaxy mergers involving the BCG and other bright satellites rather than by the accretion of preprocessed ICL from a recent cluster merger However extrapolating the current injection rate to the clusters lifetime indicates that additional mechanisms are required to match the growth observed in other clusters over cosmic times   \n",
      "2                                                               An Assessment of the CO Emission Reduction Potential of Residential Load Management in Developing and Developed Countries SUMMARY Intermittent renewable energies are increasingly dominating electricity grids and are forecasted to be the main force driving out fossil fuels from the grid in most major economies until However grids based on intermittent renewables are challenged by diurnal and seasonal mismatch between supply of sun and wind and demand for electricity including for heat pumps and electric two and four wheelers Load management and demand response measures promise to adjust for this mismatch utilizing information and pricebased approaches to steer demand towards times with high supply of intermittent renewables Here we systematically review the literature estimating CO savings from residential load management in developing and developed nations We find that load management holds high potential locally differentiated with energy mix including the respective share of renewables and fossils climate zone and the regulatory environment and price mechanism Most identified studies suggest a mitigation potential between and Load management becomes more relevant with higher shares of intermittent renewables and when electricity prices are high Importantly load management aligns consumers financial incentives with climate change mitigation thus rendering accompanying strategies politically feasible We summarize key regulatory steps to facilitate load management in economies and to realize relevant consumer surplus and mitigation potential   \n",
      "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Medium Access for PushPull Data Transmission in G Wireless Systems SUMMARY Medium access in G systems was tailored to accommodate diverse traffic classes through network resource slicing G wireless systems are expected to be significantly reliant on Artificial Intelligence AI leading to datadriven and goaloriented communication This leads to augmentation of the design space for Medium Access Control MAC protocols which is the focus of this article We introduce a taxonomy based on pushbased and pullbased communication which is useful to categorize both the legacy and the AIdriven access schemes We provide MAC protocol design guidelines for pull and pushbased communication in terms of goaloriented criteria such as timing and data relevance We articulate a framework for coexistence between pull and pushbased communications in G systems combining their advantages We highlight the design principles and main tradeoffs as well as the architectural considerations for integrating these designs in OpenRadio Access Network ORAN and G systems   \n",
      "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Tracer dynamics in an interacting active bath fluctuations and energy partition SUMMARY We investigate the dynamics of a massive tracer particle coupled to an interacting active bath modeled as a harmonic chain of overdamped active particles analytically with an aim to understand the impact of bath interactions and activity on the nonequilibrium fluctuations of the tracer From the microscopic equations we derive the tracer particles effective Langevin equation obtaining the dissipative and stochastic forces from the bath We analyze the friction kernel revealing powerlaw tails in the weak coupling limit and exponential decay in the strong coupling regime Due to the interplay between bath interactions probebath coupling and activity the mean squared displacement velocity and stationary velocity correlations exhibit different dynamical regimes which we characterize analytically Under harmonic confinement we find that energy equipartition holds at low activity but breaks down at higher activity with the kinetic energy exhibiting a nonmonotonic dependence on the activity of the bath   \n",
      "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        ...   \n",
      "12350                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The Lorentzian splitting theorem with weakened curvature condition SUMMARY In this note we present a version of the Lorentzian splitting theorem under an averaged Ricci curvature condition utilizing in its proof the properties of achronal limits developed in    \n",
      "12351                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Enumerations of rotational Steiner systems SUMMARY In this paper new rotational Steiner systems for different admissible vk pairs are introduced In particular rotational unitals of order are enumerated   \n",
      "12352                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 A Survey on the Topology of Fractal Squares SUMMARY We consider a special type of selfsimilar sets called fractal squares and give a brief review on recent results and unsolved issues with an emphasis on their topological properties   \n",
      "12353                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The properties of general Fourier partial sums of functions f in CL SUMMARY In this paper we investigated the Fourier partial sums with respect to general orthonormal systems when the function f belongs to some differentiable class of functions   \n",
      "12354                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Compositional Square Roots of expx and x SUMMARY Our work began as an effort to understand calculations by Morris Szekeres and Walker regarding fractional iteration   \n",
      "\n",
      "            main_category                        categories main_cat_main  \n",
      "0                   cs.CL                       cs.CL,cs.LG            cs  \n",
      "1             astro-ph.GA                       astro-ph.GA       physics  \n",
      "2                   cs.IR                             cs.IR            cs  \n",
      "3                   cs.NI                             cs.NI            cs  \n",
      "4      cond-mat.stat-mech  cond-mat.stat-mech,cond-mat.soft       physics  \n",
      "...                   ...                               ...           ...  \n",
      "12350             math.DG                     math.DG,gr-qc          math  \n",
      "12351             math.CO                           math.CO          math  \n",
      "12352             math.GN                           math.GN          math  \n",
      "12353             math.AP                           math.AP          math  \n",
      "12354             math.GM                     math.GM,cs.DM          math  \n",
      "\n",
      "[12355 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "newdf[\"combined\"] = newdf[\"combined\"].str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
    "newdf[\"combined\"] = newdf[\"combined\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "print(newdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5ad43d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (8895, 4)\n",
      "Test set shape: (2471, 4)\n",
      "Validation set shape: (989, 4)\n"
     ]
    }
   ],
   "source": [
    "# Perform train-test-split on newdf, ensure to add stratify to keep class imbalance intact\n",
    "train_df, test_df = train_test_split(newdf, test_size=0.2, random_state=42, stratify=newdf['main_cat_main'])\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['main_cat_main'])\n",
    "print(f\"Train set shape: {train_df.shape}\")\n",
    "print(f\"Test set shape: {test_df.shape}\")\n",
    "print(f\"Validation set shape: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "37580d94-d6e9-4aa3-86b7-05906846f878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01387cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "DistilBertForMaskedLM(\n",
      "  (activation): GELUActivation()\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (vocab_transform): Linear(in_features=768, out_features=768, bias=True)\n",
      "  (vocab_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "  (vocab_projector): Linear(in_features=768, out_features=30522, bias=True)\n",
      "  (mlm_loss_fct): CrossEntropyLoss()\n",
      ")\n",
      "DistilBertConfig {\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.52.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "llm = pipeline(model=\"distilbert/distilbert-base-uncased\",device=0)\n",
    "print(llm.model.config.is_decoder)\n",
    "print(llm.model.config.is_encoder_decoder)\n",
    "print(llm.model)\n",
    "print(llm.model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0dd0524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique categories: 8\n",
      "\n",
      "Aantal unieke categorien in newdf: 8\n",
      "Label to ID mapping (voorbeeld): {'cs': 0, 'econ': 1, 'eess': 2, 'math': 3, 'physics': 4, 'q-bio': 5, 'q-fin': 6, 'stat': 7}\n",
      "ID to Label mapping (voorbeeld): {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5df63eda77914e029f2c14bf341d6d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8895 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a05c9515304e88be89eb9915dbf361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2471 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f03bdcdc014acf98431145e1238b4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/989 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\horoi\\AppData\\Local\\Temp\\ipykernel_49140\\3807597207.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11120' max='11120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11120/11120 40:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.565400</td>\n",
       "      <td>0.443555</td>\n",
       "      <td>0.862031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.354000</td>\n",
       "      <td>0.453476</td>\n",
       "      <td>0.869086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.239600</td>\n",
       "      <td>0.485263</td>\n",
       "      <td>0.884562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.142100</td>\n",
       "      <td>0.582506</td>\n",
       "      <td>0.884837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.658291</td>\n",
       "      <td>0.886622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.045900</td>\n",
       "      <td>0.757856</td>\n",
       "      <td>0.878606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.029100</td>\n",
       "      <td>0.834854</td>\n",
       "      <td>0.872926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>0.834629</td>\n",
       "      <td>0.878320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.847642</td>\n",
       "      <td>0.882728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.857472</td>\n",
       "      <td>0.881196</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start training...\n",
      "Training completed\n",
      "\n",
      "Starting hyperparameter tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 18:42:27,784] A new study created in memory with name: no-name-d82231a6-d777-42da-978d-04dc34475896\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13344' max='13344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13344/13344 27:17, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.615300</td>\n",
       "      <td>0.488472</td>\n",
       "      <td>0.861173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.409600</td>\n",
       "      <td>0.509951</td>\n",
       "      <td>0.877071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>0.581369</td>\n",
       "      <td>0.879998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.169700</td>\n",
       "      <td>0.586740</td>\n",
       "      <td>0.893691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.088200</td>\n",
       "      <td>0.652961</td>\n",
       "      <td>0.887212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>0.700268</td>\n",
       "      <td>0.888797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 19:09:48,747] Trial 0 finished with value: 0.888797317050634 and parameters: {'learning_rate': 1.935919576485628e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'weight_decay': 0.10127885994499723}. Best is trial 0 with value: 0.888797317050634.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13344' max='13344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13344/13344 27:05, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.634900</td>\n",
       "      <td>0.442601</td>\n",
       "      <td>0.881121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.414000</td>\n",
       "      <td>0.497282</td>\n",
       "      <td>0.886976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.268200</td>\n",
       "      <td>0.596448</td>\n",
       "      <td>0.879952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.158000</td>\n",
       "      <td>0.691605</td>\n",
       "      <td>0.883905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.787576</td>\n",
       "      <td>0.878586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.041400</td>\n",
       "      <td>0.806898</td>\n",
       "      <td>0.877187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 19:36:57,286] Trial 1 finished with value: 0.8771869472898052 and parameters: {'learning_rate': 4.683859944137985e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'weight_decay': 0.03998220005599627}. Best is trial 0 with value: 0.888797317050634.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3336' max='3336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3336/3336 22:47, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.593400</td>\n",
       "      <td>0.415661</td>\n",
       "      <td>0.856208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.376013</td>\n",
       "      <td>0.876857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.376916</td>\n",
       "      <td>0.890072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160500</td>\n",
       "      <td>0.388078</td>\n",
       "      <td>0.893460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103200</td>\n",
       "      <td>0.452698</td>\n",
       "      <td>0.884931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.457719</td>\n",
       "      <td>0.890140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 19:59:48,056] Trial 2 finished with value: 0.8901403520274012 and parameters: {'learning_rate': 1.817238607053195e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 16, 'weight_decay': 0.27776385810057325}. Best is trial 2 with value: 0.8901403520274012.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13344' max='13344' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13344/13344 27:10, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.615300</td>\n",
       "      <td>0.486319</td>\n",
       "      <td>0.860993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.413800</td>\n",
       "      <td>0.499511</td>\n",
       "      <td>0.881484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.292600</td>\n",
       "      <td>0.561316</td>\n",
       "      <td>0.884838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.186600</td>\n",
       "      <td>0.585288</td>\n",
       "      <td>0.888813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.658497</td>\n",
       "      <td>0.888448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.070200</td>\n",
       "      <td>0.671296</td>\n",
       "      <td>0.886550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:27:01,961] Trial 3 finished with value: 0.886550277878506 and parameters: {'learning_rate': 1.3802174516551864e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'weight_decay': 0.148664915246595}. Best is trial 2 with value: 0.8901403520274012.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4448' max='4448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4448/4448 09:07, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.607700</td>\n",
       "      <td>0.472926</td>\n",
       "      <td>0.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.376900</td>\n",
       "      <td>0.431755</td>\n",
       "      <td>0.879000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:36:12,114] Trial 4 finished with value: 0.8790004038908618 and parameters: {'learning_rate': 1.947054786272091e-05, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'weight_decay': 0.004276486272880864}. Best is trial 2 with value: 0.8901403520274012.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1112' max='6672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1112/6672 04:02 < 20:16, 4.57 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.594200</td>\n",
       "      <td>0.448246</td>\n",
       "      <td>0.857493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:40:16,888] Trial 5 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1112' max='2224' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1112/2224 04:03 < 04:04, 4.55 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.558400</td>\n",
       "      <td>0.423271</td>\n",
       "      <td>0.869826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:44:22,214] Trial 6 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='556' max='1112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 556/1112 03:50 < 03:50, 2.41 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602700</td>\n",
       "      <td>0.440434</td>\n",
       "      <td>0.849595</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:48:14,215] Trial 7 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1112' max='4448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1112/4448 04:03 < 12:12, 4.55 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.581000</td>\n",
       "      <td>0.447782</td>\n",
       "      <td>0.861628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:52:19,100] Trial 8 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='556' max='1668' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 556/1668 03:49 < 07:39, 2.42 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.617300</td>\n",
       "      <td>0.449224</td>\n",
       "      <td>0.845194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 20:56:09,550] Trial 9 pruned. \n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best hyperparameters found:\n",
      "{'learning_rate': 1.817238607053195e-05, 'num_train_epochs': 6, 'per_device_train_batch_size': 16, 'weight_decay': 0.27776385810057325}\n",
      "\n",
      "Retraining with best hyperparameters...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3336' max='3336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3336/3336 22:57, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.593400</td>\n",
       "      <td>0.415776</td>\n",
       "      <td>0.856208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.349100</td>\n",
       "      <td>0.376725</td>\n",
       "      <td>0.876857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.243700</td>\n",
       "      <td>0.377133</td>\n",
       "      <td>0.888093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.160700</td>\n",
       "      <td>0.389097</td>\n",
       "      <td>0.892425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.103100</td>\n",
       "      <td>0.453437</td>\n",
       "      <td>0.884991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.073000</td>\n",
       "      <td>0.458862</td>\n",
       "      <td>0.890140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report on the Testset:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          cs       0.91      0.91      0.91      1051\n",
      "        econ       0.50      0.29      0.37        17\n",
      "        eess       0.55      0.58      0.57       105\n",
      "        math       0.83      0.86      0.85       423\n",
      "     physics       0.94      0.95      0.94       795\n",
      "       q-bio       0.50      0.30      0.38        20\n",
      "       q-fin       0.86      0.67      0.75         9\n",
      "        stat       0.60      0.51      0.55        51\n",
      "\n",
      "    accuracy                           0.88      2471\n",
      "   macro avg       0.71      0.63      0.66      2471\n",
      "weighted avg       0.88      0.88      0.88      2471\n",
      "\n",
      "\n",
      "Confusion Matrix on the Testset:\n",
      "[[955   0  34  32  23   1   0   6]\n",
      " [  4   5   1   2   1   1   0   3]\n",
      " [ 34   0  61   8   2   0   0   0]\n",
      " [ 28   3   7 363  17   0   0   5]\n",
      " [ 14   0   4  21 752   3   0   1]\n",
      " [  9   0   1   1   2   6   0   1]\n",
      " [  0   1   0   1   0   0   6   1]\n",
      " [ 11   1   2   7   2   1   1  26]]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "\n",
    "unique_categories = newdf['main_cat_main'].unique().tolist()\n",
    "num_labels = len(unique_categories)\n",
    "print(f\"Number of unique categories: {num_labels}\")\n",
    "\n",
    "# Use transform to apply the same mapping to the test and validation set\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['labels'] = label_encoder.fit_transform(train_df['main_cat_main'])\n",
    "for df in [test_df, val_df]:\n",
    "    df['labels'] = label_encoder.transform(df['main_cat_main'])\n",
    "\n",
    "# Use this to easy trainer and to interpret predictions\n",
    "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: i for i, label in enumerate(label_encoder.classes_)}\n",
    "\n",
    "print(f\"\\nNumber of unique categories in newdf: {num_labels}\")\n",
    "print(f\"Label to ID mapping (example): {label2id}\")\n",
    "print(f\"ID to Label mapping (example):: {id2label}\")\n",
    "\n",
    "# Transform the DataFrames to Hugging Face datasets, to speed up training and evaluation when tokenizing (used later in trainer)\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "# Tokenization function for Dataset.map() method to speed up tokenization, maybe add max_length=64 to speed up further?\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['combined'], padding=True, truncation=True)\n",
    "\n",
    "# Apply function to datasets.map() to tokenize on batch size. Allready tokenize by batch to speed up\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True,\\\n",
    "                                             remove_columns=['combined', 'main_category', 'categories', 'main_cat_main'])\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, \\\n",
    "                                           remove_columns=['combined', 'main_category', 'categories', 'main_cat_main'])\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, \\\n",
    "                                        remove_columns=['combined', 'main_category', 'categories', 'main_cat_main'])\n",
    "\n",
    "# compute_metrics function for Trainer: class imbalance, so weighted f1 (average of precision and recall) and confusion matrix are best metrics to use\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    weighted_f1 = f1_score(labels, predictions, average='weighted')\n",
    "    return {\n",
    "        'weighted_f1': weighted_f1\n",
    "    }\n",
    "\n",
    "# Hyperparameter Search Space\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 6),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8, 16]),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.3),\n",
    "    }\n",
    "\n",
    "lr = 2e-5\n",
    "batch_size = 8\n",
    "num_epochs = 10 \n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    logging_dir=\"./results\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    metric_for_best_model=\"eval_weighted_f1\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "# We have class imbalance, so weighted f1 (average of precision and recall) and confusion matrix are best metrics to use\n",
    "## Class imbalance: use class weights\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    tokenizer=tokenizer, \n",
    "    compute_metrics= compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "print(\"\\nStart training...\")\n",
    "print(\"Training completed\")\n",
    "\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_weighted_f1\"]\n",
    "\n",
    "# Run Hyperparameter Tuning \n",
    "print(\"\\nStarting hyperparameter tuning...\")\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=compute_objective,\n",
    "    n_trials=10\n",
    ")\n",
    "\n",
    "print(\"\\nBest hyperparameters found:\")\n",
    "print(best_run.hyperparameters)\n",
    "\n",
    "# Update Trainer Arguments with Best Parameters\n",
    "trainer.args.learning_rate = best_run.hyperparameters[\"learning_rate\"]\n",
    "trainer.args.num_train_epochs = best_run.hyperparameters[\"num_train_epochs\"]\n",
    "trainer.args.per_device_train_batch_size = best_run.hyperparameters[\"per_device_train_batch_size\"]\n",
    "trainer.args.weight_decay = best_run.hyperparameters[\"weight_decay\"]\n",
    "\n",
    "# Retrain Model with Best Hyperparameters\n",
    "print(\"\\nRetraining with best hyperparameters...\")\n",
    "trainer.train()\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Evaluate on Test Set\n",
    "predictions = trainer.predict(tokenized_test_dataset)\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=-1)\n",
    "true_labels = predictions.label_ids\n",
    "\n",
    "print(\"\\nClassification Report on the Testset:\")\n",
    "print(classification_report(true_labels, predicted_labels, target_names=label_encoder.classes_))\n",
    "\n",
    "print(\"\\nConfusion Matrix on the Testset:\")\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_gpu)",
   "language": "python",
   "name": "venv_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
