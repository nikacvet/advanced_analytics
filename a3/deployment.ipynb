{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c7122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am using the following SPARK_HOME: /home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 18:23:05 WARN Utils: Your hostname, nika-hp resolves to a loopback address: 127.0.1.1; using 192.168.10.236 instead (on interface wlp6s0)\n",
      "25/05/30 18:23:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/30 18:23:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/pyspark/streaming/context.py:72: FutureWarning: DStream is deprecated as of Spark 3.4.0. Migrate to Structured Streaming.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Streaming Context started.\n",
      "========= 2025-05-30 18:23:10 =========\n",
      "RDD is empty, skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 18:23:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:10 WARN BlockManager: Block input-0-1748622190000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:10 WARN BlockManager: Block input-0-1748622190200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:10 WARN BlockManager: Block input-0-1748622190400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:10 WARN BlockManager: Block input-0-1748622190600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:11 WARN BlockManager: Block input-0-1748622190800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:12 WARN BlockManager: Block input-0-1748622192000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:12 WARN BlockManager: Block input-0-1748622192200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:12 WARN BlockManager: Block input-0-1748622192400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:12 WARN BlockManager: Block input-0-1748622192600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:13 WARN BlockManager: Block input-0-1748622192800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:14 WARN BlockManager: Block input-0-1748622194000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:14 WARN BlockManager: Block input-0-1748622194200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:14 WARN BlockManager: Block input-0-1748622194400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:14 WARN BlockManager: Block input-0-1748622194600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:15 WARN BlockManager: Block input-0-1748622194800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:16 WARN BlockManager: Block input-0-1748622196200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:16 WARN BlockManager: Block input-0-1748622196400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:16 WARN BlockManager: Block input-0-1748622196600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:17 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:17 WARN BlockManager: Block input-0-1748622196800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:17 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:17 WARN BlockManager: Block input-0-1748622197000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:18 WARN BlockManager: Block input-0-1748622198200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:18 WARN BlockManager: Block input-0-1748622198400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:18 WARN BlockManager: Block input-0-1748622198600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:19 WARN BlockManager: Block input-0-1748622198800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:19 WARN BlockManager: Block input-0-1748622199000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2025-05-30 18:23:20 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 18:23:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:20 WARN BlockManager: Block input-0-1748622200200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:20 WARN BlockManager: Block input-0-1748622200400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:20 WARN BlockManager: Block input-0-1748622200600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:21 WARN BlockManager: Block input-0-1748622200800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:21 WARN BlockManager: Block input-0-1748622201000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:22 WARN BlockManager: Block input-0-1748622202200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:22 WARN BlockManager: Block input-0-1748622202400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:22 WARN BlockManager: Block input-0-1748622202600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:23 WARN BlockManager: Block input-0-1748622202800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:23 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:23 WARN BlockManager: Block input-0-1748622203000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:24 WARN BlockManager: Block input-0-1748622204200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:24 WARN BlockManager: Block input-0-1748622204400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:24 WARN BlockManager: Block input-0-1748622204600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:25 WARN BlockManager: Block input-0-1748622204800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:25 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:25 WARN BlockManager: Block input-0-1748622205000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:26 WARN BlockManager: Block input-0-1748622206400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:26 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:26 WARN BlockManager: Block input-0-1748622206600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:27 WARN BlockManager: Block input-0-1748622206800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:27 WARN BlockManager: Block input-0-1748622207000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:27 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:27 WARN BlockManager: Block input-0-1748622207200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:28 WARN BlockManager: Block input-0-1748622208400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:28 WARN BlockManager: Block input-0-1748622208600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:29 WARN BlockManager: Block input-0-1748622208800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:29 WARN BlockManager: Block input-0-1748622209000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:29 WARN BlockManager: Block input-0-1748622209200 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      "25/05/30 18:23:30 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:30 WARN BlockManager: Block input-0-1748622210600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:31 WARN BlockManager: Block input-0-1748622210800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:31 WARN BlockManager: Block input-0-1748622211000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:31 WARN BlockManager: Block input-0-1748622211200 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title an empirical study on strongweak model collaboration for repolevel code generation summary we study costefficient collaboration between strong and weak language models for repositorylevel code generation where the weak model handles simpler tasks at lower cost and the most challenging tasks are delegated to the strong model while many works propose architectures for this task few analyze performance relative to cost we evaluate a broad spectrum of collaboration strategies: contextbased pipelinebased and dynamic on github issue resolution our most effective collaborative strategy achieves equivalent performance to the strong model while reducing the cost by 40 based on our findings we offer actionable guidelines for choosing collaboration strategies under varying budget and performance constraints our results show that strongweak collaboration substantially boosts the weak models performance at a fraction of the cost pipeline and contextbased methods being most efficient we release the code for our work at https:githubcomshubhamrgandhicodegenstrongweakcollab']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loading model and tokenizer on worker                                       \n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title think: can large language models thinkaloud summary assessing higherorder thinking skills in large language models llms remains a fundamental challenge especially in tasks that go beyond surfacelevel accuracy in this work we propose think testing higherorder notion of knowledge a multiagent feedbackdriven evaluation framework grounded in blooms taxonomy think frames reasoning assessment as an iterative task of problem generation critique and revision encouraging llms to thinkaloud through stepbystep reflection and refinement this enables a systematic evaluation of both lowerorder eg remember understand and higherorder eg evaluate create thinking skills we apply think to seven stateoftheart llms and perform a detailed cognitive analysis of their outputs results reveal that while models reliably perform lowerorder categories well they struggle with applying knowledge in realistic contexts and exhibit limited abstraction structured feedback loops significantly improve reasoning performance particularly in higherorder thinking qualitative evaluations further confirm that thinkguided outputs better align with domain logic and problem structure the code of our framework provides a scalable methodology for probing and enhancing llm reasoning offering new directions for evaluation grounded in learning science which is available at our github repository']\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 3:>                  (0 + 4) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:32 WARN BlockManager: Block input-0-1748622212600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:33 WARN BlockManager: Block input-0-1748622212800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:33 WARN BlockManager: Block input-0-1748622213000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:33 WARN BlockManager: Block input-0-1748622213200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:34 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:34 WARN BlockManager: Block input-0-1748622214600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:35 WARN BlockManager: Block input-0-1748622214800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:35 WARN BlockManager: Block input-0-1748622215000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:35 WARN BlockManager: Block input-0-1748622215200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:35 WARN BlockManager: Block input-0-1748622215400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      "25/05/30 18:23:36 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:36 WARN BlockManager: Block input-0-1748622216600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:37 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:37 WARN BlockManager: Block input-0-1748622216800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:37 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:37 WARN BlockManager: Block input-0-1748622217000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:37 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:37 WARN BlockManager: Block input-0-1748622217200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:37 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:37 WARN BlockManager: Block input-0-1748622217400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title exposing gos hidden bugs: a novel concolic framework summary the widespread adoption of the go programming language in infrastructure backends and blockchain projects has heightened the need for improved security measures established techniques such as unit testing static analysis and program fuzzing provide foundational protection mechanisms although symbolic execution tools have made significant contributions opportunities remain to address the complexities of gos runtime and concurrency model in this work we present zorya a novel methodology leveraging concrete and symbolic concolic execution to evaluate go programs comprehensively by systematically exploring execution paths to uncover vulnerabilities beyond conventional testing symbolic execution offers distinct advantages and coupling it with concrete execution mitigates the path explosion problem our solution employs ghidras pcode as an intermediate representation ir this implementation detects runtime panics in the tinygo compiler and supports both generic and custom invariants furthermore pcodes generic ir nature enables analysis of programs written in other languages such as c future enhancements may include intelligent classification of concolic execution logs to identify vulnerability patterns']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title eradicating the unseen: detecting exploiting and remediating a path traversal vulnerability across github summary vulnerabilities in opensource software can cause cascading effects in the modern digital ecosystem it is especially worrying if these vulnerabilities repeat across many projects as once the adversaries find one of them they can scale up the attack very easily unfortunately since developers frequently reuse code from their own or external code resources some nearly identical vulnerabilities exist across many opensource projects we conducted a study to examine the prevalence of a particular vulnerable code pattern that enables path traversal attacks cwe22 across opensource github projects to handle this study at the github scale we developed an automated pipeline that scans github for the targeted vulnerable pattern confirms the vulnerability by first running a static analysis and then exploiting the vulnerability in the context of the studied project assesses its impact by calculating the cvss score generates a patch using gpt4 and reports the vulnerability to the maintainers using our pipeline we identified 1756 vulnerable opensource projects some of which are very influential for many of the affected projects the vulnerability is critical cvss score higher than 90 as it can be exploited remotely without any privileges and critically impact the confidentiality and availability of the system we have responsibly disclosed the vulnerability to the maintainers and 14 of the reported vulnerabilities have been remediated we also investigated the root causes of the vulnerable code pattern and assessed the side effects of the large number of copies of this vulnerable pattern that seem to have poisoned several popular llms our study highlights the urgent need to help secure the opensource ecosystem by leveraging scalable automated vulnerability management solutions and raising awareness among developers']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title sentiment spreads but topics do not in covid19 discussions within the belgian reddit community summary this study investigates how topics and sentiments on covid19 mitigation measures specifically lockdowns mask mandates and vaccinations spread through the belgian reddit community we explore 655642 posts created between 1 january 2020 and 30 june 2022 in line with previous studies for other countries and platforms we find that the volume of posts on these topics can be tied to important external events but not withinreddit interactions sentiment however is influenced by the sentiment of previous posts resulting in homophily and polarisation we define a homophily measure and find values of 0228 0198 and 0133 for lockdowns masks and vaccination respectively additionally we introduce a novel bounded confidence model that estimates internal sentiment of users from their expressed sentiment the wasserstein metric between the predicted and the observed sentiments takes values between 0493 vaccination and 0607 lockdown these results yield insight into the way the belgian reddit community experienced the pandemic and which aspects influenced the topics discussed and their associated sentiment']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:38 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:38 WARN BlockManager: Block input-0-1748622218600 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:39 WARN BlockManager: Block input-0-1748622218800 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      "25/05/30 18:23:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:39 WARN BlockManager: Block input-0-1748622219000 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title funreason: enhancing large language models function calling via selfrefinement multiscale loss and automated data refinement summary the integration of large language models llms with function calling has emerged as a crucial capability for enhancing their practical utility in realworld applications however effectively combining reasoning processes with accurate function execution remains a significant challenge traditional training approaches often struggle to balance the detailed reasoning steps with the precision of function calls leading to suboptimal performance to address these limitations we introduce funreason a novel framework that enhances llms function calling capabilities through an automated data refinement strategy and a selfrefinement multiscale loss srml approach funreason leverages llms natural reasoning abilities to generate highquality training examples focusing on query parseability reasoning coherence and function call precision the srml approach dynamically balances the contribution of reasoning processes and function call accuracy during training addressing the inherent tradeoff between these two critical aspects funreason achieves performance comparable to gpt4o while effectively mitigating catastrophic forgetting during finetuning funreason provides a comprehensive solution for enhancing llms function calling capabilities by introducing a balanced training methodology and a data refinement pipeline for code and dataset please refer to our repository at github https:githubcombingguanghaofunreason']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title research on feature fusion and multimodal patent text based on graph attention network summary aiming at the problems of crossmodal feature fusion low efficiency of long text modeling and lack of hierarchical semantic coherence in patent text semantic mining this study proposes hgmnet a deep learning framework that integrates hierarchical comparative learning hcl multimodal graph attention network mgat and multigranularity sparse attention msa which builds a dynamic mask contrast and crossstructural similarity constraints on the word sentence and paragraph hierarchies through hcl contrast and crossstructural similarity constraints are constructed at the word and paragraph levels by hcl to strengthen the local semantic and global thematic consistency of patent text mgat models patent classification codes citation relations and text semantics as heterogeneous graph structures and achieves dynamic fusion of multisource features by crossmodal gated attention msa adopts a hierarchical sparsity strategy to optimize the computational efficiency of long text modeling at word phrase sentence and paragraph granularity experiments show that the framework demonstrates significant advantages over existing deep learning methods in tasks such as patent classification and similarity matching and provides a solution with both theoretical innovation and practical value for solving the problems of patent examination efficiency improvement and technology relevance mining']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title private geometric median in nearlylinear time summary estimating the geometric median of a dataset is a robust counterpart to mean estimation and is a fundamental problem in computational geometry recently hsu24 gave an varepsilon deltadifferentially private algorithm obtaining an alphamultiplicative approximation to the geometric median objective frac 1 n sumi in n cdot mathbfxi given a dataset mathcald : mathbfxii in n subset mathbbrd their algorithm requires n gtrsim sqrt d cdot frac 1 alphavarepsilon samples which they prove is informationtheoretically optimal this result is surprising because its error scales with the empheffective radius of mathcald ie of a ball capturing most points rather than the worstcase radius we give an improved algorithm that obtains the same approximation quality also using n gtrsim sqrt d cdot frac 1 alphaepsilon samples but in time widetildeond frac d alpha2 our runtime is nearlylinear plus the cost of the cheapest nonprivate firstorder method due to clm16 to achieve our results we use subsampling and geometric aggregation tools inspired by friendlycore tck22 to speed up the warm start component of the hsu24 algorithm combined with a careful custom analysis of dpsgds sensitivity for the geometric median objective']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title the modular hamiltonian in asymptotically flat spacetime conformal to minkowski summary we consider a fourdimensional globally hyperbolic spacetime mg conformal to minkowski spacetime together with a massless conformally coupled scalar field using a bulktoboundary correspondence one can establish the existence of an injective homomorphism upsilonm between mathcalwm the weyl algebra of observables on m and a counterpart which is defined intrinsically on future null infinity imsimeqmathbbrtimesmathbbs2 a component of the conformal boundary of mg using invariance under the asymptotic symmetry group of im we can individuate thereon a distinguished twopoint correlation function whose pullback to m via upsilonm identifies a quasifree hadamard state for the bulk algebra of observables in this setting if we consider mathsfvx a future light cone stemming from xin m as well as mathcalwmathsfvxmathcalwmmathsfvx its counterpart at the boundary is the weyl subalgebra generated by suitable functions localized in mathsfkx a positive half strip on im to each such cone we associate a standard subspace of the boundary oneparticle hilbert space which coincides with the one associated naturally to mathsfkx we extend such correspondence replacing mathsfkx and mathsfvx with deformed counterparts denoted by mathsfsc and mathsfvc in addition since the one particle hilbert space at the boundary decomposes as a direct integral on the sphere of u1currents defined on the real line we prove that also the generator of the modular group associated to the standard subspace of mathsfvc decomposes as a suitable direct integral this result allows us to study the relative entropy between coherent states of the algebras associated to the deformed cones mathsfvc establishing the quantum null energy condition']\n",
      "25/05/30 18:23:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:39 WARN BlockManager: Block input-0-1748622219200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:39 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:39 WARN BlockManager: Block input-0-1748622219400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 4:>                (0 + 20) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 4:=>               (2 + 18) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4](0 + 1) / 1][Stage 4:==>              (3 + 17) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      "25/05/30 18:23:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:40 WARN BlockManager: Block input-0-1748622220600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:41 WARN BlockManager: Block input-0-1748622220800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:41 WARN BlockManager: Block input-0-1748622221000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:41 WARN BlockManager: Block input-0-1748622221200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:41 WARN BlockManager: Block input-0-1748622221400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:43 WARN BlockManager: Block input-0-1748622222800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:43 WARN BlockManager: Block input-0-1748622223000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:43 WARN BlockManager: Block input-0-1748622223200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:43 WARN BlockManager: Block input-0-1748622223400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:43 WARN BlockManager: Block input-0-1748622223600 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      "25/05/30 18:23:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:45 WARN BlockManager: Block input-0-1748622224800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:45 WARN BlockManager: Block input-0-1748622225000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:45 WARN BlockManager: Block input-0-1748622225200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:45 WARN BlockManager: Block input-0-1748622225400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:45 WARN BlockManager: Block input-0-1748622225600 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title identification of power system dynamic model parameters using the fisher information matrix summary the expected decrease in system inertia and frequency stability motivates the development and maintenance of dynamic system models by transmission system operators however some dynamic model parameters can be unavailable due to market unbundling or inaccurate due to aging infrastructure nondocumented tuning of controllers or other factors in this paper we propose the use of a numerical approximation of the fisher information matrix nfim for efficient inference of dynamic model parameters thanks to the proposed numerical implementation the method is scalable to electromagnetic transient emt models which can quickly become computationally complex even for small study systems case studies show that the nfim is coherent with parameter variances of single and multiparameter leastsquares estimators when applied to an ieee 9bus dynamic model with artificial measurements']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title black hole thermodynamics and topology summary recently the difference between the gibbonshawking temperature trm gh attributed to the hawking radiation from the de sitter cosmological horizon and the twice as high local temperature of the de sitter state thpi2trm gh has been discussed by hughes and kusmartsev from the topological point of view see arxiv:250505814 according to their approach this difference is determined by the euler characteristic chical m of the considered spacetime with euclidean time the invariant chical m is different for the global spacetime cal ms4 and for the manifold limited to a region near the horizon cal md2times s2 here we consider the application of the topological approach to reissnernordstrom rn black holes with two horizons both the outer and inner horizons are characterized by their nearhorizon topology which determines the corresponding horizon temperatures as a result of the correlation between the horizons the entropy of the rn black hole is independent of its electric charge being completely determined by the mass of the black hole this demonstrates the applicability of the topological approach to the multihorizon systems']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title monocle: hybrid localglobal incontext evaluation for longtext generation with uncertaintybased active learning summary assessing the quality of longform modelgenerated text is challenging even with advanced llmasajudge methods due to performance degradation as input length increases to address this issue we propose a divideandconquer approach which breaks down the comprehensive evaluation task into a series of localized scoring tasks followed by a final global assessment this strategy allows for more granular and manageable evaluations ensuring that each segment of the text is assessed in isolation for both coherence and quality while also accounting for the overall structure and consistency of the entire piece moreover we introduce a hybrid incontext learning approach that leverages human annotations to enhance the performance of both local and global evaluations by incorporating humangenerated feedback directly into the evaluation process this method allows the model to better align with human judgment finally we develop an uncertaintybased active learning algorithm that efficiently selects data samples for human annotation thereby reducing annotation costs in practical scenarios experimental results show that the proposed evaluation framework outperforms several representative baselines highlighting the effectiveness of our approach']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title floquet engineering spin triplet states in unconventional magnets summary we consider unconventional magnets with and without spinsinglet swave superconductivity and demonstrate the emergence of spin triplet states due to light drives in particular we find that a highfrequency linearly polarized light drive induces a spintriplet density in dwave altermagnets which does not exist in the static regime and can directly reveal the strength of the altermagnetic field in this highfrequency regime we also show that linearly polarized light enables the formation of oddfrequency spintriplet superconducting correlations possessing dwave and swave parities which can be controlled by the light drive and accessed by measuring the spin density moreover for lowfrequency linearly and circularly polarized light drives we obtain that the types of superconducting correlations are broadened due to the presence of floquet bands enabling spintriplet pairs in d and pwave unconventional magnets which are absent in the undriven phase']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title kira 3: integral reduction with efficient seeding and optimized equation selection summary we present version 3 of kira a feynman integral reduction program for highprecision calculations in quantum field theory and gravitationalwave physics building on previous versions kira 3 introduces optimized seeding and equation selection algorithms significantly improving performance for multiloop and multiscale problems new features include convenient numerical sampling symbolic integrationbyparts reductions and support for userdefined additional relations we demonstrate its capabilities through benchmarks on two and threeloop topologies showcasing up to two orders of magnitude improvement over kira 23 kira 3 is publicly available and poised to support ambitious projects in particle physics and beyond']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title reasoning is not all you need: examining llms for multiturn mental health conversations summary limited access to mental healthcare extended wait times and increasing capabilities of large language models llms has led individuals to turn to llms for fulfilling their mental health needs however examining the multiturn mental health conversation capabilities of llms remains underexplored existing evaluation frameworks typically focus on diagnostic accuracy and winrates and often overlook alignment with patientspecific goals values and personalities required for meaningful conversations to address this we introduce medagent a novel framework for synthetically generating realistic multiturn mental health sensemaking conversations and use it to create the mental health sensemaking dialogue mhsd dataset comprising over 2200 patientllm conversations additionally we present multisenseeval a holistic framework to evaluate the multiturn conversation abilities of llms in healthcare settings using humancentric criteria our findings reveal that frontier reasoning models yield belowpar performance for patientcentric communication and struggle at advanced diagnostic capabilities with average score of 31 additionally we observed variation in model performance based on patients persona and performance drop with increasing turns in the conversation our work provides a comprehensive synthetic data generation framework a dataset and evaluation framework for assessing llms in multiturn mental health conversations']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title muscle crossbridge theory with internal crossbridge dynamics summary we describe in this paper a crossbridge model in which an attached crossbridge behaves like a linear spring with a variable rest length we assume in particular that the rest length has a linear forcevelocity relation and that the force and rest length are both zero at the moment of crossbridge attachment crossbridges that are not attached in our model have a fixed probability per unit time of attachment and attached crossbridges have a probability per unit time of detachment that is a function of the crossbridge force this detachment rate is uniquely determined by the requirement that a limiting form of the model should reproduce the forcevelocity curve and heat of shortening discovered by avhillciteavhill and the detachment rate turns out to be a linearly decreasing function of the crossbridge force the parameters of the model are determined by a fit to steadystate experimental data and then an eventdriven stochastic simulation methodology is introduced in order to study the behavior of the model in a simulated quickrelease experiment the model explains how the crossbridge can act like a linear spring on a fast time scale but have very different properties on a slower time scale']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title temporal sampling for forgotten reasoning in llms summary finetuning large language models llms is intended to improve their reasoning capabilities yet we uncover a counterintuitive effect: models often forget how to solve problems they previously answered correctly during training we term this phenomenon temporal forgetting and show that it is widespread across model sizes finetuning methods both reinforcement learning and supervised finetuning and multiple reasoning benchmarks to address this gap we introduce temporal sampling a simple decoding strategy that draws outputs from multiple checkpoints along the training trajectory this approach recovers forgotten solutions without retraining or ensembling and leads to substantial improvements in reasoning performance gains from 4 to 19 points in passk and consistent gains in majorityk across several benchmarks we further extend our method to loraadapted models demonstrating that storing only adapter weights across checkpoints achieves similar benefits with minimal storage cost by leveraging the temporal diversity inherent in training temporal sampling offers a practical computeefficient way to surface hidden reasoning ability and rethink how we evaluate llms']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title adaptive classifierfree guidance via dynamic lowconfidence masking summary classifierfree guidance cfg significantly enhances controllability in generative models by interpolating conditional and unconditional predictions however standard cfg often employs a static unconditional input which can be suboptimal for iterative generation processes where model uncertainty varies dynamically we introduce adaptive classifierfree guidance acfg a novel method that tailors the unconditional input by leveraging the models instantaneous predictive confidence at each step of an iterative masked diffusion language model acfg identifies tokens in the currently generated sequence for which the model exhibits low confidence these tokens are temporarily remasked to create a dynamic localized unconditional input this focuses cfgs corrective influence precisely on areas of ambiguity leading to more effective guidance we integrate acfg into a stateoftheart masked diffusion language model and demonstrate its efficacy experiments on diverse language generation benchmarks show that acfg yields substantial improvements over standard cfg achieving for instance a 39 point gain on gpqa our work highlights the benefit of dynamically adapting guidance mechanisms to model uncertainty in iterative generation']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title evaluating large language models for code review summary context: code reviews are crucial for software quality recent ai advances have allowed large language models llms to review and fix code now there are tools that perform these reviews however their reliability and accuracy have not yet been systematically evaluated objective: this study compares different llms performance in detecting code correctness and suggesting improvements method: we tested gpt4o and gemini 20 flash on 492 ai generated code blocks of varying correctness along with 164 canonical code blocks from the humaneval benchmark to simulate the code review task objectively we expected llms to assess code correctness and improve the code if needed we ran experiments with different configurations and reported on the results results: with problem descriptions gpt4o and gemini 20 flash correctly classified code correctness 6850 and 6389 of the time respectively and corrected the code 6783 and 5426 of the time for the 492 code blocks of varying correctness without problem descriptions performance declined the results for the 164 canonical code blocks differed suggesting that performance depends on the type of code conclusion: llm code reviews can help suggest improvements and assess correctness but there is a risk of faulty outputs we propose a process that involves humans called the human in the loop llm code review to promote knowledge sharing while mitigating the risk of faulty outputs']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title mechanism of defect formation in the quantum annealing of random transversefield ising chain summary based on the strongdisorder renormalization group method a microscopic mechanism of defect formation in the quantum annealing of the random transversefield ising chain is proposed which represents the annealing process as a gradual aggregation of strongly coupled spin clusters the ferromagnetic ground states of clusters are either preserved or get excited in pairwise fusions of clusters depending on the effective annealing rate of the fusion the latter events being responsible for the appearance of defects in the final state an interesting consequence of the theory is that although the griffithsmccoy phases surrounding the critical point are gapless these phases are still effectively gapped from the point of view of quantum annealing thereby we provide an a posteriori justification of the assumption on the finiteness of gap outside of the critical point tacitly used in earlier works and also refine the functional form of its vanishing at the critical point the defect density in the final state is found to decrease with the annealing time tau asymptotically as ntausim ln2leftfractauln2tauright in addition to this our approach gives access also to the timedependent density of defects accumulated during the annealing process at intermediate times']\n",
      ">>> Predicted class IDs: [2]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['eess']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title leveraging descriptions of emotional preferences in recommender systems summary the affective attitude of liking a recommended item reflects just one category in a wide spectrum of affective phenomena that also includes emotions such as entranced or intrigued moods such as cheerful or buoyant as well as more finegrained affective states such as pleasantly surprised by the conclusion in this paper we introduce a novel recommendation task that can leverage a virtually unbounded range of affective states sought explicitly by the user in order to identify items that upon consumption are likely to induce those affective states correspondingly we create a large dataset of user preferences containing expressions of finegrained affective states that are mined from book reviews and propose a transformerbased architecture that leverages such affective expressions as input we then use the resulting dataset of affective states preferences together with the linked users and their histories of book readings ratings and reviews to train and evaluate multiple recommendation models on the task of matching recommended items with affective preferences experiments show that the best results are obtained by models that can utilize textual descriptions of items and user affective preferences']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title shutdownable agents through postagency summary many fear that future artificial agents will resist shutdown i present an idea the postagents proposal for ensuring that doesnt happen i propose that we train agents to satisfy preferences only between samelength trajectories post i then prove that post together with other conditions implies neutrality: the agent maximizes expected utility ignoring the probability distribution over trajectorylengths i argue that neutrality keeps agents shutdownable and allows them to be useful']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title a frequentist view on the twobody decaying dark matter model summary decaying dark matter ddm has emerged as an interesting framework to extend the lambdacolddarkmatter lcdm model as many particle physics models predict that dark matter may not be stable over cosmic time and can impact structure formation in particular a model in which dm decays at a rate gamma and imprints a velocity kick v onto its decay products leads to a low amplitude of fluctuations as quantified by the parameter s8 in better agreement with that measured by some past weak lensing surveys bayesian analyses have provided mixed conclusions regarding its viability with a reconstructed clustering amplitude only slightly below the standard lcdm value in this paper we perform a frequentist analysis of planckbao data we find 1sigma constraints on the halflife of 693788285gyr and a velocity kick of 125014501000kms which differ from their bayesian counterparts indicating the presence of volume effects moreover we find that under the ddm model the frequentist analysis predicts lower values of s8 in agreement with those found by kids1000 and desy3 at 15sigma we further show that previously derived kids1000 constraints that appeared to exclude the bestfit model from planck data were driven by priors on the primordial parameters as and ns when those are removed from the analysis kids1000 constraints on the ddm parameters are fully relaxed it is only when applying planckinformed priors on as and ns to the kids1000 analysis that one can constrain the model we further highlight that in the absence of such priors the region of scales bestmeasured by kids1000 does not exactly match the s8 kernel but rather a slightly smaller range of scales centered around ksim 03 hmpc one must thus be careful in applying s8 constraints to a model instead of the full data likelihood']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title transcorrelated methods for multireference problems summary we apply the transcorrelated method to problems of multireference character for this we show that the choice of reference wavefunction during the jastrow optimisation procedure is vital and we propose a workflow wherein we use conventional multiconfigurational methods to provide a reference wavefunction for jastrow factor optimisation this jastrow function is subsequently used with transcorrelatedfull configuration interaction quantum monte carlo within the xtc approximation tcfciqmc to yield highly accurate transcorrelated energies this is demonstrated for n2 using the augccpvtz basis set achieving chemical accuracy across the entire binding curve compared with experiment we also apply the method to compute excitation energies of dinitrogen co and the ammonia molecule where accurate results comparable to the best available theoretical predictions are obtained with modest basis sets']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title pathbench: a comprehensive comparison benchmark for pathology foundation models towards precision oncology summary the emergence of pathology foundation models has revolutionized computational histopathology enabling highly accurate generalized wholeslide image analysis for improved cancer diagnosis and prognosis assessment while these models show remarkable potential across cancer diagnostics and prognostics their clinical translation faces critical challenges including variability in optimal model across cancer types potential data leakage in evaluation and lack of standardized benchmarks without rigorous unbiased evaluation even the most advanced pfms risk remaining confined to research settings delaying their lifesaving applications existing benchmarking efforts remain limited by narrow cancertype focus potential pretraining data overlaps or incomplete task coverage we present pathbench the first comprehensive benchmark addressing these gaps through: multicenter inhourse datasets spanning common cancers with rigorous leakage prevention evaluation across the full clinical spectrum from diagnosis to prognosis and an automated leaderboard system for continuous model assessment our framework incorporates largescale data enabling objective comparison of pfms while reflecting realworld clinical complexity all evaluation data comes from private medical providers with strict exclusion of any pretraining usage to avoid data leakage risks we have collected 15888 wsis from 8549 patients across 10 hospitals encompassing over 64 diagnosis and prognosis tasks currently our evaluation of 19 pfms shows that virchow2 and hoptimus1 are the most effective models overall this work provides researchers with a robust platform for model development and offers clinicians actionable insights into pfm performance across diverse clinical scenarios ultimately accelerating the translation of these transformative technologies into routine pathology practice']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [4](0 + 1) / 1][Stage 4:=====>           (6 + 14) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [7]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['stat']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 4:==========>      (12 + 8) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 4:===============> (18 + 2) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      "25/05/30 18:23:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:47 WARN BlockManager: Block input-0-1748622226800 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|title                                                                                                                            |main_category  |predicted_category|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "|An Empirical Study on Strong-Weak Model Collaboration for Repo-level\\n  Code Generation                                          |cs.AI          |cs                |\n",
      "|Exposing Go's Hidden Bugs: A Novel Concolic Framework                                                                            |cs.SE          |cs                |\n",
      "|THiNK: Can Large Language Models Think-aloud?                                                                                    |cs.CL          |cs                |\n",
      "|Sentiment spreads, but topics do not, in COVID-19 discussions within the\\n  Belgian Reddit community                             |cs.SI          |cs                |\n",
      "|Eradicating the Unseen: Detecting, Exploiting, and Remediating a Path\\n  Traversal Vulnerability across GitHub                   |cs.CR          |cs                |\n",
      "|Transcorrelated Methods for Multireference Problems                                                                              |physics.chem-ph|physics           |\n",
      "|Research on feature fusion and multimodal patent text based on graph\\n  attention network                                        |cs.LG          |cs                |\n",
      "|Private Geometric Median in Nearly-Linear Time                                                                                   |cs.DS          |cs                |\n",
      "|Leveraging Descriptions of Emotional Preferences in Recommender Systems                                                          |cs.IR          |cs                |\n",
      "|The modular Hamiltonian in asymptotically flat spacetime conformal to\\n  Minkowski                                               |math-ph        |physics           |\n",
      "|FunReason: Enhancing Large Language Models' Function Calling via\\n  Self-Refinement Multiscale Loss and Automated Data Refinement|cs.LG          |cs                |\n",
      "|A frequentist view on the two-body decaying dark matter model                                                                    |astro-ph.CO    |physics           |\n",
      "|Black hole thermodynamics and topology                                                                                           |gr-qc          |physics           |\n",
      "|Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text\\n  Generation with Uncertainty-Based Active Learning            |cs.CL          |cs                |\n",
      "|Temporal Sampling for Forgotten Reasoning in LLMs                                                                                |cs.AI          |cs                |\n",
      "|Kira 3: integral reduction with efficient seeding and optimized equation\\n  selection                                            |hep-ph         |physics           |\n",
      "|Muscle Crossbridge Theory With Internal Crossbridge Dynamics                                                                     |physics.bio-ph |stat              |\n",
      "|Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking                                                             |cs.CL          |cs                |\n",
      "|Identification of Power System Dynamic Model Parameters using the Fisher\\n  Information Matrix                                   |eess.SP        |eess              |\n",
      "|Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental\\n  Health Conversations                                      |cs.CL          |cs                |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2025-05-30 18:23:30 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 18:23:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:47 WARN BlockManager: Block input-0-1748622227000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:47 WARN BlockManager: Block input-0-1748622227200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:47 WARN BlockManager: Block input-0-1748622227400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title gpumc: a stateless model checker for gpu weak memory concurrency summary gpu computing is embracing weak memory concurrency for performance improvement however compared to cpus modern gpus provide more finegrained concurrency features such as scopes have additional properties like divergence and thereby follow different weak memory consistency models these features and properties make concurrent programming on gpus more complex and errorprone to this end we present gpumc a stateless model checker to check the correctness of gpu sharedmemory concurrent programs under scopedrc11 weak memory concurrency model gpumc explores all possible executions in gpu programs to reveal various errors races barrier divergence and assertion violations in addition gpumc also automatically repairs these errors in the appropriate cases we evaluate gpumc with benchmarks and reallife gpu programs gpumc is efficient both in time and memory in verifying large gpu programs where stateoftheart tools are timed out in addition gpumc identifies all known errors in these benchmarks compared to the stateoftheart tools']\n",
      "25/05/30 18:23:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:47 WARN BlockManager: Block input-0-1748622227600 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0]                                        (0 + 1) / 1]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loading model and tokenizer on worker                                       \n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title parameterefficient finetuning with column space projection summary finetuning large language models llms with minimal computational overhead is essential for efficiently adapting them to downstream tasks under resource constraints parameterefficient finetuning peft methods such as lowrank adaptation lora facilitate this by updating only a small subset of parameters however recent studies show that lora diverges from full finetuning full ft in its learning behavior particularly in terms of spectral properties motivated by these findings we propose pica the first theoretically grounded peft method based on the spectral properties of finetuned weights pica projects gradients onto the lowrank column subspace of pretrained weights and exhibits learning patterns more closely aligned with full ft furthermore we show that combining pica with weight sharing drastically reduces the number of trainable parameters without compromising performance enabling to achieve superior performance than lora using 13x fewer trainable parameters extensive experiments demonstrate pica achieves the stateoftheart performance compared to existing peft methods']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title a structurepreserving multiscale solver for particlewave interaction in nonuniform magnetized plasmas summary particlewave interaction is of fundamental interest in plasma physics especially in the study of runaway electrons in magnetic confinement fusion analogous to the concept of photons and phonons wave packets in plasma can also be treated as quasiparticles called plasmons to model the mixture of electrons and plasmons in plasma a set of collisional kinetic equations has been derived based on weak turbulence limit and the wentzelkramersbrillouin wkb approximation there are two main challenges in solving the electronplasmon kinetic system numerically firstly nonuniform plasma density and magnetic field results in high dimensionality and the presence of multiple time scales secondly a physically reliable numerical solution requires a structurepreserving scheme that enforces the conservation of mass momentum and energy in this paper we propose a struturepreserving multiscale solver for particlewave interaction in nonuniform magnetized plasmas the solver combines a conservative local discontinuous galerkin ldg scheme for the interaction part with a trajectory averaging method for the plasmon hamiltonian flow part numerical examples for a nonuniform magnetized plasma in an infinitely long symmetric cylinder are presented it is verified that the ldg scheme rigorously preserves all the conservation laws and the trajectory averaging method significantly reduces the computational cost']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title how to improve the robustness of closedsource models on nli summary closedsource large language models llms have become increasingly popular with impressive performance across a wide range of natural language tasks these models can be finetuned to further improve performance but this often results in the models learning from datasetspecific heuristics that reduce their robustness on outofdistribution ood data existing methods to improve robustness either perform poorly or are nonapplicable to closedsource models because they assume access to model internals or the ability to change the models training procedure in this work we investigate strategies to improve the robustness of closedsource llms through datacentric methods that do not require access to model internals we find that the optimal strategy depends on the complexity of the ood data for highly complex ood datasets upsampling more challenging training examples can improve robustness by up to 15 for less complex ood datasets replacing a portion of the training set with llmgenerated examples can improve robustness by 37 more broadly we find that largescale closedsource autoregressive llms are substantially more robust than commonly used encoder models and are a more appropriate choice of baseline going forward']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title estimation of multivariate traces of states given partial classical information summary bargmann invariants of order n defined as multivariate traces of quantum states texttrrho1rho2 ldots rhon are useful in applications ranging from quantum metrology to certification of nonclassicality a standard quantum circuit used to estimate bargmann invariants is the cycle test in this work we propose generalizations of the cycle test applicable to a situation where n systems are given and unknown and classical information on m systems mleq n is available allowing estimation of invariants of order nm our main result is a generalization of results on 4th order invariants appearing in double weak values from chiribella et al phys rev research 6 043043 2024 the use of classical information on some of the states enables circuits on fewer qubits and with fewer gates decreasing the experimental requirements for their estimation and enabling multiple applications we briefly review']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4](0 + 1) / 1][Stage 7:=========>         (2 + 2) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      "25/05/30 18:23:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:49 WARN BlockManager: Block input-0-1748622228800 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      "25/05/30 18:23:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:49 WARN BlockManager: Block input-0-1748622229000 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title measure domains gap: a similar domain selection principle for multidomain recommendation summary multidomain recommendation mdr achieves the desirable recommendation performance by effectively utilizing the transfer information across different domains despite the great success most existing mdr methods adopt a single structure to transfer complex domainshared knowledge however the beneficial transferring information should vary across different domains when there is knowledge conflict between domains or a domain is of poor quality unselectively leveraging information from all domains will lead to a serious negative transfer problem ntp therefore how to effectively model the complex transfer relationships between domains to avoid ntp is still a direction worth exploring to address these issues we propose a simple and dynamic similar domain selection principle sdsp for multidomain recommendation in this paper sdsp presents the initial exploration of selecting suitable domain knowledge for each domain to alleviate ntp specifically we propose a novel prototypebased domain distance measure to effectively model the complexity relationship between domains thereafter the proposed sdsp can dynamically find similar domains for each domain based on the supervised signals of the domain metrics and the unsupervised distance measure from the learned domain prototype we emphasize that sdsp is a lightweight method that can be incorporated with existing mdr methods for better performance while not introducing excessive time overheads to the best of our knowledge it is the first solution that can explicitly measure domainlevel gaps and dynamically select appropriate domains in the mdr field extensive experiments on three datasets demonstrate the effectiveness of our proposed method']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title dependency parsing is more parameterefficient with normalization summary dependency parsing is the task of inferring natural language structure often approached by modeling word interactions via attention through biaffine scoring this mechanism works like selfattention in transformers where scores are calculated for every pair of words in a sentence however unlike transformer attention biaffine scoring does not use normalization prior to taking the softmax of the scores in this paper we provide theoretical evidence and empirical results revealing that a lack of normalization necessarily results in overparameterized parser models where the extra parameters compensate for the sharp softmax outputs produced by high variance inputs to the biaffine scoring function we argue that biaffine scoring can be made substantially more efficient by performing score normalization we conduct experiments on six datasets for semantic and syntactic dependency parsing using a onehop parser we train nlayer stacked bilstms and evaluate the parsers performance with and without normalizing biaffine scores normalizing allows us to beat the state of the art on two datasets with fewer samples and trainable parameters code: https:anonymous4opensciencerefficientsdp70c1']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title bridging the longterm gap: a memoryactive policy for multisession taskoriented dialogue summary existing taskoriented dialogue tod systems primarily focus on singlesession dialogues limiting their effectiveness in longterm memory augmentation to address this challenge we introduce a mstod dataset the first multisession tod dataset designed to retain longterm memory across sessions enabling fewer turns and more efficient task completion this defines a new benchmark task for evaluating longterm memory in multisession tod based on this new dataset we propose a memoryactive policy map that improves multisession dialogue efficiency through a twostage approach 1 memoryguided dialogue planning retrieves intentaligned history identifies key qa units via a memory judger refines them by removing redundant questions and generates responses based on the reconstructed memory 2 proactive response strategy detects and correct errors or omissions ensuring efficient and accurate task completion we evaluate map on mstod dataset focusing on response quality and effectiveness of the proactive strategy experiments on mstod demonstrate that map significantly improves task success and turn efficiency in multisession scenarios while maintaining competitive performance on conventional singlesession tasks']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title general solution of corona problem summary our main result is a description of the spectrum of bidual algebra a of a uniform algebra a this allows us to obtain abstract corona theorem for certain uniform algebras asserting density of a specific gleason part in the spectrum of an hinfty type subalgebra of a there is an isometric isomorphism of the latter subalgebra with hinftyg for a wide class of domains gsubsetmathbb cd using abstract corona theorem we show the density of the canonical image of g in the spectrum of hinftyg solving positively corona problem for this class which in particular includes balls and polydisks']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title chainofthought for autonomous driving: a comprehensive survey and future prospects summary the rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities such proficiencies have been leveraged in autonomous driving systems contributing to significant improvements in system performance models such as openai o1 and deepseekr1 leverage chainofthought cot reasoning an advanced cognitive method that simulates human thinking processes demonstrating remarkable reasoning capabilities in complex tasks by structuring complex driving scenarios within a systematic reasoning framework this approach has emerged as a prominent research focus in autonomous driving substantially improving the systems ability to handle challenging cases this paper investigates how cot methods improve the reasoning abilities of autonomous driving models based on a comprehensive literature review we present a systematic analysis of the motivations methodologies challenges and future research directions of cot in autonomous driving furthermore we propose the insight of combining cot with selflearning to facilitate selfevolution in driving systems to ensure the relevance and timeliness of this study we have compiled a dynamic repository of literature and opensource projects diligently updated to incorporate forefront developments the repository is publicly available at https:githubcomcuiyx1720awesomecot4ad']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title finegrained listwise alignment for generative medication recommendation summary accurate and safe medication recommendations are critical for effective clinical decisionmaking especially in multimorbidity cases however existing systems rely on pointwise prediction paradigms that overlook synergistic drug effects and potential adverse drugdrug interactions ddis we propose flame a finegrained listwise alignment framework for large language models llms enabling drugbydrug generation of drug lists flame formulates recommendation as a sequential decision process where each step adds or removes a single drug to provide finegrained learning signals we devise stepwise group relative policy optimization grpo with potentialbased reward shaping which explicitly models ddis and optimizes the contribution of each drug to the overall prescription furthermore flame enhances patient modeling by integrating structured clinical knowledge and collaborative information into the representation space of llms experiments on benchmark datasets demonstrate that flame achieves stateoftheart performance delivering superior accuracy controllable safetyaccuracy tradeoffs and strong generalization across diverse clinical scenarios our code is available at https:githubcomcxfannflame']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title the mirage of multimodality: where truth is tested and honesty unravels summary reasoning models have recently attracted significant attention especially for tasks that involve complex inference their strengths exemplify the system ii paradigm slow structured thinking contrasting with the system i rapid heuristicdriven yet does slower reasoning necessarily lead to greater truthfulness our findings suggest otherwise in this study we present the first systematic investigation of distortions associated with system i and system ii reasoning in multimodal contexts we demonstrate that slower reasoning models when presented with incomplete or misleading visual inputs are more likely to fabricate plausible yet false details to support flawed reasoning a phenomenon we term the mirage of multimodality to examine this we constructed a 5000sample hierarchical prompt dataset annotated by 50 human participants these prompts gradually increase in complexity revealing a consistent pattern: slower reasoning models tend to employ depthfirst thinking delving deeper into incorrect premises whereas faster chat models favor breadthfirst inference exhibiting greater caution under uncertainty our results highlight a critical vulnerability of slower reasoning models: although highly effective in structured domains such as mathematics it becomes brittle when confronted with ambiguous multimodal inputs']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title gradient flow matching for learning update dynamics in neural network training summary training deep neural networks remains computationally intensive due to the itera2 tive nature of gradientbased optimization we propose gradient flow matching gfm a continuoustime modeling framework that treats neural network training as a dynamical system governed by learned optimizeraware vector fields by leveraging conditional flow matching gfm captures the underlying update rules of optimizers such as sgd adam and rmsprop enabling smooth extrapolation of weight trajectories toward convergence unlike blackbox sequence models gfm incorporates structural knowledge of gradientbased updates into the learning objective facilitating accurate forecasting of final weights from partial training sequences empirically gfm achieves forecasting accuracy that is competitive with transformerbased models and significantly outperforms lstm and other classical baselines furthermore gfm generalizes across neural architectures and initializations providing a unified framework for studying optimization dynamics and accelerating convergence prediction']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title universal nonthermal fixed point for quasi1d bose gases summary spatiotemporal scaling dynamics connected to nonthermal fixed points has been suggested as a universal framework to describe the relaxation of isolated farfromequilibrium systems experimental studies in weaklyinteracting cold atom systems have found scaling dynamics connected to specific attractors in our experiments we study a quantum gas of strongly interacting 6li2 feshbach molecules brought far out of equilibrium by imprinting a whitenoise phase profile the observed relaxation follows the same universal dynamics as for the previously observed formation of the order parameter in a shockcooled gas of weakly interacting 87rb atoms our results point to a single universal fixed point with a large basin of attraction governing the relaxation of quasi1d bosonic systems independent of their specific initial conditions and microscopic details']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title towards the automated extraction and refactoring of nosql schemas from application code summary in this paper we present a static code analysis strategy to extract logical schemas from nosql applications our solution is based on a modeldriven reverse engineering process composed of a chain of platformindependent model transformations the extracted schema conforms to the uschema unified metamodel which can represent both nosql and relational schemas to support this process we define a metamodel capable of representing the core elements of objectoriented languages application code is first injected into a code model from which a control flow model is derived this in turn enables the generation of a model representing both data access operations and the structure of stored data from these models the uschema logical schema is inferred additionally the extracted information can be used to identify refactoring opportunities we illustrate this capability through the detection of joinlike query patterns and the automated application of field duplication strategies to eliminate expensive joins all stages of the process are described in detail and the approach is validated through a roundtrip experiment in which a application using a mongodb store is automatically generated from a predefined schema the inferred schema is then compared to the original to assess the accuracy of the extraction process']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title from onedimensional diffusion processes metastable behaviour to parabolic equations asymptotics summary consider the onedimensional elliptic operator given by beginequation lepsilon fx b x fx epsilon a x fx endequation where the drift bcolon r to r and the diffusion coefficient acolon r to r are periodic c1r functions satisfying further conditions and epsilon0 consider the initialvalued problem beginequation left beginaligned partialtuepsilonlepsilonuepsilon uepsilon0cdotu0cdot endaligned rightendequation for some bounded continuous function u0 we prove the existence of timescales thetaepsilon1dotsthetaepsilonmathfrakq such that thetaepsilon1toinfty thetaepsilonp1thetaepsilonptoinfty 1le plemathfrakq1 probability measures pxcdot xin r and kernels rtpmjmk where mj:jin z represents the set of stable equilibrium of the ode dotxt bxt such that beginequation limepsilonto0 uepsilontthetaepsilonp x sumjkin z pxmj rtp mjmk u0mk endequation for all t0 and xin r the solution uepsilon asymptotic behavior description is completed by the characterisation of its behaviour in the intermediate timescales varrhoepsilon such that varrhoepsilonthetaepsilonptoinfty varrhoepsilonthetaepsilonp1to0 for some 0le plemathfrakq where thetaepsilon01 thetaepsilonmathfrakq1infty the proof relies on the analysis of the diffusion xepsiloncdot induced by the generator lepsilon based on the resolvent approach to metastability introduced in 21']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title triangular and unitriangular factorization of twisted chevalley groups summary the existence of triangular and unitriangular factorizations has been extensively studied for untwisted chevalley groups as well as for twisted chevalley groups of types other than 2a2n n geq 1 however the case of twisted chevalley groups of type 2a2n n geq 1 has remained unresolved in the general setting of commutative rings prior work by a smolensky addressed this case only over certain fields including finite fields and the field of complex numbers these results indicate that even over fields the 2a2n case demands more refined techniques reflecting the difficulty of extending such factorizations to the broader class of commutative rings in this paper we introduce two new classes of commutative rings: those satisfying the emphspecial stable range one condition and those that are emphthetacomplete we discuss their basic properties and provide illustrative examples our main result establishes the existence of triangular and unitriangular factorizations for twisted chevalley groups of type 2a2n over a certain class of commutative rings which includes all fields all local rings with mild restrictions and several other important classes of rings']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title theoretical study of charge transport properties of curved pah organic semiconductors summary curved polycyclic aromatic hydrocarbons pahs exhibit distinctive geometric and electronic structures rendering them highly promising in addressing issues of solubility and air stability which are faced for large linear arene piconjugated organic semiconductors in this study a series of surfacecurved pahs and the heteroatom doped derivatives are selected and designed and the relationship between electronic structure and charge transport properties of these molecules is investigated by using density functional theory dft and the effects of sulfuroxygen nitrogen and boron doping on the charge transport performance of curved pah semiconductors are explored the results show that curved pahs exhibit improved solubility and stability with the degree of molecular curvature significantly affecting the materials transport properties']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title tools for characterizing the numerical error of stellar oscillation codes summary stellar oscillation codes are software instruments that evaluate the normalmode frequencies of an input stellar model while intercode comparisons are often used to confirm the correctness of calculations they are not suitable for characterizing the numerical error of an individual code to address this issue we introduce a set of tools error measures that facilitate this characterization we explore the behavior of these error measures as calculation parameters such as the number of radial grid points used to discretize the oscillation equations are varied and we summarize this behavior via an idealized error model while our analysis focuses on the gyre code it remains broadly applicable to other oscillation codes']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title continuous learning for childrens asr: overcoming catastrophic forgetting with elastic weight consolidation and synaptic intelligence summary in this work we present the first study addressing automatic speech recognition asr for children in an online learning setting this is particularly important for both childcentric applications and the privacy protection of minors where training models with sequentially arriving data is critical the conventional approach of model finetuning often suffers from catastrophic forgetting to tackle this issue we explore two established techniques: elastic weight consolidation ewc and synaptic intelligence si using a custom protocol on the myst corpus tailored to the online learning setting we achieve relative word error rate wer reductions of 521 with ewc and 436 with si compared to the finetuning baseline']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title new perspectives on the polyak stepsize: surrogate functions and negative results summary the polyak stepsize has been proven to be a fundamental stepsize in convex optimization giving near optimal gradient descent rates across a wide range of assumptions the universality of the polyak stepsize has also inspired many stochastic variants with theoretical guarantees and strong empirical performance despite the many theoretical results our understanding of the convergence properties and shortcomings of the polyak stepsize or its variants is both incomplete and fractured across different analyses we propose a new unified and simple perspective for the polyak stepsize and its variants as gradient descent on a surrogate loss we show that each variant is equivalent to minimize a surrogate function with stepsizes that adapt to a guaranteed local curvature our general surrogate loss perspective is then used to provide a unified analysis of existing variants across different assumptions moreover we show a number of negative results proving that the nonconvergence results in some of the upper bounds is indeed real']\n",
      "25/05/30 18:23:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:49 WARN BlockManager: Block input-0-1748622229200 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title ftboosted sv: towards noise robust speaker verification for english speaking classroom environments summary creating speaker verification sv systems for classroom settings that are robust to classroom noises such as babble noise is crucial for the development of ai tools that assist educational environments in this work we study the efficacy of finetuning with augmented children datasets to adapt the xvector and ecapatdnn to classroom environments we demonstrate that finetuning with augmented childrens datasets is powerful in that regard and reduces the equal error rate eer of xvector and ecapatdnn models for both classroom datasets and children speech datasets notably this method reduces eer of the ecapatdnn model on average by half a 5 improvement for classrooms in the mpt dataset compared to the ecapatdnn baseline model the xvector model shows an 8 average improvement for classrooms in the ncte dataset compared to its baseline']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title flamemoe: a transparent endtoend research platform for mixtureofexperts language models summary recent large language models such as gemini15 deepseekv3 and llama4 increasingly adopt mixtureofexperts moe architectures which offer strong efficiencyperformance tradeoffs by activating only a fraction of the model per token yet academic researchers still lack a fully open endtoend moe platform for investigating scaling routing and expert behavior we release flamemoe a completely opensource research suite composed of seven decoderonly models ranging from 38m to 17b active parameters whose architecture64 experts with top8 gating and 2 shared expertsclosely reflects modern production llms all training data pipelines scripts logs and checkpoints are publicly available to enable reproducible experimentation across six evaluation tasks flamemoe improves average accuracy by up to 34 points over dense baselines trained with identical flops leveraging full training trace transparency we present initial analyses showing that i experts increasingly specialize on distinct token subsets ii coactivation matrices remain sparse reflecting diverse expert usage and iii routing behavior stabilizes early in training all code training logs and model checkpoints are available at https:githubcomcmuflameflamemoe']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title from what to how: attributing clips latent components reveals unexpected semantic reliance summary transformerbased clip models are widely used for textimage probing and feature extraction making it relevant to understand the internal mechanisms behind their predictions while recent works show that sparse autoencoders saes yield interpretable latent components they focus on what these encode and miss how they drive predictions we introduce a scalable framework that reveals what latent components activate for how they align with expected semantics and how important they are to predictions to achieve this we adapt attribution patching for instancewise component attributions in clip and highlight key faithfulness limitations of the widely used logit lens technique by combining attributions with semantic alignment scores we can automatically uncover reliance on components that encode semantically unexpected or spurious concepts applied across multiple clip variants our method uncovers hundreds of surprising components linked to polysemous words compound nouns visual typography and dataset artifacts while text embeddings remain prone to semantic ambiguity they are more robust to spurious correlations compared to linear classifiers trained on image embeddings a case study on skin lesion detection highlights how such classifiers can amplify hidden shortcuts underscoring the need for holistic mechanistic interpretability we provide code at https:githubcommaxdreyerattributingclip']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title passive cavitation mitigation on hydrofoils via porous media: a comparative study of les and rans models summary this study numerically investigates the use of porous media as a passive strategy for mitigating cavitation on a naca 66 mod hydrofoil subjected to unsteady twophase flow']\n",
      ">>> Predicted class IDs: [3]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['math']\n",
      "25/05/30 18:23:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:49 WARN BlockManager: Block input-0-1748622229400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [4](0 + 1) / 1][Stage 8:>                (1 + 19) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [3]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['math']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:49 WARN BlockManager: Block input-0-1748622229600 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 8:===========>     (13 + 7) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [3]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['math']\n",
      ">>> Predicted class IDs: [3]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['math']\n",
      ">>> Predicted class IDs: [3](0 + 1) / 1][Stage 8:================>(19 + 1) / 20]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['math']\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|title                                                                                                                                    |main_category     |predicted_category|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|GPUMC: A Stateless Model Checker for GPU Weak Memory Concurrency                                                                         |cs.LO             |cs                |\n",
      "|Estimation of multivariate traces of states given partial classical\\n  information                                                       |quant-ph          |physics           |\n",
      "|How to Improve the Robustness of Closed-Source Models on NLI                                                                             |cs.CL             |cs                |\n",
      "|A structure-preserving multiscale solver for particle-wave interaction\\n  in non-uniform magnetized plasmas                              |math.NA           |physics           |\n",
      "|Parameter-Efficient Fine-Tuning with Column Space Projection                                                                             |cs.LG             |cs                |\n",
      "|Tools for Characterizing the Numerical Error of Stellar Oscillation\\n  Codes                                                             |astro-ph.SR       |physics           |\n",
      "|Universal non-thermal fixed point for quasi-1D Bose gases                                                                                |cond-mat.quant-gas|physics           |\n",
      "|The Mirage of Multimodality: Where Truth is Tested and Honesty Unravels                                                                  |cs.AI             |cs                |\n",
      "|Dependency Parsing is More Parameter-Efficient with Normalization                                                                        |cs.CL             |cs                |\n",
      "|Continuous Learning for Children's ASR: Overcoming Catastrophic\\n  Forgetting with Elastic Weight Consolidation and Synaptic Intelligence|eess.AS           |cs                |\n",
      "|From one-dimensional diffusion processes metastable behaviour to\\n  parabolic equations asymptotics                                      |math.PR           |math              |\n",
      "|Fine-grained List-wise Alignment for Generative Medication\\n  Recommendation                                                             |cs.LG             |cs                |\n",
      "|New Perspectives on the Polyak Stepsize: Surrogate Functions and\\n  Negative Results                                                     |math.OC           |math              |\n",
      "|General solution of corona problem                                                                                                       |math.FA           |math              |\n",
      "|Gradient Flow Matching for Learning Update Dynamics in Neural Network\\n  Training                                                        |cs.LG             |cs                |\n",
      "|FT-Boosted SV: Towards Noise Robust Speaker Verification for English\\n  Speaking Classroom Environments                                  |eess.AS           |cs                |\n",
      "|Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and\\n  Future Prospects                                                  |cs.RO             |cs                |\n",
      "|Triangular and Unitriangular Factorization of Twisted Chevalley Groups                                                                   |math.GR           |math              |\n",
      "|FLAME-MoE: A Transparent End-to-End Research Platform for\\n  Mixture-of-Experts Language Models                                          |cs.CL             |cs                |\n",
      "|Passive Cavitation Mitigation on Hydrofoils via Porous Media: A\\n  Comparative Study of LES and RANS Models                              |physics.flu-dyn   |math              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2025-05-30 18:23:40 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Loading model and tokenizer on worker                           (0 + 1) / 1]\n",
      "25/05/30 18:23:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:51 WARN BlockManager: Block input-0-1748622231000 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title multimodal federated learning with missing modalities through feature imputation network summary multimodal federated learning holds immense potential for collaboratively training models from multiple sources without sharing raw data addressing both data scarcity and privacy concerns two key challenges in healthcare a major challenge in training multimodal federated models in healthcare is the presence of missing modalities due to multiple reasons including variations in clinical practice cost and accessibility constraints retrospective data collection privacy concerns and occasional technical or human errors previous methods typically rely on publicly available real datasets or synthetic data to compensate for missing modalities however obtaining real datasets for every disease is impractical and training generative models to synthesize missing modalities is computationally expensive and prone to errors due to the high dimensionality of medical data in this paper we propose a novel lightweight lowdimensional feature translator to reconstruct bottleneck features of the missing modalities our experiments on three different datasets mimiccxr nih openi and chexpert in both homogeneous and heterogeneous settings consistently improve the performance of competitive baselines the code and implementation details are available at: https:githubcombhattarailabfedfeatgen']\n",
      "25/05/30 18:23:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:51 WARN BlockManager: Block input-0-1748622231200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:51 WARN BlockManager: Block input-0-1748622231400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 10:>                 (0 + 1) / 1]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:51 WARN BlockManager: Block input-0-1748622231600 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title efficient speech translation through model compression and knowledge distillation summary efficient deployment of large audiolanguage models for speech translation remains challenging due to their significant computational requirements in this paper we address this challenge through our system submissions to the model compression track at the international conference on spoken language translation iwslt 2025 we experiment with a combination of approaches including iterative layer pruning based on layer importance evaluation lowrank adaptation with 4bit quantization qlora and knowledge distillation in our experiments we use qwen2audio7binstruct for speech translation into german and chinese our pruned student models achieve up to a 50 reduction in both model parameters and storage footprint while retaining 97100 of the translation quality of the indomain teacher models', ' title on certain problems in the theory of root clusters summary we carry forward the work started by the author and bhagwat in 1 and develop the theory of root clusters further in this article we establish the inverse root capacity problem for number fields which is a generalization of inverse cluster size problem for number fields proved in 1 we give a field theoretic formulation for the concept of minimal generating sets of splitting fields which was introduced by the author and vanchinathan in 4 and establish the existence of field extensions over number fields for given degree and given cardinality of minimal generating set of galois closure dividing the degree we improve on the inverse problems proved in 1 and this article by proving that there exist arbitrarily large finite families of pairwise nonisomorphic extensions having additional properties that satisfy the given conditions']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title resonances in lifetimes of ads oscillon summary oscillons are classical oscillatory solutions with very long but finite lifetimes in real scalar field theories with appropriate potentials an interesting feature is that resonances appear in the lifetimes of the oscillon for the initial size of the oscillon core r0 which was discovered by honda and choptuik in the case of minkowski space in a previous work oscillons in the global antide sitter ads space have been constructed which we abbreviate as ads oscillons we present new resonance structures for the curvature radius l and the core size r0 in the lifetime of the ads oscillon we then compute exponents associated with the resonance peaks finally we observe the bifurcation of the peaks due to the reflected waves']\n",
      "25/05/30 18:23:52 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:52 WARN BlockManager: Block input-0-1748622231800 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title derivations for the mps overlap formulas of rational spin chains summary we derive a universal formula for the overlaps between integrable matrix product states mps and bethe eigenstates in mathfrakgln symmetric spin chains this formula expresses the normalized overlap as a product of a mpsindependent gaudindeterminant ratio and a mpsdependent scalar factor constructed from eigenvalues of commuting operators defined via the kmatrix associated with the mps our proof is fully representationindependent and relies solely on algebraic bethe ansatz techniques and the ktrelation we also propose a generalization of the overlap formula to mathfrakson and mathfrakspn spin chains supported by algebra embeddings and lowrank isomorphisms these results significantly broaden the class of integrable initial states for which exact overlap formulas are available with implications for quantum quenches and defect cfts', ' title variational deep learning via implicit regularization summary modern deep learning models generalize remarkably well indistribution despite being overparametrized and trained with little to no explicit regularization instead current theory credits implicit regularization imposed by the choice of architecture hyperparameters and optimization procedure however deploying deep learning models outofdistribution in sequential decisionmaking tasks or in safetycritical domains necessitates reliable uncertainty quantification not just a point estimate the machinery of modern approximate inference bayesian deep learning should answer the need for uncertainty quantification but its effectiveness has been challenged by our inability to define useful explicit inductive biases through priors as well as the associated computational burden instead in this work we demonstrate both theoretically and empirically how to regularize a variational deep network implicitly via the optimization procedure just as for standard deep learning we fully characterize the inductive bias of stochastic gradient descent in the case of an overparametrized linear model as generalized variational inference and demonstrate the importance of the choice of parametrization finally we show empirically that our approach achieves strong in and outofdistribution performance without tuning of additional hyperparameters and with minimal time and memory overhead over standard deep learning']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title seeing is believing but how much a comprehensive analysis of verbalized calibration in visionlanguage models summary uncertainty quantification is essential for assessing the reliability and trustworthiness of modern ai systems among existing approaches verbalized uncertainty where models express their confidence through natural language has emerged as a lightweight and interpretable solution in large language models llms however its effectiveness in visionlanguage models vlms remains insufficiently studied in this work we conduct a comprehensive evaluation of verbalized confidence in vlms spanning three model categories four task domains and three evaluation scenarios our results show that current vlms often display notable miscalibration across diverse tasks and settings notably visual reasoning models ie thinking with images consistently exhibit better calibration suggesting that modalityspecific reasoning is critical for reliable uncertainty estimation to further address calibration challenges we introduce visual confidenceaware prompting a twostage prompting strategy that improves confidence alignment in multimodal settings overall our study highlights the inherent miscalibration in vlms across modalities more broadly our findings underscore the fundamental importance of modality alignment and model faithfulness in advancing reliable multimodal systems']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0, 3]+ 1) / 1][Stage 11:=========>        (2 + 2) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs', 'math']\n",
      ">>> Predicted class IDs: [4, 0]+ 1) / 1][Stage 11:=============>    (3 + 1) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics', 'cs']\n",
      ">>> Loading model and tokenizer on worker                                       \n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title consistyle: style diversity in trainingfree consistent t2i generation summary in texttoimage models consistent character generation is the task of achieving text alignment while maintaining the subjects appearance across different prompts however since style and appearance are often entangled the existing methods struggle to preserve consistent subject characteristics while adhering to varying style prompts current approaches for consistent texttoimage generation typically rely on largescale finetuning on curated image sets or persubject optimization which either fail to generalize across prompts or do not align well with textual descriptions meanwhile trainingfree methods often fail to maintain subject consistency across different styles in this work we introduce a trainingfree method that achieves both style alignment and subject consistency the attention matrices are manipulated such that queries and keys are obtained from the anchor images that are used to define the subject while the values are imported from a parallel copy that is not subjectanchored additionally crossimage components are added to the selfattention mechanism by expanding the key and value matrices to do without shifting from the target style we align the statistics of the value matrices as is demonstrated in a comprehensive battery of qualitative and quantitative experiments our method effectively decouples style from subject appearance and enables faithful generation of textaligned images with consistent characters across diverse styles']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title position: adopt constraints over penalties in deep learning summary recent efforts toward developing trustworthy ai systems with accountability guarantees have led to a growing reliance on machine learning formulations that incorporate external requirements or constraints these requirements are often enforced through penalizationadding fixedweight terms to the task loss we argue that this approach is illsuited and that tailored constrained optimization methods should be adopted instead in particular no penalty coefficient may yield a solution that both satisfies the constraints and achieves good performanceie one solving the constrained problem moreover tuning these coefficients is costly incurring significant time and computational overhead in contrast tailored constrained methodssuch as the lagrangian approach which optimizes the penalization coefficients the lagrange multipliers alongside the modeli truly solve the constrained problem and add accountability ii eliminate the need for extensive penalty tuning and iii integrate seamlessly with modern deep learning pipelines']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title fundamental limits of gametheoretic llm alignment: smith consistency and preference matching summary nash learning from human feedback is a gametheoretic framework for aligning large language models llms with human preferences by modeling learning as a twoplayer zerosum game however using raw preference as the payoff in the game highly limits the potential of the gametheoretic llm alignment framework in this paper we systematically study using what choices of payoff based on the pairwise human preferences can yield desirable alignment properties we establish necessary and sufficient conditions for condorcet consistency diversity through mixed strategies and smith consistency these results provide a theoretical foundation for the robustness of gametheoretic llm alignment further we show the impossibility of preference matching ie no smooth and learnable mappings of pairwise preferences can guarantee a unique nash equilibrium that matches a target policy even under standard assumptions like the bradleyterryluce model this result highlights the fundamental limitation of gametheoretic llm alignment']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title incorporating flexible image conditioning into texttovideo diffusion models without training summary textimagetovideo ti2v generation is a critical problem for controllable video generation using both semantic and visual conditions most existing methods typically add visual conditions to texttovideo t2v foundation models by finetuning which is costly in resources and only limited to a few predefined conditioning settings to tackle this issue we introduce a unified formulation for ti2v generation with flexible visual conditioning furthermore we propose an innovative trainingfree approach dubbed flexti2v that can condition t2v foundation models on an arbitrary amount of images at arbitrary positions specifically we firstly invert the condition images to noisy representation in a latent space then in the denoising process of t2v models our method uses a novel random patch swapping strategy to incorporate visual features into video representations through local image patches to balance creativity and fidelity we use a dynamic control mechanism to adjust the strength of visual conditioning to each video frame extensive experiments validate that our method surpasses previous trainingfree image conditioning methods by a notable margin we also show more insights of our method by detailed ablation study and analysis']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title dreamprm: domainreweighted process reward model for multimodal reasoning summary reasoning has substantially improved the performance of large language models llms on complicated tasks central to the current reasoning studies process reward models prms offer a finegrained evaluation of intermediate reasoning steps and guide the reasoning process however extending prms to multimodal large language models mllms introduces challenges since multimodal reasoning covers a wider range of tasks compared to textonly scenarios the resulting distribution shift from the training to testing sets is more severe leading to greater generalization difficulty training a reliable multimodal prm therefore demands large and diverse datasets to ensure sufficient coverage however current multimodal reasoning datasets suffer from a marked quality imbalance which degrades prm performance and highlights the need for an effective data selection strategy to address the issues we introduce dreamprm a domainreweighted training framework for multimodal prms which employs bilevel optimization in the lowerlevel optimization dreamprm performs finetuning on multiple datasets with domain weights allowing the prm to prioritize highquality reasoning signals and alleviating the impact of dataset quality imbalance in the upperlevel optimization the prm is evaluated on a separate metalearning dataset this feedback updates the domain weights through an aggregation loss function thereby improving the generalization capability of trained prm extensive experiments on multiple multimodal reasoning benchmarks covering both mathematical and general reasoning show that testtime scaling with dreamprm consistently improves the performance of stateoftheart mllms further comparisons reveal that dreamprms domainreweighting strategy surpasses other data selection methods and yields higher accuracy gains than existing testtime scaling approaches']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title redahd: reductionbased endtoend automatic heuristic design with large language models summary solving nphard combinatorial optimization problems cops eg traveling salesman problems tsps and capacitated vehicle routing problems cvrps in practice traditionally involves handcrafting heuristics or specifying a search space for finding effective heuristics the main challenges from these approaches however are the sheer amount of domain knowledge and implementation efforts required from human experts recently significant progress has been made to address these challenges particularly by using large language models llms to design heuristics within some predetermined generalized algorithmic framework gaf eg ant colony optimization and guided local search for building key functionscomponents eg a priori information on how promising it is to include each edge in a solution for tsp and cvrp although existing methods leveraging this idea have shown to yield impressive optimization performance they are not fully endtoend and still require considerable manual interventions in this paper we propose a novel endtoend framework named redahd that enables these llmbased heuristic design methods to operate without the need of gafs more specifically redahd employs llms to automate the process of reduction ie transforming the cop at hand into similar cops that are betterunderstood from which llmbased heuristic design methods can design effective heuristics for directly solving the transformed cops and in turn indirectly solving the original cop our experimental results evaluated on six cops show that redahd is capable of designing heuristics with competitive or improved results over the stateoftheart methods with minimal human involvement']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title hierarchical bayesian estimation for continual learning during modelinformed precision dosing summary model informed precision dosing mipd is a bayesian framework to individualize drug therapy based on prior knowledge and patientspecific monitoring data typically prior knowledge results from controlled clinical trials with a more homogeneous patient population compared to the realworld patient population underlying the data to be analysed thus devising algorithms that can learn the distribution underlying the realworld patient population from patientspecific monitoring data is of key importance formulating continual learning in mipd as a hierarchical bayesian estimation problem we here investigate different algorithms for the resulting marginal posterior inference problem in a pharmacokinetic context and for different data sparsity scenarios as an accurate but computationally expensive reference method a metropolishastings algorithm adapted to the hierarchical setting was used furthermore several sequential algorithms were investigated: a nested particle filter a newly developed simplification termed single inner nested particle filter as well as an approximative parametric method that allows to use metropoliswithingibbs sampling the single inner nested particle filter showed the best compromise between accuracy and computational complexity applications to more challenging mipd scenarios from cytotoxic chemotherapy and anticoagulation initiation therapy are ongoing']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title its high time: a survey of temporal information retrieval and question answering summary time plays a critical role in how information is generated retrieved and interpreted in this survey we provide a comprehensive overview of temporal information retrieval and temporal question answering two research areas aimed at handling and understanding timesensitive information as the amount of timestamped content from sources like news articles web archives and knowledge bases increases systems must address challenges such as detecting temporal intent normalizing time expressions ordering events and reasoning over evolving or ambiguous facts these challenges are critical across many dynamic and timesensitive domains from news and encyclopedias to science history and social media we review both traditional approaches and modern neural methods including those that use transformer models and large language models llms we also review recent advances in temporal language modeling multihop reasoning and retrievalaugmented generation rag alongside benchmark datasets and evaluation strategies that test temporal robustness recency awareness and generalization']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title unleashing 5g seamless integration with tsn for industry 50: frame forwarding and qos treatment summary integrating timesensitive networking tsn and 5th generation 5g systems is key for providing wireless lowlatency services in industry despite research efforts challenges remain due to the lack of commercial 5g modems supporting ethernetbased sessions tunneling mechanisms must be used to enable layer 2 connectivity between tsn islands via ipbased 5g modems furthermore harmonizing traffic classification and prioritization between tsn and 5g technologies is crucial for meeting industrial service requirements in this work we propose a virtual extensible lan vxlanbased solution to harmonize frame forwarding and quality of service qos treatment among 5g and tsn our solution supports multiple virtual local area networks vlans across several production lines furthermore it supports tsn traffic mapping into 5g qos flows we use a 5g testbed to validate the effectiveness of the adopted solution our results show the average delay introduced by the proposed mechanisms is approximately 100 mus which is significantly lower than the typical 5g packet transmission delay moreover our findings demonstrate our solution preserves qos treatment between the 5g system and tsn ensuring that the priority of 5g qos flows aligns with the priorities of industrial traffic flows']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title biaxial characterization of soft elastomers: experiments and dataadaptive configurational forces for fracture summary understanding the fracture mechanics of soft solids remains a fundamental challenge due to their complex nonlinear responses under large deformations while multiaxial loading is key to probing their mechanical behavior the role of such loading in fracture processes is still poorly understood here we present a combined experimentalcomputational framework to investigate fracture in soft elastomers under equibiaxial loading we report original equibiaxial quasistatic experiments on five elastomeric materials revealing a spectrum of material and fracture behavior from brittlelike to highly deformable response with crack tip strains exceeding 150 motivated by these observations we develop a hybrid computational testbed that mirrors the experimental setup and enables virtual biaxial tests central to this framework are two components: a dataadaptive formulation of hyperelastic energy functions that flexibly captures material behavior and a postprocessing implementation of the configurational force method providing a computationally efficient estimate of the jintegral at the crack tip our dataadaptive framework for hyperelastic energy functions proves versatility to capture with high accuracy the hyperelastic behavior observed in the biaxial experiments this is important because accurately capturing the constitutive behaviour of soft solids is key for a reliable application of the configurational force method to soft solids in the limit of crack onset a critical value of the crack tip configurational force allows for a criterion of fracture toughness together our experimental theoretical and computational contributions offer a new paradigm for characterizing and designing soft materials with tailored fracture properties']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title institutionalizing folk theories of algorithms: how multichannel networks mcns govern algorithmic labor in chinese livestreaming industry summary as algorithmic systems increasingly structure platform labor workers often rely on informal folk theories experiencebased beliefs about how algorithms work to navigate opaque and unstable algorithmic environments prior research has largely treated these theories as bottomup peerdriven strategies for coping with algorithmic opacity and uncertainty in this study we shift analytical attention to intermediary organizations and examine how folk theories of algorithms can be institutionally constructed and operationalized by those organizations as tools of labor management drawing on nine months of ethnographic fieldwork and 37 interviews with livestreamers and staff at multichannel networks mcns in china we show that mcns develop and circulate dual algorithmic theories: internally they acknowledge the volatility of platform systems and adopt probabilistic strategies to manage risk externally they promote simplified prescriptive theories portraying the algorithm as transparent fair and responsive to individual effort they have further operationalize those folk theories for labor management encouraging streamers to selfdiscipline and invest in equipment training and routines while absolving mcns of accountability we contribute to cscw and platform labor literature by demonstrating how informal algorithmic knowledge once institutionalized can become infrastructures of soft control shaping not only how workers interpret platform algorithms but also how their labor is structured moralized and governed']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title knowtrace: bootstrapping iterative retrievalaugmented generation with structured knowledge tracing summary recent advances in retrievalaugmented generation rag furnish large language models llms with iterative retrievals of relevant information to handle complex multihop questions these methods typically alternate between llm reasoning and retrieval to accumulate external information into the llms context however the evergrowing context inherently imposes an increasing burden on the llm to perceive connections among critical information pieces with futile reasoning steps further exacerbating this overload issue in this paper we present knowtrace an elegant rag framework to 1 mitigate the context overload and 2 bootstrap higherquality multistep reasoning instead of simply piling the retrieved contents knowtrace autonomously traces out desired knowledge triplets to organize a specific knowledge graph relevant to the input question such a structured workflow not only empowers the llm with an intelligible context for inference but also naturally inspires a reflective mechanism of knowledge backtracing to identify contributive llm generations as process supervision data for selfbootstrapping extensive experiments show that knowtrace consistently surpasses existing methods across three multihop question answering benchmarks and the bootstrapped version further amplifies the gains']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title on path to multimodal historical reasoning: histbench and histagent summary recent advances in large language models llms have led to remarkable progress across domains yet their capabilities in the humanities particularly history remain underexplored historical reasoning poses unique challenges for ai involving multimodal source interpretation temporal inference and crosslinguistic analysis while generalpurpose agents perform well on many existing benchmarks they lack the domainspecific expertise required to engage with historical materials and questions to address this gap we introduce histbench a new benchmark of 414 highquality questions designed to evaluate ais capacity for historical reasoning and authored by more than 40 expert contributors the tasks span a wide range of historical problemsfrom factual retrieval based on primary sources to interpretive analysis of manuscripts and images to interdisciplinary challenges involving archaeology linguistics or cultural history furthermore the benchmark dataset spans 29 ancient and modern languages and covers a wide range of historical periods and world regions finding the poor performance of llms and other agents on histbench we further present histagent a historyspecific agent equipped with carefully designed tools for ocr translation archival search and image understanding in history on histbench histagent based on gpt4o achieves an accuracy of 2754 pass1 and 3647 pass2 significantly outperforming llms with online search and generalist agents including gpt4o 1860 deepseekr11449 and open deep researchsmolagents2029 pass1 and 2512 pass2 these results highlight the limitations of existing llms and generalist agents and demonstrate the advantages of histagent for historical reasoning']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title long context scaling: divide and conquer via multiagent questiondriven collaboration summary processing long contexts has become a critical capability for modern large language models llms existing works leverage agentbased divideandconquer methods for processing long contexts but these methods face crucial limitations including prohibitive accumulated latency and amplified information loss from excessive agent invocations and the disruption of inherent textual dependencies by immoderate partitioning in this paper we propose a novel multiagent framework xpanda expandagent coupled with questiondriven workflow and dynamic partitioning for robust longcontext processing xpanda overcomes these limitations through: 1 dynamic partitioning of long texts which adaptively modulates the filling rate of context windows for input sequences of vastly varying lengths 2 questionguided protocol to update flat information ensembles within centralized shared memory constructing consistent interagent knowledge across partitions and 3 selectively replaying specific partitions based on the statetracking of questioninformation couples to promote the resolution of invertedorder structures across partitions eg flashbacks we perform a comprehensive evaluation of xpanda on multiple longcontext benchmarks with length varying from 1k to 1m demonstrating xpandas feasibility for processing ultralong sequences and its significant effectiveness in enhancing the longcontext capabilities of various llms by achieving 20 improvements and 15x inference speedup over baselines of fullcontext rag and previous agentbased methods']\n",
      "25/05/30 18:23:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:53 WARN BlockManager: Block input-0-1748622233000 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title polar: a benchmark for multilingual multicultural and multievent online polarization summary online polarization poses a growing challenge for democratic discourse yet most computational social science research remains monolingual culturally narrow or eventspecific we introduce polar a multilingual multicultural and multievent dataset with over 23k instances in seven languages from diverse online platforms and realworld events polarization is annotated along three axes: presence type and manifestation using a variety of annotation platforms adapted to each cultural context we conduct two main experiments: 1 we finetune six multilingual pretrained language models in both monolingual and crosslingual setups and 2 we evaluate a range of open and closed large language models llms in fewshot and zeroshot scenarios results show that while most models perform well on binary polarization detection they achieve substantially lower scores when predicting polarization types and manifestations these findings highlight the complex highly contextual nature of polarization and the need for robust adaptable approaches in nlp and computational social science all resources will be released to support further research and effective mitigation of digital polarization globally']\n",
      "25/05/30 18:23:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:53 WARN BlockManager: Block input-0-1748622233200 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 12:>               (0 + 15) / 15]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [7]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['stat']\n",
      "25/05/30 18:23:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:53 WARN BlockManager: Block input-0-1748622233400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [2](0 + 1) / 1][Stage 12:=====>          (5 + 10) / 15]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['eess']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:53 WARN BlockManager: Block input-0-1748622233600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:54 WARN BlockManager: Block input-0-1748622233800 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------+\n",
      "|title                                                                                                                                           |main_category|predicted_category|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------+\n",
      "|Multimodal Federated Learning With Missing Modalities through Feature\\n  Imputation Network                                                     |cs.LG        |cs                |\n",
      "|Resonances in Lifetimes of AdS Oscillon                                                                                                         |hep-th       |physics           |\n",
      "|Derivations for the MPS overlap formulas of rational spin chains                                                                                |hep-th       |physics           |\n",
      "|Variational Deep Learning via Implicit Regularization                                                                                           |cs.LG        |cs                |\n",
      "|Seeing is Believing, but How Much? A Comprehensive Analysis of\\n  Verbalized Calibration in Vision-Language Models                              |cs.CV        |cs                |\n",
      "|Efficient Speech Translation through Model Compression and Knowledge\\n  Distillation                                                            |cs.CL        |cs                |\n",
      "|On Certain Problems in the Theory of Root Clusters                                                                                              |math.NT      |math              |\n",
      "|Unleashing 5G Seamless Integration with TSN for Industry 5.0: Frame\\n  Forwarding and QoS Treatment                                             |cs.NI        |eess              |\n",
      "|Hierarchical Bayesian estimation for continual learning during\\n  model-informed precision dosing                                               |stat.CO      |stat              |\n",
      "|DreamPRM: Domain-Reweighted Process Reward Model for Multimodal\\n  Reasoning                                                                    |cs.LG        |cs                |\n",
      "|RedAHD: Reduction-Based End-to-End Automatic Heuristic Design with Large\\n  Language Models                                                     |cs.LG        |cs                |\n",
      "|It's High Time: A Survey of Temporal Information Retrieval and Question\\n  Answering                                                            |cs.CL        |cs                |\n",
      "|Biaxial characterization of soft elastomers: experiments and\\n  data-adaptive configurational forces for fracture                               |cond-mat.soft|physics           |\n",
      "|KnowTrace: Bootstrapping Iterative Retrieval-Augmented Generation with\\n  Structured Knowledge Tracing                                          |cs.CL        |cs                |\n",
      "|On Path to Multimodal Historical Reasoning: HistBench and HistAgent                                                                             |cs.AI        |cs                |\n",
      "|Institutionalizing Folk Theories of Algorithms: How Multi-Channel\\n  Networks (MCNs) Govern Algorithmic Labor in Chinese Live-Streaming Industry|cs.HC        |cs                |\n",
      "|POLAR: A Benchmark for Multilingual, Multicultural, and Multi-Event\\n  Online Polarization                                                      |cs.CL        |cs                |\n",
      "|Long Context Scaling: Divide and Conquer via Multi-Agent Question-driven\\n  Collaboration                                                       |cs.CL        |cs                |\n",
      "|ConsiStyle: Style Diversity in Training-Free Consistent T2I Generation                                                                          |cs.CV        |cs                |\n",
      "|Fundamental Limits of Game-Theoretic LLM Alignment: Smith Consistency\\n  and Preference Matching                                                |cs.GT        |cs                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2025-05-30 18:23:50 =========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title testtime learning for large language models summary while large language models llms have exhibited remarkable emergent capabilities through extensive pretraining they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations known as distribution shifts in this paper we propose a testtime learning ttl paradigm for llms namely tlm which dynamically adapts llms to target domains using only unlabeled test data during testing specifically we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from llms can be achieved by minimizing the input perplexity of the unlabeled test data based on this insight we formulate the testtime learning process of llms as input perplexity minimization enabling selfsupervised enhancement of llm performance furthermore we observe that highperplexity samples tend to be more informative for model optimization accordingly we introduce a sample efficient learning strategy that actively selects and emphasizes these highperplexity samples for testtime updates lastly to mitigate catastrophic forgetting and ensure adaptation stability we adopt lowrank adaptation lora instead of fullparameter optimization which allows lightweight model updates while preserving more original knowledge from the model we introduce the adapteval benchmark for ttl and demonstrate through experiments that tlm improves performance by at least 20 compared to original llms on domain knowledge adaptation']\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 14:>                 (0 + 1) / 1]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loading model and tokenizer on worker                                       \n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title explaining concept shift with interpretable feature attribution summary regardless the amount of data a machine learning ml model is trained on there will inevitably be data that differs from their training set lowering model performance concept shift occurs when the distribution of labels conditioned on the features changes making even a welltuned ml model to have learned a fundamentally incorrect representation identifying these shifted features provides unique insight into how one dataset differs from another considering the difference may be across a scientifically relevant dimension such as time disease status population etc in this paper we propose sgshift a model for detecting concept shift in tabular data and attributing reduced model performance to a sparse set of shifted features sgshift models concept shift with a generalized additive model gam and performs subsequent feature selection to identify shifted features we propose further extensions of sgshift by incorporating knockoffs to control false discoveries and an absorption term to account for models with poor fit to the data we conduct extensive experiments in synthetic and real data across various ml models and find sgshift can identify shifted features with auc 09 and recall 90 often 2 or 3 times as high as baseline methods']\n",
      "25/05/30 18:23:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:55 WARN BlockManager: Block input-0-1748622235000 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title trustskin: a fairness pipeline for trustworthy facial affect analysis across skin tone summary understanding how facial affect analysis faa systems perform across different demographic groups requires reliable measurement of sensitive attributes such as ancestry often approximated by skin tone which itself is highly influenced by lighting conditions this study compares two objective skin tone classification methods: the widely used individual typology angle ita and a perceptually grounded alternative based on lightness l and hue h using affectnet and a mobilenetbased model we assess fairness across skin tone groups defined by each method results reveal a severe underrepresentation of dark skin tones sim 2 alongside fairness disparities in f1score up to 008 and tpr up to 011 across groups while ita shows limitations due to its sensitivity to lighting the hl method yields more consistent subgrouping and enables clearer diagnostics through metrics such as equal opportunity gradcam analysis further highlights differences in model attention patterns by skin tone suggesting variation in feature encoding to support future mitigation efforts we also propose a modular fairnessaware pipeline that integrates perceptual skin tone estimation model interpretability and fairness evaluation these findings emphasize the relevance of skin tone measurement choices in fairness assessment and suggest that itabased evaluations may overlook disparities affecting darkerskinned individuals']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title plugandplay cooccurring face attention for robust audiovisual speaker extraction summary audiovisual speaker extraction isolates a target speakers speech from a mixture speech signal conditioned on a visual cue typically using the target speakers face recording however in realworld scenarios other cooccurring faces are often present onscreen providing valuable speaker activity cues in the scene in this work we introduce a plugandplay interspeaker attention module to process these flexible numbers of cooccurring faces allowing for more accurate speaker extraction in complex multiperson environments we integrate our module into two prominent models: the avdprnn and the stateoftheart avtfgridnet extensive experiments on diverse datasets including the highly overlapped voxceleb2 and sparsely overlapped misp demonstrate that our approach consistently outperforms baselines furthermore crossdataset evaluations on lrs2 and lrs3 confirm the robustness and generalizability of our method']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title ofdmpass: frequencyselective modeling and analysis for pinchingantenna systems summary pinching antenna systems pass which utilize dielectric waveguides for adaptable antenna deployment offer novel opportunities to create controllable lineofsight links this letter is the first to investigate the integration of pass with orthogonal frequency division multiplexing ofdm to ensure their compatibility and to explore the frequencyselective behavior inherent to pass first an endtoend channel model is proposed for ofdm pass based on electromagneticcompliant modeling of waveguides and coupledmode theory which includes frequencydependent waveguide dispersion and antenna coupling effect furthermore a critical dependence of the ofdm cyclic prefix cp overhead on the proximity of the operating frequency to the waveguide cutoff is revealed moreover this letter evaluates an approximate pinching antenna location strategy based on path loss minimization from which the phase misalignment effect across subcarriers in wideband scenarios is derived numerical results show that: 1 frequencyselective effects in ofdm pass lead to substantial variations in subcarrier achievable rates highlighting the necessity of operating above the waveguide cutoff frequency for effective communication 2 frequencyindependent pinching antenna location approximation incurs significant phase misalignment particularly for wider bandwidths and larger array sizes and 3 waveguide dispersion mandates considerable cp overhead when operating near the cutoff frequency severely impacting the spectral efficiency of ofdm pass']\n",
      "25/05/30 18:23:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:55 WARN BlockManager: Block input-0-1748622235200 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      "25/05/30 18:23:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:55 WARN BlockManager: Block input-0-1748622235400 replicated to only 0 peer(s) instead of 1 peers\n",
      ">>> Predicted class IDs: [0](0 + 1) / 1][Stage 15:=========>        (2 + 2) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [2]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['eess']\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=73>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/pyspark/context.py\", line 381, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "    ^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o17.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 193, in manager\n",
      "    code = worker(sock, authenticated)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 79, in worker\n",
      "    outfile.flush()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 204, in manager\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 204, in manager\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 8.0 in stage 16.0 (TID 88)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 17.0 in stage 16.0 (TID 97)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 9.0 in stage 16.0 (TID 89)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 3.0 in stage 16.0 (TID 83)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 7.0 in stage 16.0 (TID 87)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:55 WARN BlockManager: Block input-0-1748622235600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 15.0 in stage 16.0 (TID 95)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 14.0 in stage 16.0 (TID 94)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 11.0 in stage 16.0 (TID 91)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 2.0 in stage 16.0 (TID 82)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 18.0 in stage 16.0 (TID 98)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 16.0 in stage 16.0 (TID 96)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 4.0 in stage 16.0 (TID 84)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 5.0 in stage 16.0 (TID 85)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 80)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 6.0 in stage 16.0 (TID 86)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 12.0 in stage 16.0 (TID 92)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 19.0 in stage 16.0 (TID 99)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "25/05/30 18:23:55 ERROR Executor: Exception in task 10.0 in stage 16.0 (TID 90)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/daemon.py\", line 204, in manager\n",
      "    gc.collect()\n",
      "KeyboardInterrupt\n",
      "25/05/30 18:23:55 WARN TaskSetManager: Lost task 3.0 in stage 16.0 (TID 83) (192.168.10.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "25/05/30 18:23:55 ERROR TaskSetManager: Task 3 in stage 16.0 failed 1 times; aborting job\n",
      "25/05/30 18:23:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:56 WARN BlockManager: Block input-0-1748622235800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:56 WARN TaskSetManager: Lost task 1.0 in stage 16.0 (TID 81) (192.168.10.236 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 16.0 failed 1 times, most recent failure: Lost task 3.0 in stage 16.0 (TID 83) (192.168.10.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/05/30 18:23:56 WARN TaskSetManager: Lost task 13.0 in stage 16.0 (TID 93) (192.168.10.236 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 16.0 failed 1 times, most recent failure: Lost task 3.0 in stage 16.0 (TID 83) (192.168.10.236 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.python.BatchIterator.hasNext(ArrowEvalPythonExec.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream(PythonArrowInput.scala:131)\n",
      "\tat org.apache.spark.sql.execution.python.BasicPythonArrowInput.writeIteratorToArrowStream$(PythonArrowInput.scala:124)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner.writeIteratorToArrowStream(ArrowPythonRunner.scala:30)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.$anonfun$writeIteratorToStream$1(PythonArrowInput.scala:96)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowInput$$anon$1.writeIteratorToStream(PythonArrowInput.scala:102)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\n",
      "\n",
      "Driver stacktrace:)\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o32.awaitTermination",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 187\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSpark Streaming Context started.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    186\u001b[39m ssc.start()\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[43mssc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/pyspark/streaming/context.py:239\u001b[39m, in \u001b[36mStreamingContext.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    230\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03mWait for the execution to stop.\u001b[39;00m\n\u001b[32m    232\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    236\u001b[39m \u001b[33;03m    time to wait in seconds\u001b[39;00m\n\u001b[32m    237\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jssc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    241\u001b[39m     \u001b[38;5;28mself\u001b[39m._jssc.awaitTerminationOrTimeout(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1321\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdeco\u001b[39m(*a: Any, **kw: Any) -> Any:\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    181\u001b[39m         converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/spark/.pixi/envs/default/lib/python3.11/site-packages/py4j/protocol.py:334\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    338\u001b[39m     \u001b[38;5;28mtype\u001b[39m = answer[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling o32.awaitTermination"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing RDD: \n",
      "  An exception was thrown from the Python worker. Please see the stack trace below.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/worker.py\", line 1094, in main\n",
      "    split_index = read_int(infile)\n",
      "                  ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/nika/Desktop/spark/spark-3.5.5-bin-hadoop3/python/lib/pyspark.zip/pyspark/serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 18:23:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:57 WARN BlockManager: Block input-0-1748622237000 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:57 WARN BlockManager: Block input-0-1748622237200 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:57 WARN BlockManager: Block input-0-1748622237400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:57 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:57 WARN BlockManager: Block input-0-1748622237600 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:58 WARN BlockManager: Block input-0-1748622237800 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:59 WARN BlockManager: Block input-0-1748622239200 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2025-05-30 18:24:00 =========\n",
      "+---------------------------------------------------------------------------------------------------------------------+-------------+------------------+\n",
      "|title                                                                                                                |main_category|predicted_category|\n",
      "+---------------------------------------------------------------------------------------------------------------------+-------------+------------------+\n",
      "|Enhancing Transformation from Natural Language to Signal Temporal Logic\\n  Using LLMs with Diverse External Knowledge|cs.CL        |cs                |\n",
      "|An Optimisation Framework for Unsupervised Environment Design                                                        |cs.LG        |cs                |\n",
      "|BacktrackAgent: Enhancing GUI Agent with Error Detection and\\n  Backtracking Mechanism                               |cs.CL        |cs                |\n",
      "|Positive Mass in Scalar-Torsion Holography                                                                           |hep-th       |physics           |\n",
      "|AutoReproduce: Automatic AI Experiment Reproduction with Paper Lineage                                               |cs.AI        |cs                |\n",
      "|TeroSeek: An AI-Powered Knowledge Base and Retrieval Generation Platform\\n  for Terpenoid Research                   |cs.IR        |cs                |\n",
      "|Self-Route: Automatic Mode Switching via Capability Estimation for\\n  Efficient Reasoning                            |cs.CL        |cs                |\n",
      "|DriveRX: A Vision-Language Reasoning Model for Cross-Task Autonomous\\n  Driving                                      |cs.CV        |cs                |\n",
      "|Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence\\n  Transformers                                   |cs.LG        |cs                |\n",
      "|How Do Experts Make Sense of Integrated Process Models?                                                              |cs.IR        |cs                |\n",
      "|Eigenstructure inference for high-dimensional covariance with\\n  generalized shrinkage inverse-Wishart prior         |math.ST      |stat              |\n",
      "|Mapping the Star Formation and HI Gas Properties of Galaxies Along\\n  Large-scale Structures Around the Virgo Cluster|astro-ph.GA  |physics           |\n",
      "|MIRROR: Multi-agent Intra- and Inter-Reflection for Optimized Reasoning\\n  in Tool Learning                          |cs.AI        |cs                |\n",
      "|LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks\\n  through Policy Modulation                      |cs.AI        |cs                |\n",
      "|GIFARC: Synthetic Dataset for Leveraging Human-Intuitive Analogies to\\n  Elevate AI Reasoning                        |cs.AI        |cs                |\n",
      "|A Unified RCS Modeling of Typical Targets for 3GPP ISAC Channel\\n  Standardization and Experimental Analysis         |eess.SP      |eess              |\n",
      "|Pretraining Language Models to Ponder in Continuous Space                                                            |cs.CL        |cs                |\n",
      "|Contrastive Desensitization Learning for Cross Domain Face Forgery\\n  Detection                                      |cs.CV        |cs                |\n",
      "|Supervised Contrastive Learning for Ordinal Engagement Measurement                                                   |cs.CV        |cs                |\n",
      "|Holographic superconductor with dark matter probed by entanglement\\n  entropy in higher dimensional AdS spacetime    |hep-th       |physics           |\n",
      "+---------------------------------------------------------------------------------------------------------------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "========= 2025-05-30 18:24:10 =========\n",
      "RDD is empty, skipping.\n",
      "========= 2025-05-30 18:24:20 =========\n",
      "RDD is empty, skipping.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pyspark.sql.functions import col, lit, lower, concat, regexp_replace, pandas_udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.streaming import StreamingContext\n",
    "import findspark\n",
    "import pyspark\n",
    "import json\n",
    "\n",
    "# ---- PATH SETUP ----\n",
    "cwd = Path.cwd()\n",
    "spark_home = cwd / \"spark-3.5.5-bin-hadoop3\"\n",
    "hadoop_home = cwd / \"winutils\"\n",
    "\n",
    "spark_home_str = str(spark_home.resolve())\n",
    "hadoop_home_str = str(hadoop_home.resolve())\n",
    "\n",
    "print(f\"I am using the following SPARK_HOME: {spark_home_str}\")\n",
    "if os.name == 'nt':\n",
    "    os.environ[\"HADOOP_HOME\"] = hadoop_home_str\n",
    "    print(f\"Windows detected: set HADOOP_HOME to: {hadoop_home_str}\")\n",
    "    hadoop_bin = hadoop_home / \"bin\"\n",
    "    os.environ[\"PATH\"] = f\"{hadoop_bin};{os.environ['PATH']}\"\n",
    "    print(f\"  Also added Hadoop bin directory to PATH: {hadoop_bin}\")\n",
    "\n",
    "# ---- INIT SPARK ----\n",
    "findspark.init(spark_home_str)\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"DistilBERTStreamingPredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# ---- Enable Arrow for pandas_udf ----\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# ---- SCHEMA ----\n",
    "schema = StructType([\n",
    "    StructField(\"aid\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"categories\", StringType(), True),\n",
    "    StructField(\"published\", StringType(), True)\n",
    "])\n",
    "\n",
    "# ---- BROADCAST MODEL PATH ----\n",
    "model_path = \"results\"\n",
    "model_path_broadcast = sc.broadcast(model_path)\n",
    "\n",
    "# ---- DEVICE SETUP ----\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- MODEL CACHING FOR WORKERS ----\n",
    "model_cache = None\n",
    "tokenizer_cache = None\n",
    "id2label_cache = None\n",
    "\n",
    "@pandas_udf(StringType())\n",
    "def predict_udf_batch(texts: pd.Series) -> pd.Series:\n",
    "    global model_cache, tokenizer_cache, id2label_cache\n",
    "\n",
    "    if model_cache is None:\n",
    "        try:\n",
    "            tokenizer_cache = AutoTokenizer.from_pretrained(model_path_broadcast.value)\n",
    "            model_cache = AutoModelForSequenceClassification.from_pretrained(model_path_broadcast.value)\n",
    "            model_cache.eval()\n",
    "            model_cache.to(device)\n",
    "\n",
    "            # Load id2label from config.json or fallback to JSON\n",
    "            id2label_cache = model_cache.config.id2label\n",
    "            if not id2label_cache:\n",
    "                id2label_path = os.path.join(model_path_broadcast.value, \"id2label.json\")\n",
    "                print(f\">>> model.config.id2label was empty. Attempting to load from {id2label_path}\")\n",
    "                if os.path.exists(id2label_path):\n",
    "                    with open(id2label_path, \"r\") as f:\n",
    "                        id2label_cache = json.load(f)\n",
    "                else:\n",
    "                    print(\">>> id2label.json not found. id2label will remain empty.\")\n",
    "                    id2label_cache = {}\n",
    "\n",
    "            # Convert keys to int once for consistency\n",
    "            id2label_cache = {int(k): v for k, v in id2label_cache.items()}\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Worker Init Error] {e}\")\n",
    "            return pd.Series([\"ERROR_LOADING_MODEL\"] * len(texts))\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer_cache(texts.tolist(), return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model_cache(**inputs).logits\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1).tolist()\n",
    "        labels = [id2label_cache.get(p, \"UNKNOWN\") for p in preds]\n",
    "\n",
    "        return pd.Series(labels)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Inference Error] {e}\")\n",
    "        return pd.Series([\"ERROR\"] * len(texts))\n",
    "\n",
    "# ---- STREAMING LOGIC ----\n",
    "socket_host = \"seppe.net\"\n",
    "socket_port = 7778\n",
    "\n",
    "def process_rdd(time, rdd):\n",
    "    print(f\"-------- {str(time)} --------\")\n",
    "    if rdd.isEmpty():\n",
    "        print(\"RDD is empty, skipping.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df = spark.read.json(rdd, schema=schema)\n",
    "\n",
    "        processed_df = df.withColumn(\n",
    "            'combined',\n",
    "            lower(concat(lit(\" [TITLE] \"), col('title'), lit(\" [SUMMARY] \"), col('summary')))\n",
    "        ).withColumn(\"combined\", regexp_replace(col(\"combined\"), \"[^a-zA-Z0-9\\\\s:]\", \"\")) \\\n",
    "         .withColumn(\"combined\", regexp_replace(col(\"combined\"), \"\\\\s+\", \" \"))\n",
    "\n",
    "        df_withpreds = processed_df.withColumn(\"predicted_category\", predict_udf_batch(col(\"combined\")))\n",
    "\n",
    "        df_withpreds.select(\"title\", \"main_category\", \"predicted_category\").show(truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing RDD: {e}\")\n",
    "\n",
    "# ---- STREAM SETUP ----\n",
    "lines = ssc.socketTextStream(socket_host, socket_port)\n",
    "lines.foreachRDD(process_rdd)\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d752cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/30 18:23:59 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "25/05/30 18:23:59 WARN BlockManager: Block input-0-1748622239400 replicated to only 0 peer(s) instead of 1 peers\n",
      "25/05/30 18:23:59 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:295)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:337)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:179)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "25/05/30 18:23:59 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "25/05/30 18:23:59 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:295)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:337)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:179)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "25/05/30 18:23:59 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      ">>> Loading model and tokenizer on worker                           (0 + 1) / 1]\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title enhancing transformation from natural language to signal temporal logic using llms with diverse external knowledge summary temporal logic tl especially signal temporal logic stl enables precise formal specification making it widely used in cyberphysical systems such as autonomous driving and robotics automatically transforming nl into stl is an attractive approach to overcome the limitations of manual transformation which is timeconsuming and errorprone however due to the lack of datasets automatic transformation currently faces significant challenges and has not been fully explored in this paper we propose an nlstl dataset named stldiversityenhanced stldiven which comprises 16000 samples enriched with diverse patterns to develop the dataset we first manually create a smallscale seed set of nlstl pairs next representative examples are identified through clustering and used to guide large language models llms in generating additional nlstl pairs finally diversity and accuracy are ensured through rigorous rulebased filters and human validation furthermore we introduce the knowledgeguided stl transformation kgst framework a novel approach for transforming natural language into stl involving a generatethenrefine process based on external knowledge statistical analysis shows that the stldiven dataset exhibits more diversity than the existing nlstl dataset moreover both metricbased and human evaluations indicate that our kgst approach outperforms baseline models in transformation accuracy on stldiven and deepstl datasets']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loading model and tokenizer on worker                                       \n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title backtrackagent: enhancing gui agent with error detection and backtracking mechanism summary graphical user interface gui agents have gained substantial attention due to their impressive capabilities to complete tasks through multiple interactions within gui environments however existing agents primarily focus on enhancing the accuracy of individual actions and often lack effective mechanisms for detecting and recovering from errors to address these shortcomings we propose the backtrackagent a robust framework that incorporates a backtracking mechanism to improve task completion efficiency backtrackagent includes verifier judger and reflector components as modules for error detection and recovery while also applying judgment rewards to further enhance the agents performance additionally we develop a training dataset specifically designed for the backtracking mechanism which considers the outcome pages after action executions experimental results show that backtrackagent has achieved performance improvements in both task success rate and step accuracy on mobile3m and autoui benchmarks our data and code will be released upon acceptance']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loading model and tokenizer on worker                           (1 + 3) / 4]\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title an optimisation framework for unsupervised environment design summary for reinforcement learning agents to be deployed in highrisk settings they must achieve a high level of robustness to unfamiliar scenarios one method for improving robustness is unsupervised environment design ued a suite of methods aiming to maximise an agents generalisability across configurations of an environment in this work we study ued from an optimisation perspective providing stronger theoretical guarantees for practical settings than prior work whereas previous methods relied on guarantees if they reach convergence our framework employs a nonconvexstronglyconcave objective for which we provide a provably convergent algorithm in the zerosum setting we empirically verify the efficacy of our method outperforming prior methods in a number of environments with varying difficulties']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title positive mass in scalartorsion holography summary we investigate the holographic renormalization of scalartorsion gravity in a fourdimensional bulk spacetime with nonminimal derivative coupling the asymptotic behavior of the static equations leads to an antide sitter geometry for negative cosmological constants allowing for a holographic interpretation via the adscft correspondence the existence of unique solutions throughout the bulk is addressed we study the effect of the nonminimal coupling parameter on the conformal dimension and the expectation value of the dual scalar operator showing that the effective bulk mass can be tuned through the nonminimal coupling our results provide a formalism for finding nontachyonic bulk scalar fields which vanish at the boundary']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title autoreproduce: automatic ai experiment reproduction with paper lineage summary efficient experiment reproduction is critical to accelerating progress in artificial intelligence however the inherent complexity of method design and training procedures presents substantial challenges for automation notably reproducing experiments often requires implicit domainspecific knowledge not explicitly documented in the original papers to address this we introduce the paper lineage algorithm which identifies and extracts implicit knowledge from the relevant references cited by the target paper building on this idea we propose autoreproduce a multiagent framework capable of automatically reproducing experiments described in research papers in an endtoend manner autoreproduce enhances code executability by generating unit tests alongside the reproduction process to evaluate the reproduction capability we construct reproducebench a benchmark annotated with verified implementations and introduce novel evaluation metrics to assess both the reproduction and execution fidelity experimental results demonstrate that autoreproduce outperforms the existing strong agent baselines on all five evaluation metrics by a peak margin of over 70 in particular compared to the official implementations autoreproduce achieves an average performance gap of 221 on 8974 of the executable experiment runs the code will be available at https:githubcomai9starsautoreproduce']\n",
      ">>> Predicted class IDs: [4]===========>                            (2 + 2) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Predicted class IDs: [0]=========================>              (3 + 1) / 4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loading model and tokenizer on worker                                       \n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title mapping the star formation and hi gas properties of galaxies along largescale structures around the virgo cluster summary we investigate the star formation and neutral atomic hydrogen hi gas properties of galaxies along three largescale filaments and two galaxy groups in the wide field around the virgo cluster our goal is to understand how galaxies are processed in lowdensity environments before falling into highdensity regions combining the spatial distribution of galaxies with multiwavelength colors such as w3w1 nuvr and gr we find a predominance of blue galaxies across the structures indicating normaltoenhanced star formation similar to that of isolated galaxies however one filament and one group show a significant number of red galaxies 32 and 20 respectively suggesting that star formation has been suppressed in lowdensity environments before reaching highdensity regions intriguingly these red galaxies span a wide range of stellar masses and the presence of red dwarfs support that not only mass but also environment plays an important role in the quenching of star formation in cluster outskirts one particular filament potentially connected to virgo already has a group of red populations outside virgos r200 making these galaxies good candidates for being preprocessed before entering the virgo cluster in addition several galaxies in the filaments and groups possess relatively low hi gas contents similar to cluster galaxies however the overall fraction of hideficient galaxies is not as significantly high as the fraction of red galaxies in these structures this suggests that hi gas properties are less influenced by the environment than star formation properties in lowdensity regions possibly due to gas replenishment through accretion']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title supervised contrastive learning for ordinal engagement measurement summary student engagement plays a crucial role in the successful delivery of educational programs automated engagement measurement helps instructors monitor student participation identify disengagement and adapt their teaching strategies to enhance learning outcomes effectively this paper identifies two key challenges in this problem: class imbalance and incorporating order into engagement levels rather than treating it as mere categories then a novel approach to videobased student engagement measurement in virtual learning environments is proposed that utilizes supervised contrastive learning for ordinal classification of engagement various affective and behavioral features are extracted from video samples and utilized to train ordinal classifiers within a supervised contrastive learning framework with a sequential classifier as the encoder a key step involves the application of diverse timeseries data augmentation techniques to these feature vectors enhancing model training the effectiveness of the proposed method was evaluated using a publicly available dataset for engagement measurement daisee containing videos of students who participated in virtual learning programs the results demonstrate the robust ability of the proposed method for the classification of the engagement level this approach promises a significant contribution to understanding and enhancing student engagement in virtual learning environments']\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title driverx: a visionlanguage reasoning model for crosstask autonomous driving summary autonomous driving requires realtime robust reasoning across perception prediction planning and behavior however conventional endtoend models fail to generalize in complex scenarios due to the lack of structured reasoning recent visionlanguage models vlms have been applied to driving tasks but they typically rely on isolated modules and static supervision limiting their ability to support multistage decisionmaking we present autodriverl a unified training framework that formulates autonomous driving as a structured reasoning process over four core tasks each task is independently modeled as a visionlanguage questionanswering problem and optimized using taskspecific reward models enabling finegrained reinforcement signals at different reasoning stages within this framework we train driverx a crosstask reasoning vlm designed for realtime decisionmaking driverx achieves strong performance on a public benchmark outperforming gpt4o in behavior reasoning and demonstrating robustness under complex or corrupted driving conditions our analysis further highlights the impact of vision encoder design and rewardguided reasoning compression we will release the autodriverl framework and the driverx model to support future research']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title selfroute: automatic mode switching via capability estimation for efficient reasoning summary while reasoningaugmented large language models rllms significantly enhance complex task performance through extended reasoning chains they inevitably introduce substantial unnecessary token consumption particularly for simpler problems where short chainofthought short cot suffices this overthinking phenomenon leads to inefficient resource usage without proportional accuracy gains to address this issue we propose selfroute a dynamic reasoning framework that automatically selects between general and reasoning modes based on model capability estimation our approach introduces a lightweight preinference stage to extract capabilityaware embeddings from hidden layer representations enabling realtime evaluation of the models ability to solve problems we further construct gradient10k a model difficulty estimationbased dataset with dense complexity sampling to train the router for precise capability boundary detection extensive experiments demonstrate that selfroute achieves comparable accuracy to reasoning models while reducing token consumption by 3055 across diverse benchmarks the proposed framework demonstrates consistent effectiveness across models with different parameter scales and reasoning paradigms highlighting its general applicability and practical value']\n",
      ">>> Predicted class IDs: [0]                                      (0 + 17) / 17]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]                                      (1 + 16) / 17]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Loading model and tokenizer on worker                         (4 + 13) / 17]\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loading model and tokenizer on worker\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title promptevc: controllable emotional voice conversion with natural language prompts summary controllable emotional voice conversion evc aims to manipulate emotional expressions to increase the diversity of synthesized speech existing methods typically rely on predefined labels reference audios or prespecified factor values often overlooking individual differences in emotion perception and expression in this paper we introduce promptevc that utilizes natural language prompts for precise and flexible emotion control to bridge text descriptions with emotional speech we propose emotion descriptor and prompt mapper to generate finegrained emotion embeddings trained jointly with reference embeddings to enhance naturalness we present a prosody modeling and control pipeline that adjusts the rhythm based on linguistic content and emotional cues additionally a speaker encoder is incorporated to preserve identity experimental results demonstrate that promptevc outperforms stateoftheart controllable evc methods in emotion conversion intensity control mixed emotion synthesis and prosody manipulation speech samples are available at https:jeremychee4githubiopromptevc']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title how do experts make sense of integrated process models summary a range of integrated modeling approaches have been developed to enable a holistic representation of business process logic together with all relevant business rules these approaches address inherent problems with separate documentation of business process models and business rules in this study we explore how expert process workers make sense of the information provided through such integrated modeling approaches to do so we complement verbal protocol analysis with eyetracking metrics to reveal nuanced user behaviours involved in the main phases of sensemaking namely information foraging and information processing by studying expert process workers engaged in tasks based on integrated modeling of business processes and rules we provide insights that pave the way for a better understanding of sensemaking practices and improved development of business process and business rule integration approaches our research underscores the importance of offering personalized support mechanisms that increase the efficacy and efficiency of sensemaking practices for process knowledge workers']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title llmguided reinforcement learning: addressing training bottlenecks through policy modulation summary while reinforcement learning rl has achieved notable success in various domains training effective policies for complex tasks remains challenging agents often converge to local optima and fail to maximize longterm rewards existing approaches to mitigate training bottlenecks typically fall into two categories: i automated policy refinement which identifies critical states from past trajectories to guide policy updates but suffers from costly and uncertain model training and ii humanintheloop refinement where human feedback is used to correct agent behavior but this does not scale well to environments with large or continuous action spaces in this work we design a large language modelguided policy modulation framework that leverages llms to improve rl training without additional model training or human intervention we first prompt an llm to identify critical states from a suboptimal agents trajectories based on these states the llm then provides action suggestions and assigns implicit rewards to guide policy refinement experiments across standard rl benchmarks demonstrate that our method outperforms stateoftheart baselines highlighting the effectiveness of llmbased explanations in addressing rl training bottlenecks']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title teroseek: an aipowered knowledge base and retrieval generation platform for terpenoid research summary terpenoids are a crucial class of natural products that have been studied for over 150 years but their interdisciplinary nature spanning chemistry pharmacology and biology complicates knowledge integration to address this the authors developed teroseek a curated knowledge base kb built from two decades of terpenoid literature coupled with an aipowered questionanswering chatbot and web service leveraging a retrievalaugmented generation rag framework teroseek provides structured highquality information and outperforms generalpurpose large language models llms in terpenoidrelated queries it serves as a domainspecific expert tool for multidisciplinary research and is publicly available at http:teroseekqmclabcom']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title holographic superconductor with dark matter probed by entanglement entropy in higher dimensional ads spacetime summary we investigate the holographic entanglement entropy hee with dark matter in a higherdimensional ads black hole spacetime including full back reaction revealing its role as a diagnostic tool for critical phenomena in strongly coupled systems by analyzing the hee we uncover distinct signatures of the metalsuperconductor phase transition demonstrating that the critical temperature is dynamically tuned by both the dark matter coupling strength and the chemical potential ratio between visible and dark matter sectors notably near the criticality the hee exhibits a novel scaling behavior: it grows linearly with the dark matter coupling but displays a nonlinear accelerated enhancement as the chemical potential ratio between the maxwell and dark matter sectors increases']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title gifarc: synthetic dataset for leveraging humanintuitive analogies to elevate ai reasoning summary the abstraction and reasoning corpus arc poses a stringent test of general ai capabilities requiring solvers to infer abstract patterns from only a handful of examples despite substantial progress in deep learning stateoftheart models still achieve accuracy rates of merely 4055 on 2024 arc competition indicative of a significant gap between their performance and humanlevel reasoning in this work we seek to bridge that gap by introducing an analogyinspired arc dataset gifarc leveraging large language models llms and visionlanguage models vlms we synthesize new arcstyle tasks from a variety of gif images that include analogies each new task is paired with groundtruth analogy providing an explicit mapping between visual transformations and everyday concepts by embedding robust humanintuitive analogies into arcstyle tasks gifarc guides ai agents to evaluate the task analogically before engaging in bruteforce pattern search thus efficiently reducing problem complexity and build a more concise and humanunderstandable solution we empirically validate that guiding llm with analogic approach with gifarc affects tasksolving approaches of llms to align with analogic approach of human']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title continuoustime attention: pdeguided mechanisms for longsequence transformers summary we propose a novel framework continuoustime attention which infuses partial differential equations pdes into the transformers attention mechanism to address the challenges of extremely long input sequences instead of relying solely on a static attention matrix we allow attention weights to evolve over a pseudotime dimension via diffusion wave or reactiondiffusion dynamics this mechanism systematically smooths local noise enhances longrange dependencies and stabilizes gradient flow theoretically our analysis shows that pdebased attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions empirically we benchmark our method on diverse experimentsdemonstrating consistent gains over both standard and specialized long sequence transformer variants our findings highlight the potential of pdebased formulations to enrich attention mechanisms with continuoustime dynamics and global coherence']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title mirror: multiagent intra and interreflection for optimized reasoning in tool learning summary complex tasks involving tool integration pose significant challenges for large language models llms leading to the emergence of multiagent workflows as a promising solution reflection has emerged as an effective strategy for correcting erroneous trajectories in agentic workflows however existing approaches only exploit such capability in the postaction stage where the agent observes the execution outcomes we argue that like humans llms can also engage in reflection before action execution: the agent can anticipate undesirable outcomes from its own decisions which not only provides a necessarily complementary perspective to evaluate the decision but also prevents the propagation of errors throughout the trajectory in this paper we propose mirror a framework that consists of both intrareflection which critically assesses intended actions before execution and interreflection which further adjusts the trajectory based on observations this design systematically leverages llm reflection capabilities to eliminate and rectify erroneous actions on a more comprehensive scope evaluations on both the stabletoolbench and travelplanner benchmarks demonstrate mirrors superior performance achieving stateoftheart results compared to existing approaches']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title contrastive desensitization learning for cross domain face forgery detection summary in this paper we propose a new crossdomain face forgery detection method that is insensitive to different and possibly unseen forgery methods while ensuring an acceptable low false positive rate although existing face forgery detection methods are applicable to multiple domains to some degree they often come with a high false positive rate which can greatly disrupt the usability of the system to address this issue we propose an contrastive desensitization network cdn based on a robust desensitization algorithm which captures the essential domain characteristics through learning them from domain transformation over pairs of genuine face images one advantage of cdn lies in that the learnt face representation is theoretical justified with regard to the its robustness against the domain changes extensive experiments over largescale benchmark datasets demonstrate that our method achieves a much lower false alarm rate with improved detection accuracy compared to several stateoftheart methods']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title selfpercept: introspection improves large language models detection of multiperson mental manipulation in conversations summary mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication making its detection critical for safeguarding potential victims however due to manipulations nuanced and contextspecific nature identifying manipulative language in complex multiturn and multiperson conversations remains a significant challenge for large language models llms to address this gap we introduce the multimanip dataset comprising 220 multiturn multiperson dialogues balanced between manipulative and nonmanipulative interactions all drawn from reality shows that mimic realworld scenarios for manipulative interactions it includes 11 distinct manipulations depicting reallife scenarios we conduct extensive evaluations of stateoftheart llms such as gpt4o and llama318b employing various prompting strategies despite their capabilities these models often struggle to detect manipulation effectively to overcome this limitation we propose selfpercept a novel twostage prompting framework inspired by selfperception theory demonstrating strong performance in detecting multiperson multiturn mental manipulation our code and data are publicly available at https:githubcomdanushkhannaselfpercept ']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title pretraining language models to ponder in continuous space summary humans ponder before articulating complex sentence elements enabling deeper cognitive processing through focused effort in this work we introduce this pondering process into language models by repeatedly invoking the forward process within a single token generation step during pondering instead of generating an actual token sampled from the prediction distribution the model ponders by yielding a weighted sum of all token embeddings according to the predicted token distribution the generated embedding is then fed back as input for another forward pass we show that the model can learn to ponder in this way through selfsupervised learning without any human annotations our method is straightforward and can be seamlessly integrated with various existing language models experiments across three widely used opensource architecturesgpt2 pythia and llamaand extensive downstream task evaluations demonstrate the effectiveness and generality of our method for language modeling tasks pondering language models achieve performance comparable to vanilla models with twice the number of parameters on 9 downstream benchmarks our ponderingenhanced pythia models significantly outperform the official pythia models notably ponderingenhanced pythia1b is comparable to tinyllama11b which is trained on 10 times more data the code is available at https:githubcomlumiagroupponderinglm']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title eigenstructure inference for highdimensional covariance with generalized shrinkage inversewishart prior summary in multivariate statistics estimating the covariance matrix is essential for understanding the interdependence among variables in highdimensional settings where the number of covariates increases with the sample size it is well known that the eigenstructure of the sample covariance matrix is inconsistent the inversewishart prior a standard choice for covariance estimation in bayesian inference also suffers from posterior inconsistency to address the issue of eigenvalue dispersion in highdimensional settings the shrinkage inversewishart siw prior has recently been proposed despite its conceptual appeal and empirical success the asymptotic justification for the siw prior has remained limited in this paper we propose a generalized shrinkage inversewishart gsiw prior for highdimensional covariance modeling by extending the siw framework the gsiw prior accommodates a broader class of prior distributions and facilitates the derivation of theoretical properties under specific parameter choices in particular under the spiked covariance assumption we establish the asymptotic behavior of the posterior distribution for both eigenvalues and eigenvectors by directly evaluating the posterior expectations for two sets of parameter choices this direct evaluation provides insights into the largesample behavior of the posterior that cannot be obtained through general posterior asymptotic theorems finally simulation studies illustrate that the proposed prior provides accurate estimation of the eigenstructure particularly for spiked eigenvalues achieving narrower credible intervals and higher coverage probabilities compared to existing methods for spiked eigenvectors the performance is generally comparable to that of competing approaches including the sample covariance']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [4]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['physics']\n",
      ">>> Loaded id2label_cache: {0: 'cs', 1: 'econ', 2: 'eess', 3: 'math', 4: 'physics', 5: 'q-bio', 6: 'q-fin', 7: 'stat'}\n",
      ">>> Raw input batch:\n",
      "[' title a unified rcs modeling of typical targets for 3gpp isac channel standardization and experimental analysis summary accurate radar cross section rcs modeling is crucial for characterizing target scattering and improving the precision of integrated sensing and communication isac channel modeling existing rcs models are typically designed for specific target types leading to increased complexity and lack of generalization this makes it difficult to standardize rcs models for 3gpp isac channels which need to account for multiple typical target types simultaneously furthermore 3gpp models must support both systemlevel and linklevel simulations requiring the integration of largescale and smallscale scattering characteristics to address these challenges this paper proposes a unified rcs modeling framework that consolidates these two aspects the model decomposes rcs into three components: 1 a largescale power factor representing overall scattering strength 2 a smallscale angulardependent component describing directional scattering and 3 a random component accounting for variations across target instances we validate the model through monostatic rcs measurements for uav human and vehicle targets across five frequency bands the results demonstrate that the proposed model can effectively capture rcs variations for different target types finally the model is incorporated into an isac channel simulation platform to assess the impact of target rcs characteristics on path loss delay spread and angular spread providing valuable insights for future isac system design']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]============>                          (9 + 8) / 17]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [0]============================>         (14 + 3) / 17]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['cs']\n",
      ">>> Predicted class IDs: [2]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['eess']\n",
      ">>> Predicted class IDs: [7]==================================>   (16 + 1) / 17]\n",
      ">>> id2label keys: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      ">>> Final predicted labels: ['stat']\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630d0ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pixi (spark)",
   "language": "python",
   "name": "pixi-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
